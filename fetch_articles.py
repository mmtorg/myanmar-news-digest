import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone, date
from dateutil.parser import parse as parse_date
import re

import os
import sys
from email.message import EmailMessage
from email.utils import formataddr
import unicodedata
from google import genai
from collections import defaultdict
import time
import json
import pprint as _pprint
import random
from typing import List, Dict, Optional
from urllib.parse import urlparse  # è¿½åŠ 
from collections import deque
import base64
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.oauth2.credentials import Credentials
from email.policy import SMTP
from email.header import Header
import xml.etree.ElementTree as ET
from urllib.parse import urljoin

try:
    import httpx
except Exception:
    httpx = None
try:
    import urllib3
except Exception:
    urllib3 = None

try:
    from google.api_core.exceptions import (
        ServiceUnavailable,
        ResourceExhausted,
        DeadlineExceeded,
        InternalServerError,
    )
except Exception:
    ServiceUnavailable = ResourceExhausted = DeadlineExceeded = InternalServerError = (
        Exception
    )


# Geminiæœ¬ç•ªç”¨
client_summary = genai.Client(api_key=os.getenv("GEMINI_API_SUMMARY_KEY"))
client_dedupe = genai.Client(api_key=os.getenv("GEMINI_API_DEDUPE_KEY"))


def _is_retriable_exc(e: Exception) -> bool:
    msg = (str(e) or "").lower()
    name = e.__class__.__name__.lower()

    # Googleç³»ã®æ˜ç¤ºçš„ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡
    if isinstance(
        e,
        (ServiceUnavailable, ResourceExhausted, DeadlineExceeded, InternalServerError),
    ):
        return True

    # httpx/urllib3ç³»ï¼ˆç’°å¢ƒã«ç„¡ã‘ã‚Œã°ç„¡è¦–ï¼‰
    if httpx and isinstance(
        e,
        (
            getattr(httpx, "RemoteProtocolError", Exception),
            getattr(httpx, "ReadTimeout", Exception),
            getattr(httpx, "ConnectError", Exception),
        ),
    ):
        return True
    if urllib3 and isinstance(
        e,
        (
            urllib3.exceptions.ProtocolError,
            urllib3.exceptions.ReadTimeoutError,
            urllib3.exceptions.MaxRetryError,
        ),
    ):
        return True

    # æ–‡å­—åˆ—ã§ã®åˆ¤å®šï¼ˆå®Ÿè£…å·®åˆ†å¸åï¼‰
    hints = [
        "remoteprotocolerror",
        "servererror",
        "internal",  # "500 internal", "internal error" ãªã©
        "server disconnected",
        "unavailable",
        "500",
        "503",
        "502",
        "504",
        "gateway",
        "timeout",
        "temporar",
        "overload",
    ]
    if any(h in msg or h in name for h in hints):
        return True
    return False


def call_gemini_with_retries(
    client,
    prompt,
    model="gemini-2.5-flash",
    max_retries=5,
    base_delay=2.0,
    max_delay=30.0,
    *,
    usage_tag=None,
    temperature=None,
):
    """
    - usage_tag: ãƒ­ã‚°è­˜åˆ¥å­ï¼ˆ'summary' ãªã©ï¼‰
    - temperature: ä»»æ„ï¼ˆæœªæŒ‡å®šãªã‚‰æ—¢å®šï¼‰
    - â€» å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã¯è¨­å®šã—ã¾ã›ã‚“ï¼ˆè¦æ±‚ã«ã‚ˆã‚Šæ’¤å»ƒï¼‰
    """

    # === Gemini ä½¿ç”¨é‡ãƒ­ã‚°ï¼ˆå…¥å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ ======================================
    def _usage_from_resp(resp):
        """
        google-genai ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ usage ã‚’å–ã‚Šå‡ºã™ï¼ˆsnake/camelåŒæ–¹ã«è€æ€§ï¼‰ã€‚
        æˆ»ã‚Šå€¤: dict(prompt_token_count, candidates_token_count, total_token_count,
                    cache_creation_input_token_count, cache_read_input_token_count)
        """
        usage = (
            getattr(resp, "usage_metadata", None)
            or getattr(resp, "usageMetadata", None)
            or {}
        )
        ud = {}
        if usage:
            get = (
                usage.get
                if isinstance(usage, dict)
                else lambda k, d=None: getattr(usage, k, d)
            )
            ud["prompt_token_count"] = get(
                "prompt_token_count", get("input_token_count", get("input_tokens", 0))
            )
            ud["candidates_token_count"] = get(
                "candidates_token_count",
                get("output_token_count", get("output_tokens", 0)),
            )
            ud["total_token_count"] = get(
                "total_token_count",
                get(
                    "total_tokens",
                    (ud.get("prompt_token_count", 0) or 0)
                    + (ud.get("candidates_token_count", 0) or 0),
                ),
            )
            ud["cache_creation_input_token_count"] = get(
                "cache_creation_input_token_count", 0
            )
            ud["cache_read_input_token_count"] = get("cache_read_input_token_count", 0)
        return ud

    def _log_gemini_usage(resp, *, tag: str = "gen", model: str = ""):
        """æ¨™æº–å‡ºåŠ›ï¼‹JSONLãƒ•ã‚¡ã‚¤ãƒ«(gemini_usage.log)ã¸å…¥å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨˜éŒ²"""
        try:
            u = _usage_from_resp(resp) or {}
            rec = {
                "ts": datetime.utcnow().isoformat(timespec="seconds") + "Z",
                "tag": tag,
                "model": model,
                **u,
            }
            print(
                "ğŸ“Š TOKENS[{tag}] in={in_} out={out} total={tot} (cache create/read={cc}/{cr})".format(
                    tag=tag,
                    in_=rec.get("prompt_token_count", 0),
                    out=rec.get("candidates_token_count", 0),
                    tot=rec.get("total_token_count", 0),
                    cc=rec.get("cache_creation_input_token_count", 0),
                    cr=rec.get("cache_read_input_token_count", 0),
                )
            )
            try:
                with open("gemini_usage.log", "a", encoding="utf-8") as f:
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            except Exception:
                pass
        except Exception as e:
            print(f"âš ï¸ usage log failed: {e}")

    # === Free tier monitor (10 Requests per Minute / 250 Requests per Day / 250k Tokens per Minute[input]) ======
    # è¿½åŠ ãƒ­ã‚°: å‡ºåŠ›å´ã® Tokens per Minute (output) ã‚‚é›†è¨ˆã—ã¦è¡¨ç¤ºã™ã‚‹
    class _FreeTierWatch:
        def __init__(self, rpm_limit=10, rpd_limit=250, tpm_limit=250_000):
            self.rpm_limit = int(os.getenv("GEMINI_FREE_RPM", rpm_limit))
            # â€œRPDâ€ ã®ç•¥ç§°ã¯ä½¿ã‚ãšã€æ­£å¼åç§°ã§æ‰±ã†
            self.requests_per_day_limit = int(os.getenv("GEMINI_FREE_RPD", rpd_limit))
            # ç„¡æ–™æ ã®TPMåˆ¤å®šã¯å…¥åŠ›ãŒåŸºæº–
            self.tpm_limit = int(os.getenv("GEMINI_FREE_TPM", tpm_limit))

            self.req_times = deque()  # ç›´è¿‘60ç§’ã®æˆåŠŸãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†æ™‚åˆ»
            self.tpm_in_points = deque()  # ç›´è¿‘60ç§’ã® (æ™‚åˆ», å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³)
            self.tpm_out_points = deque()  # ç›´è¿‘60ç§’ã® (æ™‚åˆ», å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³)
            self.day_key = None  # MMT æ—¥ä»˜ã‚­ãƒ¼ï¼ˆUTC+6:30ï¼‰
            self.requests_per_day_count = 0

            # â€œè¶ŠãˆãŸç¬é–“ã ã‘â€é€šçŸ¥ã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ
            self._over_rpm = False
            self._over_tpm_in = False
            self._over_rpd = False

            # æ¯å›ã®ãƒ¬ãƒ¼ãƒˆçª“ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå‡ºåŠ›ï¼ˆæ¨™æº–å‡ºåŠ›ã®ã¿ï¼æ—¢å®šONï¼‰
            self._rate_window_log_enabled = str(
                os.getenv("GEMINI_RATE_WINDOW_LOG", "1")
            ).lower() not in ("0", "false", "off")

        def _mmt_today(self, now_utc):
            mmt = timezone(timedelta(hours=6, minutes=30))
            return now_utc.astimezone(mmt).date()

        def record(
            self,
            prompt_tokens: int,
            output_tokens: int = 0,
            *,
            tag: str = "gen",
            model: str = "",
        ):
            now = datetime.utcnow().replace(tzinfo=timezone.utc)

            # ç›´è¿‘60ç§’çª“ï¼ˆRequests per Minute / Tokens per Minuteï¼‰
            self.req_times.append(now)
            self.tpm_in_points.append((now, int(prompt_tokens or 0)))
            self.tpm_out_points.append((now, int(output_tokens or 0)))
            cutoff = now - timedelta(seconds=60)
            while self.req_times and self.req_times[0] < cutoff:
                self.req_times.popleft()
            while self.tpm_in_points and self.tpm_in_points[0][0] < cutoff:
                self.tpm_in_points.popleft()
            while self.tpm_out_points and self.tpm_out_points[0][0] < cutoff:
                self.tpm_out_points.popleft()

            rpm = len(self.req_times)
            tpm_in = sum(tok for _, tok in self.tpm_in_points)
            tpm_out = sum(tok for _, tok in self.tpm_out_points)

            # Requests per Day â€” MMTæ—¥ä»˜ã§ã‚«ã‚¦ãƒ³ãƒˆ
            today_mmt = self._mmt_today(now)
            if self.day_key != today_mmt:
                self.day_key = today_mmt
                self.requests_per_day_count = 0
                self._over_rpd = False  # æ—¥ã¾ãŸãã§ãƒªã‚»ãƒƒãƒˆ
            self.requests_per_day_count += 1  # ã“ã®æˆåŠŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨ˆä¸Š

            # è¶…éåˆ¤å®šï¼ˆå…¥åŠ›TPM/RPM/Requests per Dayï¼‰
            over_rpm = rpm > self.rpm_limit
            over_tpm_in = tpm_in > self.tpm_limit
            over_rpd = self.requests_per_day_count > self.requests_per_day_limit

            def _emit_exceeded(kind_label: str, detail: str):
                # kind_label ã¯æ­£å¼åç§°ã§ï¼š "Requests per Minute" / "Tokens per Minute (input)" / "Requests per Day"
                print(
                    f"ğŸš© FREE-TIER EXCEEDED [{kind_label}] {detail} | tag={tag} model={model}"
                )

            # è¶…éé€šçŸ¥ï¼ˆæ­£å¼åç§°ï¼‰
            if over_rpm and not self._over_rpm:
                self._over_rpm = True
                _emit_exceeded(
                    "Requests per Minute", f"{rpm}>{self.rpm_limit} within last 60s"
                )
            elif not over_rpm:
                self._over_rpm = False

            if over_tpm_in and not self._over_tpm_in:
                self._over_tpm_in = True
                _emit_exceeded(
                    "Tokens per Minute (input)",
                    f"input={tpm_in} > {self.tpm_limit} in last 60s",
                )
            elif not over_tpm_in:
                self._over_tpm_in = False

            if over_rpd and not self._over_rpd:
                self._over_rpd = True
                _emit_exceeded(
                    "Requests per Day",
                    f"{self.requests_per_day_count}>{self.requests_per_day_limit} (MMT day {today_mmt})",
                )

            # æ¯å›ã®ãƒ¬ãƒ¼ãƒˆçª“ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆäººé–“å¯èª­ã€JSONå‡ºåŠ›ãªã—ï¼‰
            if self._rate_window_log_enabled:
                print(
                    "â„¹ï¸ WINDOW [rate] "
                    f"Requests per Minute={rpm} | "
                    f"Tokens per Minute (input)={tpm_in} | "
                    f"Tokens per Minute (output)={tpm_out} | "
                    f"Requests per Day={self.requests_per_day_count} "
                    f"(MMT day {today_mmt}) | tag={tag} model={model}"
                )

    # æœ‰åŠ¹/ç„¡åŠ¹ãƒˆã‚°ãƒ«ï¼ˆæ—¢å®š=æœ‰åŠ¹ï¼‰
    _FREE_TIER_CHECK_ENABLED = str(
        os.getenv("GEMINI_FREE_TIER_CHECK", "1")
    ).lower() not in ("0", "false", "off")
    _FREE_TIER_MON = _FreeTierWatch() if _FREE_TIER_CHECK_ENABLED else None
    # =============================================================================================================

    # ä»»æ„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã‘è¨­å®šï¼ˆä¸Šé™ã¯å…¥ã‚Œãªã„ï¼‰
    cfg = {}
    if temperature is not None:
        cfg["temperature"] = float(temperature)

    kwargs = {}
    if cfg:
        try:
            kwargs["config"] = genai.types.GenerateContentConfig(**cfg)  # type: ignore[attr-defined]
        except Exception:
            kwargs["config"] = cfg

    delay = base_delay
    for attempt in range(1, max_retries + 1):
        try:
            resp = client.models.generate_content(
                model=model, contents=prompt, **kwargs
            )

            # 1) ä½¿ç”¨é‡ãƒ­ã‚°
            _log_gemini_usage(resp, tag=(usage_tag or "gen"), model=model)

            # 2) ç„¡æ–™æ ç›£è¦–ï¼ˆMMTæ—¥æ¬¡ / RPM / å…¥åŠ›TPMï¼‰
            try:
                if _FREE_TIER_MON:
                    u = _usage_from_resp(resp) or {}
                    _FREE_TIER_MON.record(
                        int(u.get("prompt_token_count") or 0),
                        output_tokens=int(
                            u.get("candidates_token_count") or 0
                        ),  # å‡ºåŠ›é‡ã®TPMé›†è¨ˆç”¨
                        tag=(usage_tag or "gen"),
                        model=model,
                    )
            except Exception:
                pass

            return resp
        except Exception as e:
            if not _is_retriable_exc(e) or attempt == max_retries:
                raise
            print(
                f"âš ï¸ Gemini retry {attempt}/{max_retries} after: {e.__class__.__name__} | {e}"
            )
            time.sleep(min(max_delay, delay) + random.random() * 0.5)
            delay *= 2


# è¦ç´„ç”¨ã«é€ã‚‹æœ¬æ–‡ã®æœ€å¤§æ–‡å­—æ•°ï¼ˆå›ºå®šï¼‰
# Irrawaddyè‹±èªè¨˜äº‹ãŒ3500æ–‡å­—ãã‚‰ã„ã‚ã‚‹
BODY_MAX_CHARS = 3500

# ãƒŸãƒ£ãƒ³ãƒãƒ¼æ¨™æº–æ™‚ (UTC+6:30)
MMT = timezone(timedelta(hours=6, minutes=30))


# ä»Šæ—¥ã®æ—¥ä»˜
# ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é€Ÿå ±æ€§é‡è¦–ã§ä»Šæ—¥åˆ†ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã®æ–¹é‡
def get_today_date_mmt():
    # æœ¬ç•ªç”¨ã€ä»Šæ—¥ã®æ—¥ä»˜
    now_mmt = datetime.now(MMT)
    return now_mmt.date()


# å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆå…¨ãƒ¡ãƒ‡ã‚£ã‚¢å…±é€šã§ä½¿ç”¨ã™ã‚‹ï¼‰
NEWS_KEYWORDS = [
    # ãƒŸãƒ£ãƒ³ãƒãƒ¼ï¼ˆå›½åãƒ»ç¾è¡Œåç§°ï¼‰
    "á€™á€¼á€”á€ºá€™á€¬",
    "á€™á€¼á€”á€ºá€™á€¬á€·",
    "Myanmar",
    "myanmar",
    # ãƒ“ãƒ«ãƒï¼ˆæ—§å›½åãƒ»é€šç§°ï¼‰
    "á€—á€™á€¬",
    "Burma",
    "burma",
    # ã‚¢ã‚¦ãƒ³ã‚µãƒ³ã‚¹ãƒ¼ãƒãƒ¼ï¼ˆAung San Suu Kyiï¼‰
    "á€¡á€±á€¬á€„á€ºá€†á€”á€ºá€¸á€…á€¯á€€á€¼á€Šá€º",
    "Aung San Suu Kyi",
    "aung san suu kyi",
    # ãƒŸãƒ³ã‚¢ã‚¦ãƒ³ãƒ•ãƒ©ã‚¤ãƒ³ï¼ˆMin Aung Hlaingï¼‰
    "á€™á€„á€ºá€¸á€¡á€±á€¬á€„á€ºá€œá€¾á€­á€¯á€„á€º",
    "Min Aung Hlaing",
    "min aung hlaing",
    # ãƒãƒ£ãƒƒãƒˆï¼ˆKyatï¼‰
    "Kyat",
    "kyat",
    # å¾´å…µåˆ¶ï¼ˆConscription / Military Draftï¼‰, å¾´å…µ, å…µå½¹
    "á€…á€…á€ºá€á€•á€ºá€á€„á€ºá€á€±á€«á€ºá€›á€±á€¸",
    "á€…á€…á€ºá€™á€¾á€¯á€‘á€™á€ºá€¸",
    "á€¡á€á€„á€ºá€¸á€á€•á€ºá€á€„á€ºá€á€±á€«á€ºá€á€¼á€„á€ºá€¸",
    "á€á€•á€ºá€á€„á€ºá€á€±á€«á€º ",
    "Conscription",
    "conscription",
    "Military Draft",
    "Military draft",
    "military draft",
    "Military Service",
    "Military service",
    "military service",
    # ãƒ­ãƒ’ãƒ³ã‚®ãƒ£ ãŠé¡˜ã„ã•ã‚Œã¦ãªã„
    # "á€›á€­á€¯á€Ÿá€„á€ºá€‚á€»á€¬",
    # "Rohingya",
    # "rohingya",
    # å›½å¢ƒè²¿æ˜“ãƒ»äº¤æ˜“
    "á€”á€šá€ºá€…á€•á€ºá€€á€¯á€”á€ºá€á€½á€šá€ºá€›á€±á€¸",
    # ãƒ¤ãƒ³ã‚´ãƒ³ç®¡åŒº
    # "á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸",
    # ãƒ¤ãƒ³ã‚´ãƒ³
    "á€›á€”á€ºá€€á€¯á€”á€º",
    "Yangon Region",
    "Yangon region",
    "yangon region",
]

# Unicodeæ­£è¦åŒ–ï¼ˆNFCï¼‰ã‚’é©ç”¨
NEWS_KEYWORDS = [unicodedata.normalize("NFC", kw) for kw in NEWS_KEYWORDS]

# --- Ayeyarwady (ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£) ç³»ã ã‘ã‚’æŠœãå‡ºã™ã‚µãƒ–ã‚»ãƒƒãƒˆ ---
AYEYARWADY_KEYWORDS = [
    "á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸",
    "Ayeyarwady Region",
    "Ayeyarwady region",
    "ayeyarwady region",
]
AYEYARWADY_KEYWORDS = [unicodedata.normalize("NFC", kw) for kw in AYEYARWADY_KEYWORDS]

def is_ayeyarwady_hit(title: str, body: str) -> bool:
    """ã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡ã«ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹"""
    return any(kw in title or kw in body for kw in AYEYARWADY_KEYWORDS)

# ã€Œãƒãƒ£ãƒƒãƒˆã€èªã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šè²¨èªã¨ã—ã¦æ‹¾ã†
CURRENCY_WORD = r"(?:á€™á€¼á€”á€ºá€™á€¬(?:á€·)?(?:á€”á€­á€¯á€„á€ºá€„á€¶)?\s*)?(?:á€„á€½á€±\s*)?á€€á€»á€•á€º(?:á€„á€½á€±)?"

_DIGITS = r"[0-9á€-á‰][0-9á€-á‰,\.]*"
_SCALE = r"(?:á€á€±á€¬á€„á€ºá€¸|á€á€­á€”á€ºá€¸|á€á€”á€ºá€¸)"
_TRAIL = r"(?:\s*(?:á€€á€»á€±á€¬á€º|á€œá€±á€¬á€€á€º|á€á€”á€·á€º))?"

# 1) æ•°å­—â†’é€šè²¨ï¼ˆnum/scale ã¯ã“ã“ã§1å›ã ã‘å®šç¾©ï¼‰
_KYAT_NUM_FIRST = re.compile(
    rf"""
    (?P<num>{_DIGITS})\s*(?P<scale>{_SCALE})?\s*(?:{CURRENCY_WORD})
    {_TRAIL}
    """,
    re.VERBOSE,
)

# 2) é€šè²¨â†’æ•°å­—ï¼ˆåŒã˜ã‚°ãƒ«ãƒ¼ãƒ—åã‚’ã“ã“ã§ã‚‚1å›ã ã‘å®šç¾©ï¼‰
_KYAT_CCY_FIRST = re.compile(
    rf"""
    (?:{CURRENCY_WORD})\s*(?P<scale>{_SCALE})?\s*(?P<num>{_DIGITS})
    {_TRAIL}
    """,
    re.VERBOSE,
)


class _OrPattern:
    """è¤‡æ•°ã® compiled regex ã‚’ã¾ã¨ã‚ã€.search ã§æœ€åˆã«å½“ãŸã£ãŸ Match ã‚’è¿”ã™è–„ã„ãƒ©ãƒƒãƒ‘"""

    def __init__(self, *compiled):
        self._compiled = compiled
        self.pattern = " | ".join(p.pattern for p in compiled)  # å‚è€ƒç”¨
        self.flags = compiled[0].flags if compiled else 0

    def search(self, string, pos=0):
        for p in self._compiled:
            m = p.search(string, pos)
            if m:
                return m
        return None


KYAT_PATTERN = _OrPattern(_KYAT_NUM_FIRST, _KYAT_CCY_FIRST)


def any_keyword_hit(title: str, body: str) -> bool:
    # é€šå¸¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´
    if any(kw in title or kw in body for kw in NEWS_KEYWORDS):
        return True
    # é€šè²¨ã€Œá€€á€»á€•á€ºã€ã ã‘ã¯æ­£è¦è¡¨ç¾ã§åˆ¤å®š
    if KYAT_PATTERN.search(title) or KYAT_PATTERN.search(body):
        return True
    return False



def clean_html_content(html: str) -> str:
    html = html.replace("\xa0", " ").replace("&nbsp;", " ")
    # åˆ¶å¾¡æ–‡å­—ï¼ˆã‚«ãƒ†ã‚´ãƒªCï¼‰ã‚’é™¤å¤–ã€å¯è¦–Unicodeã¯ãã®ã¾ã¾
    return "".join(c for c in html if unicodedata.category(c)[0] != "C")


# æœ¬æ–‡ãŒå–å¾—ã§ãã‚‹ã¾ã§ã€Œrequestsã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€
def fetch_with_retry(url, retries=3, wait_seconds=2):
    for attempt in range(retries):
        try:
            res = requests.get(url, timeout=10)
            if res.status_code == 200 and res.text.strip():
                return res
        except Exception as e:
            print(f"Attempt {attempt + 1} failed for {url}: {e}")
        time.sleep(wait_seconds)
    raise Exception(f"Failed to fetch {url} after {retries} attempts.")


# æœ¬æ–‡ãŒç©ºãªã‚‰ã€Œä¸€å®šç§’æ•°å¾…ã£ã¦å†å–å¾—ã€
def extract_paragraphs_with_wait(soup_article, retries=2, wait_seconds=2):
    for attempt in range(retries + 1):
        paragraphs = soup_article.select("div.entry-content p")
        if not paragraphs:
            paragraphs = soup_article.select("div.node-content p")
        if not paragraphs:
            paragraphs = soup_article.select("article p")
        if not paragraphs:
            paragraphs = soup_article.find_all("p")

        if paragraphs:
            return paragraphs

        print(f"Paragraphs not found, waiting {wait_seconds}s and retrying...")
        time.sleep(wait_seconds)
    return []


# === æ±ç”¨ã® <p> æŠ½å‡ºå™¨ï¼ˆã‚µã‚¤ãƒˆå…±é€šï¼‰ ===
def extract_body_generic_from_soup(soup):
    for sel in ["div.entry-content p", "div.node-content p", "article p"]:
        ps = soup.select(sel)
        if ps:
            break
    else:
        ps = soup.find_all("p")
    txts = [p.get_text(strip=True) for p in ps if p.get_text(strip=True)]
    return "\n".join(txts).strip()


# === requests ã‚’ä½¿ã†ã‚·ãƒ³ãƒ—ãƒ«ãª fetch_onceï¼ˆ1å›ï¼‰ ===
def fetch_once_requests(url, timeout=15):
    r = requests.get(url, timeout=timeout)
    r.raise_for_status()
    # æ–‡å­—åŒ–ã‘å›é¿ã®ãŸã‚ bytes ã‚’è¿”ã™ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ã¯ BeautifulSoup ã«ä»»ã›ã‚‹ï¼‰
    return r.content


# === å†ãƒ•ã‚§ãƒƒãƒä»˜ããƒ»æœ¬æ–‡å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===
def get_body_with_refetch(
    url, fetcher, extractor, retries=3, wait_seconds=2, quiet=False
):
    """
    fetcher(url) -> html(bytes or str)
    extractor(soup) -> body(str)
    """
    last_err = None
    for attempt in range(retries + 1):
        try:
            html = fetcher(url)
            # bytes/str ã©ã¡ã‚‰ã§ã‚‚ BeautifulSoup ã«æ¸¡ã›ã‚‹
            soup = BeautifulSoup(html, "html.parser")

            # èª¤ã£ã¦ latin-1 ç³»ã§è§£é‡ˆã•ã‚ŒãŸå ´åˆã¯ UTF-8 ã§å†è§£é‡ˆã—ã¦ä¿é™ºã‚’ã‹ã‘ã‚‹
            enc = (getattr(soup, "original_encoding", None) or "").lower()
            if enc in ("iso-8859-1", "latin-1", "windows-1252"):
                soup = BeautifulSoup(html, "html.parser", from_encoding="utf-8")

            body = extractor(soup)
            if body:
                return unicodedata.normalize("NFC", body)

            if not quiet:
                print(f"[refetch] body empty, retrying {attempt+1}/{retries} â†’ {url}")
        except Exception as e:
            last_err = e
            if not quiet:
                print(f"[refetch] EXC {attempt+1}/{retries}: {e} â†’ {url}")
        time.sleep(wait_seconds)

    if not quiet and last_err:
        print(f"[refetch] give up after {retries+1} tries â†’ {url}")
    return ""


# === Irrawaddyå°‚ç”¨ ===
# æœ¬æ–‡ãŒå–å¾—ã§ãã‚‹ã¾ã§ã€Œrequestsã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€
def fetch_with_retry_irrawaddy(url, retries=3, wait_seconds=2, session=None):
    """
    Irrawaddy å°‚ç”¨ãƒ•ã‚§ãƒƒãƒãƒ£ï¼ˆå˜ç™ºãƒˆãƒ©ã‚¤ç‰ˆï¼‰ã€‚
    - curl_cffi ã§1å› â†’ å¤±æ•—ãªã‚‰ cloudscraper ã§1å› â†’ å¤±æ•—ãªã‚‰ requests ã§1å›ã€‚
    - è¿½åŠ ã®ãƒªãƒˆãƒ©ã‚¤ã‚„ /amp ã¸ã®å†è©¦è¡Œã¯å®Ÿæ–½ã—ãªã„ã€‚
    """
    import os
    import random
    import time
    import urllib.parse

    UA = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/128.0.0.0 Safari/537.36"
    )
    HEADERS = {
        "User-Agent": UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-User": "?1",
        "Sec-Fetch-Dest": "document",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": "https://www.irrawaddy.com/",
        "Connection": "keep-alive",
    }

    def _amp_url(u: str) -> str:
        # https://.../path/ ãªã‚‰ https://.../path/amp
        # https://.../path  ãªã‚‰ https://.../path/amp
        if not u.endswith("/"):
            u = u + "/"
        return urllib.parse.urljoin(u, "amp")

    # --- Try 1: curl_cffi (Chrome æŒ‡ç´‹) å˜ç™º ---
    try:
        from curl_cffi import requests as cfr  # type: ignore[import-not-found]
        proxies = {
            "http": os.getenv("HTTP_PROXY") or os.getenv("http_proxy"),
            "https": os.getenv("HTTPS_PROXY") or os.getenv("https_proxy"),
        }
        r = cfr.get(
            url,
            headers=HEADERS,
            impersonate="chrome124",
            timeout=30,
            allow_redirects=True,
            proxies={k: v for k, v in proxies.items() if v},
        )
        if r.status_code == 200 and (r.text or "").strip():
            return r
    except Exception as e:
        print(f"[fetch-cffi] EXC: {e} â†’ {url}")

    # --- Try 2: cloudscraper å˜ç™º ---
    try:
        import cloudscraper
        import requests as rq
        sess = session or rq.Session()
        scraper = cloudscraper.create_scraper(
            sess=sess,
            browser={"browser": "chrome", "platform": "windows", "mobile": False},
            delay=7,
        )
        r = scraper.get(url, headers=HEADERS, timeout=30, allow_redirects=True)
        if r.status_code == 200 and getattr(r, "text", "").strip():
            return r
    except Exception as e:
        print(f"[fetch-cs] EXC: {e} â†’ {url}")

    # --- Try 3: requests å˜ç™ºï¼ˆ/news/ ã®ã¨ãã®ã¿ /amp ã‚’1å›ã ã‘è©¦ã™ï¼‰ ---
    try:
        import requests
        sess = session or requests.Session()
        r2 = sess.get(url, headers=HEADERS, timeout=20, allow_redirects=True)
        print(
            f"[fetch-rq] final: HTTP {r2.status_code} len={len(getattr(r2,'text',''))} â†’ {url}"
        )
        if r2.status_code == 200 and getattr(r2, "text", "").strip():
            return r2
        # 403/503 ã‹ã¤ /news/ ã®è¨˜äº‹URLã«é™ã‚Šã€/amp ã‚’â€œ1å›ã ã‘â€è©¦ã™
        if r2.status_code in (403, 503) and "/news/" in url:
            amp = _amp_url(url)
            r3 = sess.get(amp, headers=HEADERS, timeout=20, allow_redirects=True)
            print(
                f"[fetch-rq] amp: HTTP {r3.status_code} len={len(getattr(r3,'text',''))} â†’ {amp}"
            )
            if r3.status_code == 200 and getattr(r3, "text", "").strip():
                return r3
        try:
            svr = r2.headers.get("server") or r2.headers.get("Server")
            ray = r2.headers.get("cf-ray")
            sucuri = r2.headers.get("x-sucuri-id") or r2.headers.get("x-sucuri-block")
            print(f"[fetch-rq] headers: server={svr} cf-ray={ray} sucuri={sucuri}")
        except Exception:
            pass
    except Exception as e:
        print(f"[fetch-rq] EXC final: {e} â†’ {url}")

    raise Exception(f"Failed to fetch {url} after {retries} attempts.")


# === DVBå°‚ç”¨ ===
def fetch_with_retry_dvb(url, retries=4, wait_seconds=2, session=None):
    """
    DVB (https://burmese.dvb.no) å‘ã‘ã®å¤šæ®µãƒ•ã‚§ãƒƒãƒãƒ£ã€‚
    1) curl_cffi(ChromeæŒ‡ç´‹) â†’ 2) cloudscraper â†’ 3) requests ã®é †ã€‚
    403/429/503 ã¯æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã€‚/post/* ã§ã¯ /amp / ?output=amp ã‚‚è©¦ã™ã€‚
    """
    import os
    import time
    import random

    UA = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/128.0.0.0 Safari/537.36"
    )
    BASE = "https://burmese.dvb.no"
    HEADERS = {
        "User-Agent": UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9,my;q=0.8,ja;q=0.7",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-User": "?1",
        "Sec-Fetch-Dest": "document",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": f"{BASE}/",
        "Connection": "keep-alive",
    }

    def _amp_candidates(u: str):
        u = u.strip()
        q = "&" if "?" in u else "?"
        return [u.rstrip("/") + "/amp", u + f"{q}output=amp"]

    # --- Try 1: curl_cffi ---
    try:
        from curl_cffi import requests as cfr  # type: ignore

        proxies = {
            "http": os.getenv("HTTP_PROXY") or os.getenv("http_proxy"),
            "https": os.getenv("HTTPS_PROXY") or os.getenv("https_proxy"),
        }
        for attempt in range(retries):
            r = cfr.get(
                url,
                headers=HEADERS,
                impersonate="chrome124",
                timeout=30,
                allow_redirects=True,
                proxies={k: v for k, v in proxies.items() if v},
            )
            if r.status_code == 200 and (r.text or "").strip():
                return r
            # è¨˜äº‹URLã¯AMPç³»ã‚‚è©¦ã™
            if r.status_code in (403, 503) and "/post/" in url:
                for amp in _amp_candidates(url):
                    r2 = cfr.get(
                        amp,
                        headers=HEADERS,
                        impersonate="chrome124",
                        timeout=30,
                        allow_redirects=True,
                        proxies={k: v for k, v in proxies.items() if v},
                    )
                    if r2.status_code == 200 and (r2.text or "").strip():
                        return r2
            if r.status_code in (403, 429, 503):
                time.sleep(wait_seconds * (2**attempt) + random.uniform(0, 0.8))
                continue
            break
    except Exception as e:
        print(f"[dvb-cffi] EXC: {e} â†’ {url}")

    # --- Try 2: cloudscraper ---
    try:
        import cloudscraper
        import requests as rq

        sess = session or rq.Session()
        scraper = cloudscraper.create_scraper(
            sess=sess,
            browser={"browser": "chrome", "platform": "windows", "mobile": False},
            delay=7,
        )
        for attempt in range(retries):
            try:
                r = scraper.get(url, headers=HEADERS, timeout=30, allow_redirects=True)
                if r.status_code == 200 and getattr(r, "text", "").strip():
                    return r
                if r.status_code in (403, 503) and "/post/" in url:
                    for amp in _amp_candidates(url):
                        r2 = scraper.get(
                            amp, headers=HEADERS, timeout=30, allow_redirects=True
                        )
                        if r2.status_code == 200 and getattr(r2, "text", "").strip():
                            return r2
                if r.status_code in (403, 429, 503):
                    time.sleep(wait_seconds * (2**attempt) + random.uniform(0, 0.8))
                    continue
                break
            except Exception as e:
                print(f"[dvb-cs] {attempt+1}/{retries} EXC: {e} â†’ {url}")
                time.sleep(wait_seconds * (2**attempt) + random.uniform(0, 0.8))
    except Exception as e:
        print(f"[dvb-cs] INIT EXC: {e} â†’ {url}")

    # --- Try 3: requests ---
    try:
        import requests

        sess = session or requests.Session()
        r2 = sess.get(url, headers=HEADERS, timeout=30, allow_redirects=True)
        if r2.status_code == 200 and getattr(r2, "text", "").strip():
            return r2
        if r2.status_code in (403, 503) and "/post/" in url:
            for amp in _amp_candidates(url):
                r3 = sess.get(amp, headers=HEADERS, timeout=30, allow_redirects=True)
                if r3.status_code == 200 and getattr(r3, "text", "").strip():
                    return r3
    except Exception as e:
        print(f"[dvb-rq] EXC final: {e} â†’ {url}")

    raise Exception(f"Failed to fetch DVB {url} after {retries} attempts.")


def _norm_text(text: str) -> str:
    return unicodedata.normalize("NFC", text)


def _norm_id(u):
    """ID/URLç…§åˆç”¨ã®è»½é‡æ­£è¦åŒ–ï¼šæœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’è½ã¨ã™"""
    if isinstance(u, str):
        return u.rstrip("/")
    return u


def _parse_category_date_text(text: str):
    # ä¾‹: 'August 9, 2025'
    text = re.sub(r"\s+", " ", text.strip())
    return datetime.strptime(text, "%B %d, %Y").date()


def _article_date_from_meta_mmt(soup):
    meta = soup.find("meta", attrs={"property": "article:published_time"})
    if not meta or not meta.get("content"):
        return None
    iso = meta["content"].replace("Z", "+00:00")  # æœ«å°¾Zå¯¾ç­–
    dt = datetime.fromisoformat(iso)
    return dt.astimezone(MMT).date()


def _extract_title(soup):
    t = soup.find("title")
    return _norm_text(t.get_text(strip=True)) if t else None


def _is_excluded_by_ancestor(node) -> bool:
    excluded = {
        "jnews_inline_related_post",
        "jeg_postblock_21",
        "widget",
        "widget_jnews_popular",
        "jeg_postblock_5",
        "jnews_related_post_container",
        "widget widget_jnews_popular",
        "jeg_footer_primary clearfix",
    }
    for anc in node.parents:
        classes = anc.get("class", [])
        if any(c in excluded for c in classes):
            return True
    return False


# æœ¬æ–‡æŠ½å‡º
def extract_body_irrawaddy(soup):
    # <div class="content-inner "> é…ä¸‹ã® <p>ã®ã¿ï¼ˆé™¤å¤–ãƒ–ãƒ­ãƒƒã‚¯é…ä¸‹ã¯é™¤å¤–ï¼‰
    paragraphs = []
    content_inners = soup.select("div.content-inner")
    if not content_inners:
        content_inners = [
            div
            for div in soup.find_all("div")
            if "content-inner" in (div.get("class") or [])
        ]
    for root in content_inners:
        for p in root.find_all("p"):
            if _is_excluded_by_ancestor(p):
                continue
            txt = p.get_text(strip=True)
            if txt:
                paragraphs.append(_norm_text(txt))
    return "\n".join(paragraphs).strip()


#  Irrawaddy ç”¨ fetch_onceï¼ˆæ—¢å­˜ã® fetch_with_retry_irrawaddy ã‚’1å›ãƒ©ãƒƒãƒ—ï¼‰
def fetch_once_irrawaddy(url, session=None):
    r = fetch_with_retry_irrawaddy(url, retries=1, wait_seconds=0, session=session)
    # cloudscraper ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚‚ bytes ã‚’è¿”ã™ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ã¯ BeautifulSoup ã«ä»»ã›ã‚‹ï¼‰
    return r.content


# === ã“ã“ã¾ã§ ===


# ===== ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªãƒ’ãƒƒãƒˆæ™‚ã®å…±é€šãƒ­ã‚¬ãƒ¼ï¼ˆç°¡ç´ ç‰ˆï¼‰ =====
LOG_NO_KEYWORD_MISSES = True


def log_no_keyword_hit(source: str, url: str, title: str, body: str, stage: str):
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªãƒ’ãƒƒãƒˆã®è¨˜äº‹ã‚’æ¨™æº–å‡ºåŠ›ã«å‡ºã™ï¼ˆstageãƒ»æœ¬æ–‡æŠœç²‹ã¯å‡ºåŠ›ã—ãªã„ï¼‰ã€‚
    """
    if not LOG_NO_KEYWORD_MISSES:
        return
    try:
        title = unicodedata.normalize("NFC", title or "")
    except Exception:
        pass

    print("\n----- NO KEYWORD HIT -----")
    print(f"[source] {source}")
    print(f"[url]    {url}")
    print(f"[title]  {title}")
    print("----- END NO KEYWORD HIT -----\n")


# Mizzimaã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸å·¡å›ã§å–å¾—
def get_mizzima_articles_from_category(
    date_obj, base_url, source_name, category_path, max_pages=3
):
    # ==== ãƒ­ãƒ¼ã‚«ãƒ«å®šæ•° Mizzimaé™¤å¤–å¯¾è±¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼‰====
    EXCLUDE_TITLE_KEYWORDS = [
        # æ˜¥ã®é©å‘½æ—¥èªŒ
        "á€”á€½á€±á€¦á€¸á€á€±á€¬á€ºá€œá€¾á€”á€ºá€›á€±á€¸ á€”á€±á€·á€…á€‰á€ºá€™á€¾á€á€ºá€…á€¯",
        # å†™çœŸãƒ‹ãƒ¥ãƒ¼ã‚¹
        "á€“á€¬á€á€ºá€•á€¯á€¶á€á€á€„á€ºá€¸",
    ]

    article_urls = []

    for page_num in range(1, max_pages + 1):
        if page_num == 1:
            url = f"{base_url}{category_path}"
        else:
            url = f"{base_url}{category_path}/page/{page_num}/"

        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                continue

            soup = BeautifulSoup(res.content, "html.parser")
            links = [
                a["href"]
                for a in soup.select("main.site-main article a.post-thumbnail[href]")
            ]
            article_urls.extend(links)

        except Exception as e:
            print(f"Error crawling category page {url}: {e}")
            continue

    filtered_articles = []
    for url in article_urls:
        try:
            res_article = fetch_with_retry(url)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            meta_tag = soup_article.find("meta", property="article:published_time")
            if not meta_tag or not meta_tag.has_attr("content"):
                continue

            date_str = meta_tag["content"]
            article_datetime_utc = datetime.fromisoformat(date_str)
            article_datetime_mmt = article_datetime_utc.astimezone(MMT)
            article_date = article_datetime_mmt.date()

            if article_date != date_obj:
                continue

            title_tag = soup_article.find("meta", attrs={"property": "og:title"})
            if not title_tag or not title_tag.has_attr("content"):
                continue
            title = title_tag["content"].strip()

            # === é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‚’NFCæ­£è¦åŒ–ã—ã¦ã‹ã‚‰ï¼‰ ===
            title_nfc = unicodedata.normalize("NFC", title)
            if any(kw in title_nfc for kw in EXCLUDE_TITLE_KEYWORDS):
                print(f"SKIP: excluded keyword in title â†’ {url} | TITLE: {title_nfc}")
                continue

            content_div = soup_article.find("div", class_="entry-content")
            if not content_div:
                continue

            paragraphs = []
            for p in content_div.find_all("p"):
                if p.find_previous("h2", string=re.compile("Related Posts", re.I)):
                    break
                paragraphs.append(p)

            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            body_text = unicodedata.normalize("NFC", body_text)

            if not body_text.strip():
                continue

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šã¯æ­£è¦åŒ–æ¸ˆã¿ã‚¿ã‚¤ãƒˆãƒ«ã§è¡Œã†
            if not any_keyword_hit(title, body_text):
                log_no_keyword_hit(
                    source_name, url, title, body_text, "mizzima:category"
                )
                continue

            filtered_articles.append(
                {
                    "source": source_name,
                    "url": url,
                    "title": title,
                    "date": article_date.isoformat(),
                    "body": body_text,
                }
            )

        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return filtered_articles


# BCCã¯RSSã‚ã‚‹ã®ã§ãã‚Œä½¿ã†
def get_bbc_burmese_articles_for(target_date_mmt):
    
    # ==== ãƒ­ãƒ¼ã‚«ãƒ«å®šæ•° ====
    NOISE_PATTERNS = [
        r"BBC\s*News\s*á€™á€¼á€”á€ºá€™á€¬",  # å›ºå®šç½²åï¼ˆBurmeseè¡¨è¨˜ï¼‰
        r"BBC\s*Burmese",  # è‹±èªè¡¨è¨˜
    ]

    # BBC Burmese ã®ã€Œã‚¿ã‚¤ãƒˆãƒ«é™¤å¤–ã€ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå°†æ¥è¿½åŠ ã—ã‚„ã™ã„ã‚ˆã†é…åˆ—ã§ä¿æŒï¼‰
    EXCLUDE_TITLE_KEYWORDS = [
        "á€”á€­á€¯á€„á€ºá€„á€¶á€á€á€”á€ºá€¸á€á€á€„á€ºá€¸á€™á€»á€¬á€¸ á€¡á€”á€¾á€…á€ºá€á€»á€¯á€•á€º",
    ]
    # åˆæˆå·®ç•°ã‚’é¿ã‘ã‚‹ãŸã‚ NFC ã«æ­£è¦åŒ–ã—ã¦ãŠã
    EXCLUDE_TITLE_KEYWORDS = [
        unicodedata.normalize("NFC", kw) for kw in EXCLUDE_TITLE_KEYWORDS
    ]

    def _remove_noise_phrases(text: str) -> str:
        """BBCç½²åãªã©ã®ãƒã‚¤ã‚ºãƒ•ãƒ¬ãƒ¼ã‚ºã‚’é™¤å»"""
        if not text:
            return text
        for pat in NOISE_PATTERNS:
            text = re.sub(pat, "", text, flags=re.IGNORECASE)
        return text.strip()

    # MEMO: ãƒ­ã‚°ç”¨
    # ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆä¸­ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã“ã«ãƒ’ãƒƒãƒˆã—ãŸã‹ã‚’è¿”ã™ï¼ˆå‘¨è¾ºæ–‡è„ˆã¤ãï¼‰
    # def _find_hits(text: str, keywords):
    #     hits = []
    #     for kw in keywords:
    #         start = 0
    #         while True:
    #             i = text.find(kw, start)
    #             if i == -1:
    #                 break
    #             s = max(0, i - 30)
    #             e = min(len(text), i + len(kw) + 30)
    #             ctx = text[s:e].replace("\n", " ")
    #             hits.append({"kw": kw, "pos": i, "ctx": ctx})
    #             start = i + len(kw)
    #     return hits

    rss_url = "https://feeds.bbci.co.uk/burmese/rss.xml"
    session = requests.Session()

    try:
        res = session.get(rss_url, timeout=10)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    soup = BeautifulSoup(res.content, "xml")
    articles = []

    for item in soup.find_all("item"):
        pub_date_tag = item.find("pubDate")
        if not pub_date_tag:
            continue

        # RSSã¯UTC â†’ MMTã¸å¤‰æ›ã—ã€å¯¾è±¡æ—¥ã ã‘é€šã™
        try:
            pub_date = parse_date(pub_date_tag.text)
            pub_date_mmt = pub_date.astimezone(MMT).date()
        except Exception as e:
            print(f"âŒ pubDate parse error: {e}")
            continue

        if pub_date_mmt != target_date_mmt:
            continue

        title = (
            (item.find("title") or {}).get_text(strip=True)
            if item.find("title")
            else ""
        )
        link = (
            (item.find("link") or {}).get_text(strip=True) if item.find("link") else ""
        )
        if not link:
            continue
        
        # === ã‚¿ã‚¤ãƒˆãƒ«é™¤å¤–ï¼ˆRSS ã‹ã‚‰å–å¾—ã—ãŸã‚¿ã‚¤ãƒˆãƒ«ã§å…ˆã«åˆ¤å®šã—ã¦æ—©æœŸã‚¹ã‚­ãƒƒãƒ—ï¼‰ ===
        rss_title_nfc = unicodedata.normalize("NFC", title or "")
        if any(kw in rss_title_nfc for kw in EXCLUDE_TITLE_KEYWORDS):
            print(f"SKIP: excluded title keyword (BBC) â†’ {link} | TITLE: {rss_title_nfc}")
            continue

        try:
            article_res = session.get(link, timeout=10)
            article_res.raise_for_status()
            article_soup = BeautifulSoup(article_res.content, "html.parser")

            # ===== ã“ã“ã§é™¤å¤–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã¾ã¨ã‚ã¦å‰Šé™¤ =====
            # è¨˜äº‹ç½²åã‚„ãƒ¡ã‚¿æƒ…å ±
            for node in article_soup.select(
                'section[role="region"][aria-labelledby="article-byline"]'
            ):
                node.decompose()
            # ã€ŒãŠã™ã™ã‚ï¼æœ€ã‚‚èª­ã¾ã‚ŒãŸã€ãƒ–ãƒ­ãƒƒã‚¯
            for node in article_soup.select(
                'section[data-e2e="recommendations-heading"][role="region"]'
            ):
                node.decompose()
            # ã¤ã„ã§ã«ãƒ˜ãƒƒãƒ€ãƒ¼/ãƒŠãƒ“/ãƒ•ãƒƒã‚¿ãƒ¼ç­‰ã®ãƒã‚¤ã‚ºã‚‚è½ã¨ã—ã¦ãŠãï¼ˆä»»æ„ï¼‰
            for node in article_soup.select(
                'header[role="banner"], nav[role="navigation"], footer[role="contentinfo"], aside'
            ):
                node.decompose()
            # ============================================

            # æœ¬æ–‡ã¯ main å†…ã® <p> ã«é™å®š
            main = article_soup.select_one('main[role="main"]') or article_soup
            paragraphs = [p.get_text(strip=True) for p in main.find_all("p")]
            # ç©ºè¡Œã‚„ãƒã‚¤ã‚ºã‚’å‰Šã‚‹
            paragraphs = [t for t in paragraphs if t]
            body_text = "\n".join(paragraphs)

            # ãƒŸãƒ£ãƒ³ãƒãƒ¼æ–‡å­—ã®åˆæˆå·®ç•°ã‚’é¿ã‘ã‚‹ãŸã‚ NFC æ­£è¦åŒ–
            title_nfc = unicodedata.normalize("NFC", title)
            title_nfc = _remove_noise_phrases(title_nfc)
            body_text_nfc = unicodedata.normalize("NFC", body_text)
            body_text_nfc = _remove_noise_phrases(body_text_nfc)

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
            if not any_keyword_hit(title_nfc, body_text_nfc):
                log_no_keyword_hit(
                    "BBC Burmese", link, title_nfc, body_text_nfc, "bbc:article"
                )
                continue

            # MEMO: ãƒ­ã‚°ç”¨ã€=== ãƒ‡ãƒãƒƒã‚°: åˆ¤å®šå‰ã«ã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡ã®è¦ç´„ã‚’å‡ºã™ ===
            # print("----- DEBUG CANDIDATE -----")
            # print("URL:", link)
            # print("TITLE:", repr(title_nfc))
            # print("BODY_HEAD:", repr(body_text_nfc[:500]))
            # print("BODY_LEN:", len(body_text_nfc))

            # # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆãƒ’ãƒƒãƒˆè©³ç´°ã‚‚å–ã‚‹ï¼‰
            # title_hits = _find_hits(title_nfc, NEWS_KEYWORDS)
            # body_hits  = _find_hits(body_text_nfc, NEWS_KEYWORDS)
            # total_hits = title_hits + body_hits

            # if not total_hits:
            #     print("SKIP: no keyword hits.")
            #     continue

            # # === ãƒ‡ãƒãƒƒã‚°: ã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã“ã§å½“ãŸã£ãŸã‹ ===
            # print("HITS:", len(total_hits))
            # if title_hits:
            #     print(" - in TITLE:")
            #     for h in title_hits[:10]:
            #         print(f"   kw={repr(h['kw'])} ctx=â€¦{h['ctx']}â€¦")
            # if body_hits:
            #     print(" - in BODY:")
            #     for h in body_hits[:10]:  # é•·ããªã‚‹ã®ã§æœ€å¤§10ä»¶
            #         print(f"   kw={repr(h['kw'])} ctx=â€¦{h['ctx']}â€¦")

            print(f"âœ… æŠ½å‡ºè¨˜äº‹: {title_nfc} ({link})")
            articles.append(
                {
                    "title": title_nfc,
                    "url": link,
                    "date": pub_date_mmt.isoformat(),
                    "source": "BBC Burmese",
                    "body": body_text_nfc,
                }
            )

        except Exception as e:
            print(f"âŒ è¨˜äº‹å–å¾—/è§£æã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return articles


# khit_thit_mediaã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸å·¡å›ã§å–å¾—
def get_khit_thit_media_articles_from_category(date_obj, max_pages=3):
    # è¿½åŠ ã‚«ãƒ†ã‚´ãƒªã‚’å«ã‚€å·¡å›å¯¾è±¡
    CATEGORY_URLS = [
        "https://yktnews.com/category/news/",
        "https://yktnews.com/category/politics/",
        "https://yktnews.com/category/editor-choice/",
        "https://yktnews.com/category/interview/",
        "https://yktnews.com/category/china-watch/",
    ]

    HASHTAG_TOKEN_RE = re.compile(
        r"(?:(?<=\s)|^)\#[^\s#]+"
    )  # ç©ºç™½orè¡Œé ­ã‹ã‚‰å§‹ã¾ã‚‹ #ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»ï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰

    def _remove_hashtag_links(soup):
        """
        <a>ã‚„<strong><a>â€¦</a></strong>ã®ã‚ˆã†ãªå…¥ã‚Œå­ã‚’å«ã‚ã€
        ãƒ†ã‚­ã‚¹ãƒˆãŒ '#' ã§å§‹ã¾ã‚‹ã‚¢ãƒ³ã‚«ãƒ¼ã‚’æœ¬æ–‡ã‹ã‚‰é™¤å»ã™ã‚‹ã€‚
        """
        # aè¦ç´ ã®ä¸­ã§å¯è¦–ãƒ†ã‚­ã‚¹ãƒˆãŒ '#' ã§å§‹ã¾ã‚‹ã‚‚ã®ã‚’ã¾ã‚‹ã”ã¨å‰Šé™¤
        for a in soup.select("a"):
            txt = a.get_text(strip=True)
            if txt.startswith("#"):
                a.decompose()

    collected_urls = set()
    for base_url in CATEGORY_URLS:
        for page in range(1, max_pages + 1):
            url = f"{base_url}page/{page}/" if page > 1 else base_url
            print(f"Fetching {url}")
            try:
                res = fetch_with_retry(url)
            except Exception as e:
                print(f"[khitthit] stop pagination (missing/unreachable): {url} -> {e}")
                break

            soup = BeautifulSoup(res.content, "html.parser")
            entry_links = soup.select("p.entry-title.td-module-title a[href]")
            if not entry_links:
                print(f"[khitthit] stop pagination (no entries): {url}")
                break

            for a in entry_links:
                href = a.get("href")
                if not href:
                    continue
                if href in collected_urls:  # â† æ—¢å‡ºURLã¯æ˜ç¤ºã‚¹ã‚­ãƒƒãƒ—
                    continue
                collected_urls.add(href)

    filtered_articles = []
    for url in collected_urls:
        try:
            res_article = fetch_with_retry(url)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            # æ—¥ä»˜å–å¾—
            meta_tag = soup_article.find("meta", property="article:published_time")
            if not meta_tag or not meta_tag.has_attr("content"):
                continue
            date_str = meta_tag["content"]
            article_datetime_utc = datetime.fromisoformat(date_str)
            article_datetime_mmt = article_datetime_utc.astimezone(MMT)
            article_date = article_datetime_mmt.date()
            if article_date != date_obj:
                continue  # å¯¾è±¡æ—¥ã§ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_tag = soup_article.find("h1")
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)

            # æœ¬æ–‡å–å¾—  â† ã“ã®ç›´å‰ã« â€œãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°é™¤å»â€ ã‚’å·®ã—è¾¼ã‚€
            _remove_hashtag_links(soup_article)  # â‘  HTMLæ®µéšã§ #ã‚¢ãƒ³ã‚«ãƒ¼ã‚’é™¤å»
            paragraphs = extract_paragraphs_with_wait(soup_article)
            # â‘¡ ãƒ†ã‚­ã‚¹ãƒˆåŒ–å¾Œã‚‚ä¿é™ºã§ #ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
            body_text = "\n".join(
                HASHTAG_TOKEN_RE.sub("", p.get_text(strip=True)).strip()
                for p in paragraphs
                if p.get_text(strip=True)  # ç©ºãƒ‘ãƒ©ã¯ãã‚‚ãã‚‚æ¨ã¦ã‚‹
            )
            body_text = unicodedata.normalize("NFC", body_text)
            if not body_text.strip():
                continue  # æœ¬æ–‡ãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—

            if not any_keyword_hit(title, body_text):
                log_no_keyword_hit(
                    "Khit Thit Media", url, title, body_text, "khitthit:category"
                )
                continue  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç„¡ã—ã¯é™¤å¤–

            filtered_articles.append(
                {
                    "url": url,
                    "title": title,
                    "date": date_obj.isoformat(),
                    "source": "Khit Thit Media",  # deduplicate_by_urlã®ãƒ­ã‚°ã§ä½¿ã‚ã‚Œã‚‹
                    "body": body_text,
                }
            )
        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    before = len(filtered_articles)
    filtered_articles = deduplicate_by_url(filtered_articles)
    print(f"[khitthit] dedup: {before} -> {len(filtered_articles)}")  # æœ€å°ãƒ­ã‚°

    return filtered_articles


# irrawaddy
def get_irrawaddy_articles_for(date_obj, debug=True):
    """
    æŒ‡å®šã® Irrawaddy ã‚«ãƒ†ã‚´ãƒªURLç¾¤ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ï¼‰ã‚’1å›ãšã¤å·¡å›ã—ã€
    MMTã®æŒ‡å®šæ—¥(æ—¢å®š: ä»Šæ—¥)ã«ãƒ’ãƒƒãƒˆã™ã‚‹è¨˜äº‹ã®ã¿è¿”ã™ã€‚
    ã•ã‚‰ã«ãƒ›ãƒ¼ãƒ  https://www.irrawaddy.com/ ã®
    data-id="kuDRpuo" ã‚«ãƒ©ãƒ å†…ã‹ã‚‰ã‚‚åŒæ§˜ã«å€™è£œåé›†ã™ã‚‹ã€‚

    - /category/news/asia, /category/news/world ã¯é™¤å¤–ï¼ˆå…ˆé ­ä¸€è‡´ãƒ»å¤§å°ç„¡è¦–ï¼‰
    - ä¸€è¦§ã§ã¯ã€Œæ™‚è¨ˆã‚¢ã‚¤ã‚³ãƒ³ä»˜ãã®æ—¥ä»˜ãƒªãƒ³ã‚¯ã€ã‹ã‚‰å½“æ—¥å€™è£œã‚’æŠ½å‡º
    - è¨˜äº‹å´ã§ã¯ <meta property="article:published_time"> ã‚’ MMT ã«å¤‰æ›ã—ã¦å†ç¢ºèª
    - æœ¬æ–‡ã¯ <div class="content-inner "> é…ä¸‹ã® <p> ã‹ã‚‰æŠ½å‡ºï¼ˆç‰¹å®šãƒ–ãƒ­ãƒƒã‚¯é…ä¸‹ã¯é™¤å¤–ï¼‰
    è¿”ã‚Šå€¤: [{url, title, date}]
    ä¾å­˜: MMT, get_today_date_mmt, fetch_with_retry, any_keyword_hit
    """

    session = requests.Session()

    # ==== å·¡å›å¯¾è±¡ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã€é‡è¤‡ã‚ã‚Šã§ã‚‚OKï¼šå†…éƒ¨ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰ ====
    CATEGORY_PATHS_RAW = [
        "/category/news/",
        "/category/politics",
        "/category/news/war-against-the-junta",
        "/category/news/conflicts-in-numbers",
        "/category/news/junta-crony",
        "/category/news/ethnic-issues",
        "/category/business",
        "/category/business/economy",
        "/category/Features",
        "/category/Opinion",
        "/category/Opinion/editorial",
        "/category/Opinion/commentary",
        "/category/Opinion/guest-column",
        "/category/Opinion/analysis",
        "/category/in-person",
        "/category/in-person/interview",
        "/category/in-person/profile",
        "/category/Specials",
        "/category/specials/women",
        "/category/from-the-archive",
        "/category/Specials/myanmar-china-watch",
        # "/category/Video" # é™¤å¤–ä¾é ¼æœ‰
        # "/category/culture/books" #é™¤å¤–ä¾é ¼æœ‰
        # "/category/Cartoons" # é™¤å¤–ä¾é ¼æœ‰
        # "/category/election-2020", # 2021å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Opinion/letters", # 2014å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Dateline", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/specials/places-in-history", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/specials/on-this-day", # 2023å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Specials/myanmar-covid-19", # 2022å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Travel", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle/Food", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle/fashion-design", # 2019å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/photo", # 2016å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/photo-essay", # 2021å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
    ]
    BASE = "https://www.irrawaddy.com"
    EXCLUDE_PREFIXES = [
        "/category/news/asia",  # é™¤å¤–ä¾é ¼æœ‰
        "/category/news/world",  # é™¤å¤–ä¾é ¼æœ‰
        "/video",  # "/category/Video"ã¯é™¤å¤–å¯¾è±¡ã ãŒã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ã‚ã‚‹
        "/cartoons",  # "/category/Cartoons"ã¯é™¤å¤–å¯¾è±¡ã ãŒã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ã‚ã‚‹
    ]  # å…ˆé ­ä¸€è‡´ãƒ»å¤§å°ç„¡è¦–

    def _is_excluded_url(href: str) -> bool:
        try:
            p = urlparse(href or "").path.lower()
        except Exception:
            p = (href or "").lower()
        return any(p.startswith(x) for x in EXCLUDE_PREFIXES)

    # ==== æ­£è¦åŒ–ãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ãƒ»é™¤å¤– ====
    def _norm(p: str) -> str:
        return re.sub(r"/{2,}", "/", p.strip())

    paths, seen = [], set()
    for p in CATEGORY_PATHS_RAW:
        q = _norm(p)
        if any(q.lower().startswith(x) for x in EXCLUDE_PREFIXES):
            continue
        if q not in seen:
            seen.add(q)
            paths.append(q)

    # 2) ç°¡æ˜“ãƒ­ã‚¬ãƒ¼ï¼ˆæ¶ˆã™æ™‚ã¯ã“ã®1è¡Œã¨ dbg(...) ã‚’æ¶ˆã™ã ã‘ï¼‰
    dbg = (lambda *a, **k: print(*a, **k)) if debug else (lambda *a, **k: None)

    results = []
    seen_urls = set()
    candidate_urls = []
    fallback_titles: Dict[str, str] = {}

    # ==== 1) å„ã‚«ãƒ†ã‚´ãƒªURLã‚’1å›ãšã¤å·¡å› â†’ å½“æ—¥å€™è£œæŠ½å‡º ====
    for rel_path in paths:
        url = f"{BASE}{rel_path}"
        # print(f"Fetching {url}")
        try:
            res = fetch_with_retry_irrawaddy(url, session=session)
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            continue

        soup = BeautifulSoup(res.content, "html.parser")
        wrapper = soup.select_one("div.jeg_content")  # ãƒ†ãƒ¼ãƒã«ã‚ˆã£ã¦ã¯ç„¡ã„ã“ã¨ã‚‚ã‚ã‚‹

        # âœ… union æ–¹å¼ï¼šwrapper å†…â†’è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒšãƒ¼ã‚¸å…¨ä½“ã®é †ã§æ¢ç´¢
        scopes = ([wrapper] if wrapper else []) + [soup]

        for scope in scopes:
            # ãƒ’ãƒ¼ãƒ­ãƒ¼æ ï¼‹é€šå¸¸ãƒªã‚¹ãƒˆï¼‹æ±ç”¨ãƒ¡ã‚¿ã‚’ä¸€ç™ºã§æ‹¾ã†
            links = scope.select(
                ".jnews_category_hero_container .jeg_meta_date a[href], "
                "div.jeg_postblock_content .jeg_meta_date a[href], "
                ".jeg_post_meta .jeg_meta_date a[href]"
            )
            # æ™‚è¨ˆã‚¢ã‚¤ã‚³ãƒ³ä»˜ãã ã‘ã«é™å®šï¼ˆãƒã‚¤ã‚ºå›é¿ï¼‰
            links = [a for a in links if a.find("i", class_="fa fa-clock-o")]

            # ï¼ˆä»»æ„ï¼‰ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
            # dbg(f"[cat] union-links={len(links)} @ {url}")
            for a in links[:2]:
                _txt = re.sub(r"\s+", " ", a.get_text(" ", strip=True))
                # dbg("   â†’", _txt, "|", a.get("href"))

            found = 0
            for a in links:
                href = a.get("href") or ""
                raw = a.get_text(" ", strip=True)
                try:
                    shown_date = _parse_category_date_text(raw)
                except Exception:
                    # å¿…è¦æœ€å°é™ã®ãƒ‡ãƒãƒƒã‚°ã ã‘
                    # dbg("[cat] date-parse-fail:", re.sub(r"\s+", " ", raw)[:120])
                    continue

                # â–¼ ã“ã“ã§ /video ãªã©ã‚’é™¤å¤–
                if _is_excluded_url(href):
                    continue

                if shown_date == date_obj and href and href not in seen_urls:
                    candidate_urls.append(href)
                    seen_urls.add(href)
                    found += 1

            # wrapper å†…ã§â€œå½“æ—¥â€ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ soup ã¾ã§åºƒã’ãšçµ‚äº†ã€‚
            # wrapper ãŒç„¡ã„å ´åˆï¼ˆscopes ãŒ [soup] ã ã‘ã®æ™‚ï¼‰ã‚‚1å‘¨ã§æŠœã‘ã‚‹ã€‚
            if found > 0:
                # dbg(f"[cat] STOP (added {found} candidates) @ {url}")
                break

    # ==== 1.5) ãƒ›ãƒ¼ãƒ ï¼ˆkuDRpuoã‚«ãƒ©ãƒ ï¼‰å·¡å› â†’ å½“æ—¥å€™è£œæŠ½å‡ºï¼ˆæ–°è¦ï¼‰ ====
    try:
        home_url = f"{BASE}/"
        res_home = fetch_with_retry_irrawaddy(home_url, session=session)
        soup_home = BeautifulSoup(res_home.content, "html.parser")

        # data-id ã§ã‚¹ã‚³ãƒ¼ãƒ—ç‰¹å®šï¼ˆclass ã§ã‚‚æ‹¾ãˆã‚‹ã‚ˆã†ã«å†—é•·åŒ–ï¼‰
        home_scope = soup_home.select_one(
            'div.elementor-element-kuDRpuo[data-id="kuDRpuo"], '
            "div.elementor-element-kuDRpuo, "
            '[data-id="kuDRpuo"]'
        )

        if home_scope:
            links = home_scope.select(".jeg_meta_date a[href]")
            links = [a for a in links if a.find("i", class_="fa fa-clock-o")]
            for a in links:
                href = a.get("href") or ""
                raw = a.get_text(" ", strip=True)
                try:
                    shown_date = _parse_category_date_text(raw)
                except Exception:
                    continue

                # â–¼ ã“ã“ã§ã‚‚é™¤å¤–
                if _is_excluded_url(href):
                    continue

                if shown_date == date_obj and href and href not in seen_urls:
                    candidate_urls.append(href)
                    seen_urls.add(href)
    except Exception as e:
        print(f"Error scanning homepage column kuDRpuo: {e}")

    # ãƒ­ã‚°ã€å€™è£œURLåé›†ãŒçµ‚ã‚ã£ãŸç›´å¾Œï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‹ãƒ›ãƒ¼ãƒ çµ±åˆã®ã‚ã¨ï¼‰
    dbg(f"[irrawaddy] candidates={len(candidate_urls)} (unique)")

    # ==== 1.9) ãƒ•ã‚£ãƒ¼ãƒ‰/å¤–éƒ¨RSSãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆGitHub Actionsç­‰ã§403ãŒç¶šãå ´åˆï¼‰ ====
    def _mmt_date(dt: datetime) -> date:
        try:
            return dt.astimezone(MMT).date()
        except Exception:
            # naive ã®å ´åˆã¯UTCâ†’MMTæ›ç®—ã¨ã¿ãªã™
            return (dt.replace(tzinfo=timezone.utc)).astimezone(MMT).date()

    def _fetch_text(url: str, timeout: int = 20) -> str:
        try:
            r = requests.get(
                url,
                headers={
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/128.0.0.0 Safari/537.36"
                    )
                },
                timeout=timeout,
            )
            if r.status_code == 200 and (r.text or "").strip():
                return r.text
        except Exception:
            pass
        return ""

    def _rss_items_from_google_news() -> List[Dict[str, str]]:
        # Google News RSSï¼ˆIrrawaddyé™å®šï¼‰
        gnews = (
            "https://news.google.com/rss/search?"  
            "q=site:irrawaddy.com&hl=en-US&gl=US&ceid=US:en"
        )
        xml = _fetch_text(gnews)
        if not xml:
            return []
        try:
            root = ET.fromstring(xml)
        except Exception:
            return []
        items = []
        for it in root.findall(".//item"):
            title = (it.findtext("title") or "").strip()
            link = (it.findtext("link") or "").strip()
            pub = (it.findtext("pubDate") or "").strip()
            if not link:
                continue
            items.append({"title": title, "link": link, "pubDate": pub})
        return items

    def _parse_rfc822_date(s: str) -> Optional[datetime]:
        s = (s or "").strip()
        if not s:
            return None
        try:
            return parse_date(s)
        except Exception:
            return None

    def _fallback_candidates_via_feeds() -> List[Dict[str, str]]:
        # 1) WordPress JSONï¼ˆå¤šãã®å ´åˆWAFå¯¾è±¡ï¼‰
        wp_json = _fetch_text(
            "https://www.irrawaddy.com/wp-json/wp/v2/posts?per_page=50&_fields=link,date"
        )
        cands: List[Dict[str, str]] = []
        if wp_json:
            try:
                arr = json.loads(wp_json)
                for o in arr:
                    link = (o.get("link") or "").strip()
                    ds = o.get("date") or ""
                    dt = _parse_rfc822_date(ds) or (
                        parse_date(ds) if ds else None
                    )
                    if link and dt and _mmt_date(dt) == date_obj and not _is_excluded_url(link):
                        cands.append({"url": link, "title": "", "date": date_obj.isoformat()})
            except Exception:
                pass

        # 2) ã‚µã‚¤ãƒˆRSSï¼ˆ403ã®å¯èƒ½æ€§ã‚ã‚Š â†’ å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if not cands:
            feed_xml = _fetch_text("https://www.irrawaddy.com/feed")
            if feed_xml:
                try:
                    root = ET.fromstring(feed_xml)
                    for it in root.findall(".//item"):
                        title = (it.findtext("title") or "").strip()
                        link = (it.findtext("link") or "").strip()
                        pub = (it.findtext("pubDate") or "").strip()
                        dt = _parse_rfc822_date(pub)
                        if link and dt and _mmt_date(dt) == date_obj and not _is_excluded_url(link):
                            cands.append({
                                "url": link,
                                "title": title,
                                "date": date_obj.isoformat(),
                            })
                except Exception:
                    pass

        # 3) Google News RSSï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not cands:
            for it in _rss_items_from_google_news():
                link = it.get("link") or ""
                title = it.get("title") or ""
                pub = it.get("pubDate") or ""
                dt = _parse_rfc822_date(pub)
                if link and dt and _mmt_date(dt) == date_obj and not _is_excluded_url(link):
                    cands.append({
                        "url": link,
                        "title": title,
                        "date": date_obj.isoformat(),
                    })

        # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
        seen = set()
        uniq = []
        for o in cands:
            u = o["url"].strip()
            if u and u not in seen:
                seen.add(u)
                uniq.append(o)
        return uniq

    if len(candidate_urls) == 0:
        dbg("[irrawaddy] fallback to RSS/Google News due to 0 candidates")
        feed_cands = _fallback_candidates_via_feeds()
        # æ—¢å­˜ãƒ‘ã‚¹ã¨åŒã˜å½¢å¼ã«åˆã‚ã›ã‚‹
        for o in feed_cands:
            u = o.get("url") or ""
            if u and u not in seen_urls:
                candidate_urls.append(u)
                seen_urls.add(u)
                if o.get("title"):
                    fallback_titles[u] = o.get("title") or ""

    # ==== 2) å€™è£œè¨˜äº‹ã§å³å¯†ç¢ºèªï¼ˆmetaæ—¥ä»˜/æœ¬æ–‡/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰ ====
    for url in candidate_urls:
        if _is_excluded_url(url):  # ãƒ™ãƒ«ãƒˆï¼†ã‚µã‚¹ãƒšãƒ³ãƒ€ãƒ¼
            continue
        try:
            title = ""
            body = ""
            # â‘  ç›´æ¥å–å¾—ï¼ˆ1å›ã ã‘ï¼‰
            try:
                html_once = _fetch_text(url, timeout=20)
                if html_once:
                    soup_article = BeautifulSoup(html_once, "html.parser")
                    # ãƒ¡ã‚¿æ—¥ä»˜ã§å½“æ—¥ä»¥å¤–ã¯é™¤å¤–
                    if _article_date_from_meta_mmt(soup_article) != date_obj:
                        continue
                    title = _extract_title(soup_article) or ""
                    body = extract_body_irrawaddy(soup_article) or ""
            except Exception:
                pass

            # â‘¡ ç›´æ¥å–å¾—ã§ããªã„å ´åˆã€r.jina.ai çµŒç”±ã®æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not body:
                def _jina_fetch(u: str) -> str:
                    # r.jina.ai ã¯ Readability æŠ½å‡ºã—ãŸãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
                    alt = f"https://r.jina.ai/http://{u.lstrip('/') }"
                    t = _fetch_text(alt, timeout=25)
                    if not t and "/news/" in u:
                        # AMP ã‚’è©¦ã™
                        amp = u if u.endswith("/amp") else urljoin(u.rstrip("/") + "/", "amp")
                        alt2 = f"https://r.jina.ai/http://{amp.lstrip('/') }"
                        t = _fetch_text(alt2, timeout=25)
                    return t

                body_txt = _jina_fetch(url)
                if body_txt:
                    body = body_txt
                    if not title:
                        # ãƒ•ã‚£ãƒ¼ãƒ‰ã§æ‹¾ã£ãŸã‚¿ã‚¤ãƒˆãƒ«ã‚’æœ€çµ‚æ‰‹æ®µã¨ã—ã¦æµç”¨
                        title = fallback_titles.get(url, "")

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»–åª’ä½“ã¨åŒæ§˜ï¼‰è¿½åŠ 
            # å¿µã®ãŸã‚ã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡ã‚’NFCæ­£è¦åŒ–ã—ã¦ã‹ã‚‰åˆ¤å®š
            title_nfc = unicodedata.normalize("NFC", title)
            body_nfc = unicodedata.normalize("NFC", body)
            if not any_keyword_hit(title_nfc, body_nfc):
                # ãƒ­ã‚°ã‚’å‡ºã—ã¦ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ¬æ–‡æŠœç²‹ã¯å‡ºã•ãªã„å…±é€šãƒ­ã‚¬ãƒ¼ï¼‰
                log_no_keyword_hit("Irrawaddy", url, title_nfc, body_nfc, "irrawaddy:article")
                continue

            results.append(
                {
                    "url": url,
                    "title": title_nfc,
                    "date": date_obj.isoformat(),
                    "body": body_nfc,
                    "source": "body_nfc",  # é‡è¤‡å‰Šé™¤é–¢æ•°ã‚’ä½¿ã†ãŸã‚è¿½åŠ 
                }
            )
        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    # ==== 3) æœ€çµ‚é‡è¤‡æ’é™¤ï¼ˆURLã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ãƒ»å…ˆå‹ã¡ï¼‰ ====
    before_dedup = len(results)
    results = deduplicate_by_url(results)

    # ãƒ­ã‚°ã€é‡è¤‡å‰Šé™¤ä»¶æ•°
    dbg(f"[irrawaddy] dedup: {before_dedup} -> {len(results)}")

    # ãƒ­ã‚°ã€æœ€çµ‚çš„ãªresultã®ä¸­èº«
    dbg(f"[irrawaddy] kept={len(results)}")

    def _one(s: str, n: int = 60) -> str:
        s = re.sub(r"\s+", " ", (s or "")).strip()
        return s[:n]

    for r in results[:3]:
        dbg(f"  - {_one(r.get('title'))} | {r.get('url')}")
    if len(results) > 3:
        dbg(f"  ... (+{len(results)-3} more)")

    return results


# DVB
def get_dvb_articles_for(date_obj: date, debug: bool = True) -> List[Dict]:
    """
    - /category/... ã®ä¸€è¦§ï¼ˆ1ãƒšãƒ¼ã‚¸ç›®ï¼‹?page=2ï¼‰ã‹ã‚‰ã€æŒ‡å®šæ—¥ã¨ä¸€è‡´ã™ã‚‹ã‚«ãƒ¼ãƒ‰ã ã‘å€™è£œåŒ–ã€‚
    - è¨˜äº‹ãƒšãƒ¼ã‚¸ã§ã¯ <title> / .full_content p ã‚’æŠ½å‡ºã€‚
    - ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ã‚’NFCæ­£è¦åŒ–ã—ã¦ any_keyword_hit ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
    - è¿”ã‚Šå€¤: [{url, title, date, body, source}]
    â€» DVBå°‚ç”¨ fetch_with_retry_dvb ã‚’ä½¿ç”¨ã€‚
    ä»¥ä¸‹3ã‚«ãƒ†ã‚´ãƒªä»¥å¤–ã®è¨˜äº‹ã¯ã€ã™ã¹ã¦/category/8/newsã«å«ã¾ã‚Œã¦ã„ã‚‹ã€‚
    - /category/1799/international-news
    - /category/1793/sports-news
    - /category/6/features
    å½“è©²3ã‚«ãƒ†ã‚´ãƒªã¯é™¤å¤–ã—ãŸã„ã‚°ãƒ«ãƒ¼ãƒ—ã«ãªã‚‹ã®ã§/category/8/newsã®ã¿ã‚’å–å¾—å¯¾è±¡ã¨ã™ã‚‹ã€‚
    """
    BASE = "https://burmese.dvb.no"
    CATEGORY_PATHS = [
        "/category/8/news",
        # "/category/17/news_politics-new",
        # "/category/16/news_economics-new",
        # "/category/15/news_health-news-news",
        # "/category/18/news_social-news",
        # "/category/1787/news_education-news",
        # "/category/10/news_environment-weather",
        # "/category/1789/news_labour-news",
        # "/category/1788/news_farmers-news",
        # "/category/1797/news_criminals-news",
        # "/category/9/news_media-news",
        # "/category/6/features",
        # "/category/13/interview",
        # "/category/1799/international-news",
        # "/category/1793/sports-news",
    ]

    def _norm_path(p: str) -> str:
        return re.sub(r"/{2,}", "/", (p or "").strip())

    def _parse_dvb_date(text: str) -> Optional[date]:
        if not text:
            return None
        s = re.sub(r"\s+", " ", text.strip())
        try:
            return datetime.strptime(s, "%B %d, %Y").date()
        except ValueError:
            return None

    def _extract_title_dvb(soup: BeautifulSoup) -> str:
        t = (soup.title.string or "").strip() if soup.title else ""
        if t:
            return t
        h = soup.select_one(".text-2xl, h1, .post-title")
        return (h.get_text(" ", strip=True) if h else "").strip()

    def _extract_body_dvb(soup: BeautifulSoup) -> str:
        host = soup.select_one(".full_content")
        if not host:
            return ""
        parts = []
        for p in host.select("p"):
            txt = p.get_text(" ", strip=True)
            txt = re.sub(r"\s+", " ", txt)
            if txt:
                parts.append(txt)
        return "\n".join(parts).strip()

    log = (lambda *a, **k: print(*a, **k)) if debug else (lambda *a, **k: None)
    results: List[Dict] = []
    candidate_urls: List[str] = []
    seen_urls = set()

    # å…±æœ‰ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆcookies/æŒ‡ç´‹ã‚’ä¸€è¦§â†’è¨˜äº‹ã§å¼•ãç¶™ãï¼‰
    try:
        sess = requests.Session()
    except Exception:
        sess = None

    # ---- 1) ã‚«ãƒ†ã‚´ãƒªä¸€è¦§å·¡å›ï¼ˆå„ã‚«ãƒ†ã‚´ãƒªã«ã¤ã page=1,2ï¼‰
    for rel in CATEGORY_PATHS:
        rel = _norm_path(rel)
        for page_no in (1, 2):
            url = f"{BASE}{rel}" if page_no == 1 else f"{BASE}{rel}?page=2"
            try:
                res = fetch_with_retry_dvb(url, retries=4, wait_seconds=2, session=sess)
            except Exception as e:
                log(f"[warn] fetch fail {url}: {e}")
                continue

            if getattr(res, "status_code", 200) != 200:
                log(f"[skip] non-200 ({res.status_code}) {url}")
                continue

            soup = BeautifulSoup(
                getattr(res, "content", None) or res.text, "html.parser"
            )

            # ä¸€è¦§ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆç‰¹å¾´ã§ç‰¹å®šã€‚ç„¡ã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ãƒšãƒ¼ã‚¸å…¨ä½“ï¼‰
            blocks = soup.select(
                "div.md\\:grid.grid-cols-3.gap-4.mt-5, div.grid.grid-cols-3.gap-4.mt-5"
            ) or [soup]

            found = 0
            for scope in blocks:
                anchors = scope.select('a[href^="/post/"]')
                for a in anchors:
                    href = a.get("href") or ""
                    # ç¬¬ä¸€å€™è£œï¼šã‚«ãƒ¼ãƒ‰å†…ã® date ãƒ–ãƒ­ãƒƒã‚¯
                    date_div = a.select_one(
                        "div.flex.gap-1.text-xs.mt-2.text-gray-500 div"
                    )
                    date_text = (
                        date_div.get_text(" ", strip=True) if date_div else ""
                    ).strip()
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šè‹±èªæœˆåãƒ‘ã‚¿ãƒ¼ãƒ³
                    if not date_text:
                        full = a.get_text(" ", strip=True)
                        m = re.search(
                            r"(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s*\d{4}",
                            full,
                        )
                        date_text = m.group(0) if m else ""
                    d = _parse_dvb_date(date_text)
                    if d and d == date_obj:
                        uabs = href if href.startswith("http") else f"{BASE}{href}"
                        if uabs not in seen_urls:
                            candidate_urls.append(uabs)
                            seen_urls.add(uabs)
                            found += 1
            log(f"[list] {url} -> candidates+{found}")

    log(f"[dvb] candidates total = {len(candidate_urls)} (unique)")

    # ---- 2) å€™è£œè¨˜äº‹ãƒšãƒ¼ã‚¸ã§æŠ½å‡ºï¼ˆany_keyword_hit ã§çµã‚Šè¾¼ã¿ï¼‰
    for url in candidate_urls:
        try:
            res = fetch_with_retry_dvb(url, retries=4, wait_seconds=2, session=sess)
            if getattr(res, "status_code", 200) != 200:
                log(f"[skip] non-200 article {res.status_code} {url}")
                continue
            soup = BeautifulSoup(
                getattr(res, "content", None) or res.text, "html.parser"
            )

            title = _extract_title_dvb(soup)
            body = _extract_body_dvb(soup)
            if not title or not body:
                log(f"[skip] empty title/body {url}")
                continue

            title_nfc = unicodedata.normalize("NFC", title)
            body_nfc = unicodedata.normalize("NFC", body)
            if not any_keyword_hit(title_nfc, body_nfc):
                log_no_keyword_hit("DVB", url, title_nfc, body_nfc, "dvb:article")
                continue

            results.append(
                {
                    "url": url,
                    "title": title_nfc,
                    "date": date_obj.isoformat(),
                    "body": body_nfc,
                    "source": "dvb",
                }
            )
        except Exception as e:
            log(f"[warn] article fail {url}: {e}")
            continue

    # ---- 3) é‡è¤‡æ’é™¤
    before = len(results)
    results = deduplicate_by_url(results)
    log(f"[dvb] dedup: {before} -> {len(results)}")

    # ---- 4) ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºï¼ˆå…ˆé ­æ•°ä»¶ï¼‰
    def _one(s: str, n: int = 60) -> str:
        return re.sub(r"\s+", " ", (s or "").strip())[:n]

    for r in results[:3]:
        log(f"  - {_one(r.get('title'))} | {r.get('url')}")
    if len(results) > 3:
        log(f"  ... (+{len(results)-3} more)")

    return results

# Myanmar Now (mm) 
def get_myanmar_now_articles_mm(date_obj, max_pages=3):
    """
    Myanmar Now (mm) ã®å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰å¯¾è±¡æ—¥ã®è¨˜äº‹ã‚’å–å¾—ã—ã¦è¿”ã™ã€‚
    - ã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã‚’æœ€å¤§ max_pages ãƒšãƒ¼ã‚¸å·¡å›
    - ä¸€è¦§ã§ã¯ <span class="date meta-item tie-icon">Month D, YYYY</span> ã‚’è¦‹ã¦ä»Šæ—¥ã ã‘æŠ½å‡º
    - å€‹åˆ¥è¨˜äº‹ã§ã¯ <meta property="article:published_time" content="..."> ã‚’UTCâ†’MMTå¤‰æ›ã—ã¦æœ€çµ‚ç¢ºèª
    - ã‚¿ã‚¤ãƒˆãƒ«æœ«å°¾ã® " - Myanmar Now" ã‚’é™¤å»
    - æœ¬æ–‡ã¯ div.entry-content.entry.clearfix å†…ã® <p> ã ã‘ï¼ˆç”»åƒç­‰ã¯å«ã‚ãªã„ï¼‰
    è¿”ã‚Šå€¤: list[dict] {url, title, date(ISO str, MMT), body, source="Myanmar Now (mm)"}
    """

    BASE_CATEGORIES = [
        "https://myanmar-now.org/mm/news/category/news/",                 # ãƒ‹ãƒ¥ãƒ¼ã‚¹
        "https://myanmar-now.org/mm/news/category/news/3/",               # æ”¿æ²»
        "https://myanmar-now.org/mm/news/category/news/17/",              # çµŒæ¸ˆ
        "https://myanmar-now.org/mm/news/category/news/social-issue/",    # ç¤¾ä¼š
        "https://myanmar-now.org/mm/news/category/news/19/",              # æ•™è‚²
        "https://myanmar-now.org/mm/news/category/news/international-news/",  # å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹
        "https://myanmar-now.org/mm/news/category/multimedia/16/",        # å¥åº·
        "https://myanmar-now.org/mm/news/category/in-depth/",             # ç‰¹é›†è¨˜äº‹
        "https://myanmar-now.org/mm/news/category/in-depth/analysis/",    # åˆ†æ
        "https://myanmar-now.org/mm/news/category/in-depth/investigation/", # èª¿æŸ»å ±é“
        "https://myanmar-now.org/mm/news/category/in-depth/profile/",     # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«
        "https://myanmar-now.org/mm/news/category/in-depth/society/",     # ç¤¾ä¼šåˆ†é‡
        "https://myanmar-now.org/mm/news/category/opinion/",              # è«–èª¬
        "https://myanmar-now.org/mm/news/category/opinion/commentary/",   # è«–è©•
        "https://myanmar-now.org/mm/news/category/opinion/29/",           # ç·¨é›†é•·ã®è«–èª¬
        "https://myanmar-now.org/mm/news/category/opinion/interview/",    # ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
        # "https://myanmar-now.org/mm/news/category/opinion/essay/",        # ã‚¨ãƒƒã‚»ã‚¤
        # "https://myanmar-now.org/mm/news/category/opinion/26/",           # é¢¨åˆº
        # "https://myanmar-now.org/mm/news/category/multimedia/video/",     # å‹•ç”»ãƒ‹ãƒ¥ãƒ¼ã‚¹
        # "https://myanmar-now.org/mm/news/category/multimedia/13/",        # ãƒ•ã‚©ãƒˆã‚¨ãƒƒã‚»ã‚¤
    ]

    # "September 8, 2025" ã®ã‚ˆã†ãªè‹±èªè¡¨è¨˜
    today_label = f"{date_obj.strftime('%B')} {date_obj.day}, {date_obj.year}"

    def _strip_source_suffix(title: str) -> str:
        if not title:
            return title
        return re.sub(r"\s*-\s*Myanmar Now\s*$", "", title).strip()

    def _collect_article_urls_from_category(cat_url: str) -> set[str]:
        urls = set()
        for page in range(1, max_pages + 1):
            url = f"{cat_url}page/{page}/" if page > 1 else cat_url
            try:
                res = fetch_with_retry(url)
            except Exception:
                break
            soup = BeautifulSoup(res.content, "html.parser")

            for span in soup.select("span.date.meta-item.tie-icon"):
                if (span.get_text(strip=True) or "") != today_label:
                    continue
                a = span.find_parent("a", href=True)
                if not a:
                    parent_a = span
                    while parent_a and parent_a.name != "a":
                        parent_a = parent_a.parent
                    if parent_a and parent_a.name == "a" and parent_a.get("href"):
                        a = parent_a
                if a and a.get("href"):
                    href = a["href"]
                    if "/mm/news/" in href:
                        urls.add(href)
        return urls

    collected = set()
    for base in BASE_CATEGORIES:
        collected |= _collect_article_urls_from_category(base)

    results = []
    for url in collected:
        try:
            res = fetch_with_retry(url)
            soup = BeautifulSoup(res.content, "html.parser")

            # 1) å…¬é–‹æ—¥æ™‚ã‚’ãƒã‚§ãƒƒã‚¯
            meta = soup.find("meta", attrs={"property": "article:published_time"})
            if not meta or not meta.get("content"):
                continue
            try:
                dt_utc = datetime.fromisoformat(meta["content"])
            except Exception:
                dt_utc = parse_date(meta["content"]).astimezone(timezone.utc)
            dt_mmt = dt_utc.astimezone(MMT)
            if dt_mmt.date() != date_obj:
                continue

            # 2) ã‚¿ã‚¤ãƒˆãƒ«
            title_raw = (soup.title.get_text(strip=True) if soup.title else "").strip()
            title = _strip_source_suffix(unicodedata.normalize("NFC", title_raw))
            if not title:
                h1 = soup.find("h1")
                if h1:
                    title = _strip_source_suffix(unicodedata.normalize("NFC", h1.get_text(strip=True)))
            if not title:
                continue

            # 3) æœ¬æ–‡
            body_parts = []
            content_root = soup.select_one("div.entry-content.entry.clearfix") or soup
            for p in content_root.find_all("p"):
                txt = p.get_text(strip=True)
                if txt:
                    body_parts.append(txt)
            body = unicodedata.normalize("NFC", "\n".join(body_parts).strip())
            if not body:
                paragraphs = extract_paragraphs_with_wait(soup)
                body = unicodedata.normalize("NFC", "\n".join(
                    p.get_text(strip=True) for p in paragraphs if getattr(p, "get_text", None)
                )).strip()
            if not body:
                continue

            # 4) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
            if not any_keyword_hit(title, body):
                log_no_keyword_hit("Myanmar Now (mm)", url, title, body, "mnw:article")
                continue

            results.append({
                "url": url,
                "title": title,
                "date": dt_mmt.isoformat(),
                "body": body,
                "source": "Myanmar Now (mm)",
            })
        except Exception as e:
            print(f"[warn] Myanmar Now article fetch failed: {url} ({e})")
            continue

    before = len(results)
    results = deduplicate_by_url(results)
    print(f"[myanmar-now-mm] dedup: {before} -> {len(results)}")
    return results

# åŒã˜URLã®é‡è¤‡å‰Šé™¤
def deduplicate_by_url(articles):
    seen_urls = set()
    unique_articles = []
    for art in articles:
        if art["url"] in seen_urls:
            print(
                f"ğŸ›‘ URL Duplicate Removed: {art['source']} | {art['title']} | {art['url']}"
            )
            continue
        seen_urls.add(art["url"])
        unique_articles.append(art)
    return unique_articles


# ç¿»è¨³å¯¾è±¡ã‚­ãƒ¥ãƒ¼
translation_queue = []


def process_and_enqueue_articles(
    articles,
    source_name,
    seen_urls=None,
    bypass_keyword=False,
    trust_existing_body=False,
):
    if seen_urls is None:
        seen_urls = set()

    queued_items = []
    for art in articles:
        if art["url"] in seen_urls:
            continue
        seen_urls.add(art["url"])

        try:
            # â‘  ã¾ãšã¯è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«æœ¬æ–‡ãŒæ¥ã¦ã„ãŸã‚‰ãã‚Œã‚’ä½¿ã†
            body_text = (art.get("body") or "").strip() if trust_existing_body else ""

            # â‘¡ ç„¡ã‘ã‚Œã°ãƒ•ã‚§ãƒƒãƒï¼ˆå†…éƒ¨ã§å†ãƒ•ã‚§ãƒƒãƒä»˜ããƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ï¼‰
            if not body_text:
                if source_name == "Irrawaddy" or "irrawaddy.com" in art["url"]:
                    body_text = get_body_with_refetch(
                        art["url"],
                        fetcher=lambda u: fetch_once_irrawaddy(
                            u, session=requests.Session()
                        ),
                        extractor=extract_body_irrawaddy,  # æ—¢å­˜ã®æŠ½å‡ºå™¨ã‚’ä½¿ç”¨
                        retries=3,
                        wait_seconds=2,
                        quiet=False,
                    )
                else:
                    body_text = get_body_with_refetch(
                        art["url"],
                        fetcher=fetch_once_requests,
                        extractor=extract_body_generic_from_soup,
                        retries=2,
                        wait_seconds=1,
                        quiet=True,
                    )

            # â‘¢ æ­£è¦åŒ–
            title_nfc = unicodedata.normalize("NFC", art["title"])
            body_nfc = unicodedata.normalize("NFC", body_text)
            
            # ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»/å…¨ä½“/éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ã®ãƒ’ãƒƒãƒˆåˆ¤å®šï¼ˆå„1å›ã®ã¿ï¼‰
            is_ayeyar = is_ayeyarwady_hit(title_nfc, body_nfc)
            # éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ã®ãƒ’ãƒƒãƒˆï¼ˆNEWS_KEYWORDS ã®ã¿ï¼‰
            hit_non_aye = any_keyword_hit(title_nfc, body_nfc)
            # å…¨ä½“ãƒ’ãƒƒãƒˆ = éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ or ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£
            hit_full = hit_non_aye or is_ayeyar

            # â‘£ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆIrrawaddyãªã©å¿…è¦ã«å¿œã˜ã¦ãƒã‚¤ãƒ‘ã‚¹ï¼‰
            if not bypass_keyword:
                if not hit_full:
                    log_no_keyword_hit(
                        source_name,
                        art["url"],
                        title_nfc,
                        body_nfc,
                        "enqueue:after-fetch",
                    )
                    continue

            # â‘¤ ã‚­ãƒ¥ãƒ¼æŠ•å…¥
            queued_items.append(
                {
                    "source": source_name,
                    "url": art["url"],
                    "title": art["title"],  # ç¿»è¨³å‰ã‚¿ã‚¤ãƒˆãƒ«
                    "body": body_text,  # ç¿»è¨³å‰æœ¬æ–‡
                    "is_ayeyar": is_ayeyar,  # ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ãƒ’ãƒƒãƒˆåˆ¤å®š
                    "hit_full": hit_full,  # å…¨ä½“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
                    "hit_non_ayeyar": hit_non_aye,  # éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£åˆ¤å®š
                }
            )

        except Exception as e:
            print(f"Error processing {art['url']}: {e}")
            continue

    translation_queue.extend(queued_items)


# MEMO: ãƒ­ã‚°ç”¨ã€ãƒ‡ãƒãƒƒã‚¯ç”¨é–¢æ•°
# def process_translation_batches(batch_size=10, wait_seconds=60):
#     summarized_results = []

#     # ãƒ†ã‚¹ãƒˆç”¨ã« translation_queue ã®ä¸­èº«ã‚’ãã®ã¾ã¾ summarized_results ã«è©°ã‚ã‚‹
#     for item in translation_queue:
#         summarized_results.append({
#             "source": item["source"],
#             "url": item["url"],
#             "title": item["title"],      # ç¿»è¨³å‰ã‚¿ã‚¤ãƒˆãƒ«
#             "summary": item["body"][:2000]  # è¦ç´„ã®ä»£ã‚ã‚Šã«æœ¬æ–‡å†’é ­
#         })

#     # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆsummarized_results ã®ä¸­èº«ã‚’çœç•¥ã›ãšç¢ºèªï¼‰
#     print("===== DEBUG: summarized_results =====")
#     pprint.pprint(summarized_results, width=120, compact=False)
#     print("===== END DEBUG =====")

#     # ã“ã“ã§å‡¦ç†çµ‚äº†
#     return summarized_results


# é‡è¤‡è¨˜äº‹å‰Šé™¤å‡¦ç†ã‚»ãƒƒãƒˆ
def _strip_tags(text: str) -> str:
    # è¦ç´„ã«å«ã‚ãŸ <br> ãªã©ã‚’ç´ ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼ˆæœ€ä½é™ï¼‰
    text = text.replace("<br>", "\n")
    return re.sub(r"<[^>]+>", "", text)


def _safe_json_loads_maybe_extract(text: str):
    """
    ç”ŸæˆAIãŒå‰å¾Œã«ä½™è¨ˆãªæ–‡ã‚’ä»˜ã‘ãŸå ´åˆã§ã‚‚JSONéƒ¨åˆ†ã ã‘æŠ½å‡ºã—ã¦èª­ã‚€ä¿é™ºã€‚
    """
    try:
        return json.loads(text)
    except Exception:
        # æœ€å¾Œã® { ... } ã‚’ç´ æœ´ã«æŠ½å‡º
        m = re.search(r"\{.*\}", text, flags=re.DOTALL)
        if m:
            return json.loads(m.group(0))
        raise


# é‡è¤‡åˆ¤å®šã®ãƒ­ã‚°å‡ºåŠ›
def log_dedupe_report(
    data: dict,
    id_map: dict,
    id_to_meta: dict,
    article_ids_in_order: list[str],
    *,
    printer=print,
    header="ğŸ§© DEDUPE REPORT",
):
    """
    LLMå¿œç­”ãƒ‡ãƒ¼ã‚¿(data)ã¨ã€IDâ†’è¨˜äº‹ãƒ¡ã‚¿æƒ…å ±ã®ãƒãƒƒãƒ—ã‚’å—ã‘å–ã‚Šã€
    é‡è¤‡åˆ¤å®šãƒ¬ãƒãƒ¼ãƒˆã‚’æ•´å½¢ã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚

    - data: {"kept":[...], "removed":[...], "clusters":[...]}
    - id_map: {id -> å…ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ}
    - id_to_meta: {id -> {"title": str, "source": str}}
    - article_ids_in_order: å…¥åŠ›é †åºã®IDãƒªã‚¹ãƒˆï¼ˆå…ƒé…åˆ—ã®é †ã‚’ä¿ã¤ãŸã‚ã«ä½¿ç”¨ï¼‰
    - printer: å‡ºåŠ›é–¢æ•°ï¼ˆprint ã‚„ logger.info ãªã©ï¼‰
    """
    kept_list = data.get("kept") or []
    removed_list = data.get("removed") or []
    clusters = data.get("clusters") or []

    kept_ids = [x.get("id") for x in kept_list if x.get("id") in id_map]
    kept_set = set(kept_ids)

    printer(f"\n===== {header} =====")

    # 1) Kept æ¦‚è¦
    printer(f"Kept: {len(kept_ids)} item(s)")
    for k in kept_list:
        kid = k.get("id")
        meta = id_to_meta.get(kid, {})
        why = (k.get("why") or "").strip()
        if kid in id_map:
            why_part = (
                f"  | why: {why}" if why else ""
            )  # â† ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å¼ã«å…¥ã‚Œãªã„
            printer(
                f"  âœ“ [{kid}] {meta.get('title','(no title)')}  | src={meta.get('source','')}"
                f"{why_part}"
            )
        else:
            printer(f"  âœ“ [{kid}] (unknown id)")

    # 2) Removed è©³ç´°ï¼ˆã©ã‚Œã®é‡è¤‡ã¨ã—ã¦è½ã¡ãŸã‹ï¼‰
    printer(f"\nRemoved (LLM-reported): {len(removed_list)} item(s)")
    for r in removed_list:
        rid = r.get("id")
        dup = r.get("duplicate_of")
        why = (r.get("why") or "").strip()
        rmeta = id_to_meta.get(rid, {"title": "(unknown)", "source": ""})
        kmeta = id_to_meta.get(dup, {"title": "(unknown)", "source": ""})
        unknown_flags = []
        if rid not in id_map:
            unknown_flags.append("RID_NOT_IN_INPUT")
        if dup and dup not in id_map:
            unknown_flags.append("KEPT_NOT_IN_INPUT")
        uf = f"  [{', '.join(unknown_flags)}]" if unknown_flags else ""
        reason_line = f"\n      reason: {why}" if why else ""  # â† å…ˆã«ä½œã‚‹
        printer(
            f"  - [{rid}] {rmeta['title']}  | src={rmeta['source']}\n"
            f"      â†’ duplicate of [{dup}] {kmeta['title']}  | src={kmeta['source']}{uf}"
            f"{reason_line}"
        )

    # 3) å®Ÿå·®åˆ†ï¼ˆå…¥åŠ› - keptï¼‰
    derived_removed_ids = [aid for aid in article_ids_in_order if aid not in kept_set]
    printer(f"\nRemoved (derived by kept-set): {len(derived_removed_ids)} item(s)")
    for rid in derived_removed_ids:
        rmeta = id_to_meta.get(rid, {"title": "(unknown)", "source": ""})
        rrec = next((x for x in removed_list if x.get("id") == rid), None)
        if rrec:
            dup = rrec.get("duplicate_of")
            why = (rrec.get("why") or "").strip()
            kmeta = id_to_meta.get(dup, {"title": "(unknown)", "source": ""})
            reason_line = f"\n      reason: {why}" if why else ""  # â† å…ˆã«ä½œã‚‹
            printer(
                f"  - [{rid}] {rmeta['title']}  | src={rmeta['source']}\n"
                f"      â†’ duplicate of [{dup}] {kmeta['title']}  | src={kmeta['source']}"
                f"{reason_line}"
            )
        else:
            printer(
                f"  - [{rid}] {rmeta['title']}  | src={rmeta['source']} (â€» LLMã®removedã«æœªè¨˜è¼‰)"
            )

    # 4) å‚ç…§æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    unknown_kept = [
        kid for kid in [x.get("id") for x in kept_list] if kid not in id_map
    ]
    unknown_removed = [r.get("id") for r in removed_list if r.get("id") not in id_map]
    if unknown_kept:
        printer(f"\nâš ï¸ Keptã«æœªçŸ¥ã®IDãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {unknown_kept}")
    if unknown_removed:
        printer(f"âš ï¸ Removedã«æœªçŸ¥ã®IDãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {unknown_removed}")

    # 5) ã‚¯ãƒ©ã‚¹ã‚¿æ¦‚è¦ï¼ˆä»»æ„ï¼‰
    if clusters:
        printer("\nCluster summary:")
        cluster_kept_map = {
            k.get("cluster_id"): k.get("id") for k in kept_list if k.get("cluster_id")
        }
        for c in clusters:
            cid = c.get("cluster_id")
            members = c.get("member_ids") or []
            event_key = c.get("event_key") or ""
            kept_id_for_cluster = cluster_kept_map.get(cid)
            printer(
                f"  â€¢ cluster={cid}  members={len(members)}  kept={kept_id_for_cluster}  event='{event_key}'"
            )

    printer("===== END DEDUPE REPORT =====\n")


def dedupe_articles_with_llm(
    client,
    summarized_results,
    debug=True,
    *,
    logger=None,
    ultra_max_chars=300,
    summary_fallback_chars=600,
):
    """
    summarized_results (list[dict]) ã‚’å—ã‘å–ã‚Šã€é‡è¤‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã”ã¨ã«1æœ¬ã ã‘æ®‹ã—ãŸé…åˆ—ã‚’è¿”ã™ã€‚
    Irrawaddyï¼ˆsource == "Irrawaddy" ã¾ãŸã¯ URL ã« "irrawaddy.com" ã‚’å«ã‚€ï¼‰ã¯
    LLM ã§ã®é‡è¤‡åˆ¤å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å¸¸ã« keep ã™ã‚‹ã€‚
    ä¾å­˜: call_gemini_with_retries, _safe_json_loads_maybe_extract, _strip_tags, log_dedupe_report
    """

    if not summarized_results:
        return summarized_results

    # å‡ºåŠ›é–¢æ•°
    if debug:
        printer = logger.info if logger else print
    else:

        def _noop(*args, **kwargs):
            return None

        printer = _noop

    # ===== LLMå…¥åŠ›ç”¨ï¼ˆIrrawaddy ã‚’é™¤å¤–ï¼‰ã‚’æ§‹ç¯‰ =====
    irrawaddy_ids = set()
    articles_for_llm = []
    id_map_llm = {}
    id_to_meta_llm = {}
    ids_in_order_llm = []
    all_ids_in_order = []  # è¿”å´æ™‚ã®é †åºç¶­æŒç”¨

    for idx, it in enumerate(summarized_results):
        _id_raw = it.get("url") or f"idx-{idx}"
        _id = _norm_id(_id_raw)  # â˜… å…¥åŠ›å´ï¼ˆè‡ªåˆ†å´ï¼‰ã®IDã‚’æ­£è¦åŒ–
        all_ids_in_order.append(_id)

        # Irrawaddy åˆ¤å®šï¼ˆã”æŒ‡å®šã©ãŠã‚Šï¼‰
        is_irrawaddy = (it.get("source") == "Irrawaddy") or (
            "irrawaddy.com" in (it.get("url") or "")
        )
        if is_irrawaddy:
            irrawaddy_ids.add(_id)
            continue  # LLM ã«ã¯é€ã‚‰ãªã„

        # é Irrawaddy â†’ LLM å…¥åŠ›ã¸
        body_ultra = (it.get("ultra") or "").strip()
        body_fallback = _strip_tags(it.get("summary", ""))[:summary_fallback_chars]
        body = body_ultra[:ultra_max_chars] if body_ultra else body_fallback

        ids_in_order_llm.append(_id)
        id_map_llm[_id] = it
        id_to_meta_llm[_id] = {"title": it.get("title"), "source": it.get("source")}
        articles_for_llm.append(
            {
                "id": _id,
                "source": it.get("source"),
                "title": it.get("title"),
                "body": body,
            }
        )

    # ã™ã¹ã¦ Irrawaddy ã ã£ãŸå ´åˆã¯ãã®ã¾ã¾è¿”ã™
    if not articles_for_llm:
        if debug and irrawaddy_ids:
            printer(
                f"â­ï¸ å…¨ {len(irrawaddy_ids)} ä»¶ãŒ Irrawaddyã€‚LLM é‡è¤‡åˆ¤å®šã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
        return summarized_results

    # ===== ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆLLM ã«é€ã‚‹åˆ†ã®ã¿ï¼‰ =====
    if debug:
        if irrawaddy_ids:
            printer(f"â­ï¸ Irrawaddy {len(irrawaddy_ids)} ä»¶ã¯å¸¸ã« keepï¼ˆLLM ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚")
        printer("===== DEBUG 2: articles SENT TO LLM =====")
        printer(_pprint.pformat(articles_for_llm, width=120, compact=False))
        printer("===== END DEBUG 2 =====\n")

    # ===== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆé Irrawaddy ã®ã¿ï¼‰ =====
    prompt = (
        "ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡è¤‡åˆ¤å®šãƒ•ã‚£ãƒ«ã‚¿ã§ã™ã€‚\n"
        "ä»¥å¾Œã®åˆ¤å®šã¯å„è¨˜äº‹ã®ã€Œtitleã€ã¨ã€Œbodyï¼ˆã“ã‚Œã¯è¶…è¦ç´„ã¾ãŸã¯çŸ­ç¸®è¦ç´„ï¼‰ã€ã®ã¿ã‚’ä½¿ç”¨ã—ã€å…ƒæœ¬æ–‡ã«ã¯æˆ»ã£ã¦å†å‚ç…§ã—ã¾ã›ã‚“ã€‚\n"
        "ç›®çš„ï¼šåŒä¸€ä¸»æ—¨ï¼ˆãƒˆãƒ”ãƒƒã‚¯ + è§’åº¦ + ç™ºä¿¡ä¸»ä½“ï¼‰ã‚’å ±ã˜ã‚‹è¨˜äº‹ã‚’æŸã­ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰1æœ¬ã ã‘æ®‹ã—ã¾ã™ã€‚å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ã€‚\n\n"
        "ã€å®šç¾©ã€‘\n"
        "ãƒ»ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ï¼šwho / what / where / when ã®ã†ã¡å°‘ãªãã¨ã‚‚3è¦ç´ ãŒä¸€è‡´ï¼ˆè¨€ã„æ›ãˆãƒ»è¨€èªå·®ã¯åŒä¸€æ‰±ã„ã€‚æ—¥ä»˜ã¯Â±14æ—¥ã‚’åŒä¸€æ‰±ã„å¯ï¼‰ã€‚\n"
        "ãƒ»è¨˜äº‹ã®ç¨®é¡ï¼ˆtypeï¼‰ï¼šä»¥ä¸‹ã®æ­£è¦åŒ–ã‚«ãƒ†ã‚´ãƒªã®ã„ãšã‚Œã‹1ã¤ã«å†…éƒ¨ã§åˆ†é¡ã—ã¦ç”¨ã„ã‚‹ï¼ˆå‡ºåŠ›ã«ã¯å«ã‚ãªã„ï¼‰ã€‚\n"
        "  é€Ÿå ±/å˜å ±, æ”¿ç­–ç™ºè¡¨è¦ç‚¹, å…¬å¼ç™ºè¡¨/å£°æ˜, ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼, è§£èª¬/èƒŒæ™¯, ç‰©å£°æ˜, çµ„ç¹”å£°æ˜, å…¬ç¤º,\n"
        "  ãƒ‡ãƒ¼ã‚¿/çµ±è¨ˆ, ã¾ã¨ã‚/ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ, ãƒ©ã‚¤ãƒ–/æ™‚ç³»åˆ—æ›´æ–°,\n"
        "  å†™çœŸ/æ˜ åƒç‰¹é›†, ç¤¾èª¬/è«–èª¬/å¯„ç¨¿, ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«\n"
        "  è¿‘ã„åŒç¾©èªã¯å†…éƒ¨ã§æ­£è¦åŒ–ï¼šã€press release/announcementâ†’å…¬å¼ç™ºè¡¨/å£°æ˜ã€ã€explainer/analysisâ†’è§£èª¬/èƒŒæ™¯ã€\n"
        "  ã€roundup/digestâ†’ã¾ã¨ã‚/ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã€ã€live updatesâ†’ãƒ©ã‚¤ãƒ–/æ™‚ç³»åˆ—æ›´æ–°ã€\n"
        "  åˆ¤åˆ¥ä¸èƒ½ãªå ´åˆã¯ type=ä¸æ˜ ã¨ã—ã€ç¨®é¡ä¸€è‡´ã«ã¯æ•°ãˆãªã„ã€‚\n"
        "ãƒ»ç™ºä¿¡ä¸»ä½“ï¼ˆprovenanceï¼‰ï¼šä»¥ä¸‹ã®ã„ãšã‚Œã‹1ã¤ã‚’å†…éƒ¨ã§æ¨å®šã—ã¦ç”¨ã„ã‚‹ã€‚\n"
        "  â‘  æœ¬äººæŒ‡ç¤º/é¦–é•·ã®ç›´è¨€ï¼ˆä¾‹ï¼šãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³ãŒã€ŒæŒ‡ç¤º/å‘½ä»¤/è¡¨æ˜ã€ï¼‰\n"
        "  â‘¡ å…¬å¼æ©Ÿé–¢ã®ç™ºè¡¨ï¼ˆå®˜å ±/ä¼šè¦‹/æ–‡æ›¸/åºƒå ±ï¼‰\n"
        "  â‘¢ åŒ¿åã®è»ç­‹/é–¢ä¿‚è€…/æ¶ˆæ¯ç­‹/å†…éƒ¨ç­‹ï¼ˆã€Œè»ç­‹ã«ã‚ˆã‚Œã°ã€ã€Œé–¢ä¿‚è€…ã«ã‚ˆã‚‹ã¨ã€ç­‰ï¼‰\n"
        "  â‘£ ç¾åœ°é‹ç”¨ãƒ»æ²»å®‰éƒ¨éšŠ/å§”å“¡ä¼šã®å®Ÿå‹™é€šé”\n\n"
        "ã€åˆ¤å®šæ–¹é‡ã€‘\n"
        "1) åŒä¸€ä¸»æ—¨ = ã€ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ã€ã‹ã¤ã€ç¨®é¡ä¸€è‡´ï¼ˆtypeãŒä¸€è‡´ã€ã‹ã¤ä¸æ˜ä»¥å¤–ï¼‰ã€ã‹ã¤ã€ç™ºä¿¡ä¸»ä½“ï¼ˆprovenanceï¼‰ä¸€è‡´ã€ã®å…¨ã¦ã‚’æº€ãŸã™å ´åˆã«é™ã‚‹ã€‚\n"
        "   â€» ã¾ã¨ã‚/ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ/è¤‡æ•°æ¡ˆä»¶åˆ—æŒ™ã®è¦ç´„ã¨ã€å˜ä¸€æ¡ˆä»¶ã®é€Ÿå ±ãƒ»è§£èª¬ã¯é‡è¤‡ã«ã—ãªã„ï¼ˆåˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼‰ã€‚\n"
        "   â€» åŒä¸€ãƒ†ãƒ¼ãƒï¼ˆä¾‹ï¼šé¸æŒ™é‹å‹•è¦åˆ¶ï¼‰ã§ã‚‚ã€å†…å®¹è¦åˆ¶ã€ã¨ã€é‹ç”¨ãƒ»æ‰‹ç¶šï¼ˆè¨±èªå¯/å ´æ‰€/æ™‚é–“/è­¦å‚™/ç®¡ç†ï¼‰ã€ã¯åˆ¥è§’åº¦ã¨ã—ã¦å¿…ãšåˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã™ã‚‹ã€‚\n"
        "   ä¾‹ï¼šã€ˆè»ã¸ã®æ‰¹åˆ¤çš„é¸æŒ™é‹å‹•ã‚’ç¦ã˜ã‚‹ï¼ˆå†…å®¹è¦åˆ¶ï¼‰ã€‰ã¨ã€ˆè»ã®ç®¡ç†ä¸‹ãƒ»äº‹å‰è¨±å¯ã§ã®ã¿é¸æŒ™æ´»å‹•å¯ï¼ˆé‹ç”¨ãƒ»æ‰‹ç¶šï¼‰ã€‰ã¯åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã€‚\n"
        "   ä¾‹ï¼šã€ˆMAHæœ¬äººãŒâ€œæ‰¹åˆ¤ç¦æ­¢â€ã‚’æŒ‡ç¤ºï¼ˆæœ¬äººæŒ‡ç¤ºï¼‰ã€‰ã¨ã€ˆãƒãƒ”ãƒ‰ãƒ¼è»ç­‹ãŒâ€œè¨±å¯åˆ¶ãƒ»ç®¡ç†ä¸‹â€ã¨ä¼èï¼ˆè»ç­‹ï¼‰ã€‰ã¯ã€è§’åº¦ã‚‚ç™ºä¿¡ä¸»ä½“ã‚‚ç•°ãªã‚‹ãŸã‚åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã€‚\n"
        "2) ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–ï¼šè¨˜äº‹ã¯æœ€ã‚‚ä¸€è‡´åº¦ãŒé«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã®ã¿æ‰€å±ã€‚ä¸ç¢ºå®Ÿãªã‚‰åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã™ã‚‹ã€‚\n"
        "3) æ®‹ã™åŸºæº–ï¼ša)å›ºæœ‰æƒ…å ±é‡ï¼ˆåœ°å/äººæ•°/é‡‘é¡/çµ„ç¹”å/æ–°è¦äº‹å®Ÿï¼‰ b)å…·ä½“æ€§/æ˜ç­ã• c)ã‚¿ã‚¤ãƒˆãƒ«æƒ…å ±é‡ã€‚\n"
        "   åŒç‚¹ãªã‚‰ æœ¬æ–‡é•·ï¼ˆbodyã®æ–‡å­—æ•°ï¼‰â†’ sourceæ˜‡é † â†’ idæ˜‡é † ã®é †ã§æ±ºå®šã€‚\n"
        "4) å…¥åŠ›å¤–ã®äº‹å®Ÿã¯åŠ ãˆãªã„ã€‚çµ±åˆè¨˜äº‹ã¯ä½œã‚‰ãªã„ã€‚\n\n"
        "ã€å‡ºåŠ›ã®åˆ¶ç´„ã€‘\n"
        "ãƒ»JSONã®ã¿ã‚’è¿”ã™ã€‚ä½™è¨ˆãªãƒ†ã‚­ã‚¹ãƒˆã‚„ã‚­ãƒ¼ã¯ç¦æ­¢ã€‚\n"
        "ãƒ»kept/removed/clusters ã® id ã¯å¿…ãšå…¥åŠ› articles ã® id ã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚\n"
        "ãƒ»clusters[].member_ids ã¯å…¥åŠ› id ã‚’é‡è¤‡ãªãã™ã¹ã¦å«ã‚€ã“ã¨ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã¨ keptä»¶æ•°ã¯åŒæ•°ã€‚\n"
        "ãƒ»removed[].duplicate_of ã¯åŒä¸€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã® kept id ã‚’æŒ‡ã™ã“ã¨ã€‚\n"
        "ãƒ»why ã¯16ã€œ24å­—ç¨‹åº¦ã€event_key ã¯25å­—ä»¥å†…ã«åã‚ã‚‹ã“ã¨ã€‚\n\n"
        "å…¥åŠ›:\n"
        f'{{\\n  "articles": {json.dumps(articles_for_llm, ensure_ascii=False)}\\n}}\\n\\n'
        "å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSONã®ã¿ï¼‰:\n"
        "{\n"
        '  "kept": [ {"id":"<æ®‹ã™è¨˜äº‹ID>", "cluster_id":"<ID>", "why":"16-24å­—"} ],\n'
        '  "removed": [ {"id":"<é™¤å¤–è¨˜äº‹ID>", "duplicate_of":"<æ®‹ã—ãŸè¨˜äº‹ID>", "why":"16-24å­—"} ],\n'
        '  "clusters": [ {"cluster_id":"<ID>", "member_ids":["<id1>","<id2>","..."], "event_key":"25å­—ä»¥å†…"} ]\n'
        "}\n"
    )

    try:
        resp = call_gemini_with_retries(client, prompt, model="gemini-2.5-flash")
        data = _safe_json_loads_maybe_extract(resp.text)

        # â˜… LLMå¿œç­”å†…ã®IDã‚’ã™ã¹ã¦æ­£è¦åŒ–ã—ã¦ãŠã
        for k in ("kept", "removed"):
            arr = data.get(k) or []
            for rec in arr:
                if "id" in rec:
                    rec["id"] = _norm_id(rec["id"])
                if "duplicate_of" in rec and rec["duplicate_of"]:
                    rec["duplicate_of"] = _norm_id(rec["duplicate_of"])

        for c in data.get("clusters", []) or []:
            if "cluster_id" in c:
                c["cluster_id"] = _norm_id(c["cluster_id"])
            if "member_ids" in c and isinstance(c["member_ids"], list):
                c["member_ids"] = [_norm_id(x) for x in c["member_ids"]]

        kept_ids_others = [
            x.get("id") for x in data.get("kept", []) if x.get("id") in id_map_llm
        ]

        # ãƒ¬ãƒãƒ¼ãƒˆï¼ˆLLM ã«é€ã£ãŸåˆ†ã®ã¿ï¼‰
        if debug:
            log_dedupe_report(
                data=data,
                id_map=id_map_llm,
                id_to_meta=id_to_meta_llm,
                article_ids_in_order=ids_in_order_llm,
                printer=printer,
                header="ğŸ§© DEDUPE REPORT (non-Irrawaddy only)",
            )

        # kept ãŒå‡ºãŸã¨ãã®ã¿ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ã—ã€Irrawaddy ã‚’åˆæµ
        if kept_ids_others:
            kept_union = set(kept_ids_others) | irrawaddy_ids
            filtered = [
                obj
                for obj, _id in zip(summarized_results, all_ids_in_order)
                if _id in kept_union
            ]
            return filtered

        # ã†ã¾ãåˆ¤å®šã§ããªã‹ã£ãŸã‚‰åŸæœ¬ã‚’è¿”ã™ï¼ˆIrrawaddy ã‚‚å½“ç„¶æ®‹ã‚‹ï¼‰
        return summarized_results

    except Exception as e:
        print(f"ğŸ›‘ Dedupe failed, returning original list: {e}")
        return summarized_results


# ===== è¦ç´„ãƒ»ç¿»è¨³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ãƒ¼ãƒ„ =====
STEP12_FILTERS = (
    "Step 1: ä¾‹å¤–ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰\n"
    "Q1. è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯æœ¬æ–‡ãŒ `Myawaddy`, `á€™á€¼á€á€á€®`, `Muse`, `á€™á€°á€†á€šá€º`, `å›½å¢ƒè²¿æ˜“`, `å›½å¢ƒäº¤æ˜“`ã«é–¢ã™ã‚‹å†…å®¹ã§ã™ã‹ï¼Ÿ\n"
    "â†’ Yes ã®å ´åˆã€ã“ã®å¾Œã®åˆ¤å®šã¯è¡Œã‚ãš Step 3 ã«é€²ã‚“ã§ãã ã•ã„ã€‚\n"
    "â†’ No ã®å ´åˆã¯ Step 2 ã¸é€²ã‚“ã§ãã ã•ã„ã€‚\n\n"
    "Step 2: é™¤å¤–æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆåœ°åŸŸä¾‹å¤–ã‚ã‚Šï¼ä¸»é¡Œï¼è¨˜äº‹ã®è§’åº¦ã§åˆ¤å®šï¼‰\n"
    "Q2. æœ¬è¨˜äº‹ã®ä¸»é¡ŒãŒã€ç‰¹å®šã®åœ°åŸŸï¼ˆéƒ¡åŒºãƒ»ã‚¿ã‚¦ãƒ³ã‚·ãƒƒãƒ—ãƒ»å¸‚ãƒ»æ‘ï¼‰ã«ãŠã‘ã‚‹\n"
    "    å±€åœ°çš„æ²»å®‰ã‚¤ãƒ™ãƒ³ãƒˆã®ã€Œç™ºç”Ÿãã®ã‚‚ã®ã€ã‚’é€Ÿå ±ãƒ»è¨˜éŒ²ãƒ»å ±é“ã™ã‚‹è¨˜äº‹ã§ã™ã‹ï¼Ÿ\n"
    "    ï¼ˆæˆ¦é—˜ãƒ»äº¤æˆ¦ã€æ”»æ’ƒã€”è¥²æ’ƒ/çˆ†ç ´/ç‹™æ’ƒ/IED/ãƒ‰ãƒ­ãƒ¼ãƒ³ã€•ã€è¡çªã€çˆ†æ’ƒ/ç ²æ’ƒ/ç©ºçˆ†ã€\n"
    "      å¼·ç›—/ç•¥å¥ªã€æŠ—è­°æ´»å‹•ã€æŠ•é™ã€è§£æ”¾ã€æ®ºäººäº‹ä»¶ã®ç™ºç”Ÿå ±é“ãƒ»è¢«å®³é›†è¨ˆï¼‰\n"
    "\n"
    "é™¤å¤–ã—ãªã„æ˜ç¢ºãªä¾‹ï¼ˆ= No ã¨ã™ã‚‹ï¼‰ï¼š\n"
    "- äººç‰©/çµ„ç¹”ã®ç™ºè¨€ãƒ»åè«–ãƒ»å£°æ˜ãƒ»ä¼šè¦‹ãƒ»ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ãƒ»å‘Šç™ºãƒ»å¦å®šãŒä¸»é¡Œã®ã‚‚ã®\n"
    "  ï¼ˆäº‹ä»¶ã®å…·ä½“ä¾‹ã‚„åœ°åãƒ»äººæ•°ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ä¸»é¡ŒãŒä¸Šè¨˜ã§ç¤ºã—ãŸã€Œç™ºè¨€ã€ãªã‚‰ Noï¼‰\n"
    "- äº‹ä»¶ã®ç™ºç”Ÿãã®ã‚‚ã®ã§ã¯ãªãã€äº‹ä»¶ãŒå¼•ç”¨ã¨ã—ã¦ä½¿ã‚ã‚Œã¦ã„ã‚‹ã ã‘ã®è¨˜äº‹\n"
    "\n"
    "ä¸»é¡Œåˆ¤å®šã®æ‰‹ãŒã‹ã‚Šï¼š\n"
    "- ã‚¿ã‚¤ãƒˆãƒ«ãƒ»å†’é ­å„ªå…ˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«å…ˆé ­60å­—ï¼‹æœ¬æ–‡å†’é ­300å­—ã‚’é‡ã¿ä»˜ã‘ï¼‰\n"
    "- ç™ºè¨€ä¸»é¡Œã‚’ç¤ºã™åˆå›³èªï¼ˆæ—¥æœ¬èª/è‹±èª/ãƒ“ãƒ«ãƒèªï¼‰ï¼š\n"
    "  ã€Œå£°æ˜ã€ã€Œç™ºè¡¨ã€ã€Œåè«–ã€ã€Œå¦å®šã€ã€Œä¼šè¦‹ã€ã€Œè«‡è©±ã€ã€Œã¨è¿°ã¹ãŸã€ã€Œã¨èªã£ãŸã€ã€Œã¨ä¸»å¼µã€ã€\n"
    '  "statement","press conference","spokesperson","said","denied","accused",\n'
    "  ã€Œá€•á€¼á€±á€¬á€†á€­á€¯ã€ã€Œá€‘á€¯á€á€ºá€•á€¼á€”á€ºã€ã€Œá€á€¯á€¶á€·á€•á€¼á€”á€ºã€ã€Œá€†á€­á€¯á€á€Šá€ºã€ã€Œá€•á€¼á€±á€¬á€€á€¼á€¬á€¸ã€ã€Œá€•á€¼á€±á€¬á€›á€±á€¸á€†á€­á€¯á€á€½á€„á€·á€ºá€›á€¾á€­á€á€°ã€ã€Œá€á€á€„á€ºá€¸á€…á€¬á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€•á€½á€²ã€\n"
    "â†’ ä¸Šè¨˜ã«è©²å½“ã™ã‚Œã° No ã¨ã—ã¦ Step 3 ã¸é€²ã‚€ã€‚\n"
    "\n"
    "â†’ Yes ã®å ´åˆã§ã‚‚ã€è¨˜äº‹ã®ä¸»ãŸã‚‹ç™ºç”Ÿå ´æ‰€ãŒæ¬¡ã®åœ°åŸŸã«è©²å½“ã™ã‚‹ãªã‚‰é™¤å¤–ã›ãš Step 3 ã¸é€²ã‚“ã§ãã ã•ã„ï¼š\n"
    "   ãƒ»ãƒ¤ãƒ³ã‚´ãƒ³ç®¡åŒº / Yangon Region / á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸\n"
    "   ãƒ»ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç®¡åŒº / Ayeyarwady Region / á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸\n"
    "â†’ ä¸Šè¨˜ä»¥å¤–ã®åœ°åŸŸã§ã‚ã‚Œã°å‡¦ç†ã‚’çµ‚äº†ã—ã€Step 3 ã«ã¯é€²ã¾ãªã„ã§ãã ã•ã„ã€‚å›ç­”ã¯ exit ã®1èªã®ã¿ï¼ˆè¨˜å·ãƒ»è£…é£¾ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãªã—ã€å°æ–‡å­—ï¼‰ã§è¿”ã—ã¦ãã ã•ã„ã€‚\n"
    "â†’ No ã®å ´åˆã¯ Step 3 ã¸é€²ã‚“ã§ãã ã•ã„ã€‚\n"
)

STEP3_TASK = (
    "Step 3: ç¿»è¨³ã¨è¦ç´„å‡¦ç†\n"
    "ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã€æœ¬æ–‡ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\n"
    "ã€ç¿»è¨³æ™‚ã®ç”¨èªçµ±ä¸€ãƒ«ãƒ¼ãƒ«ï¼ˆå¿…ãšå¾“ã†ã“ã¨ï¼‰ã€‘\n"
    "ã“ã®ãƒ«ãƒ¼ãƒ«ã¯è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®ç¿»è¨³ã«å¿…ãšé©ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
    "ã‚¯ãƒ¼ãƒ‡ã‚¿ãƒ¼æŒ‡å°è€…â‡’ç·å¸ä»¤å®˜\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆæŒ‡å°è€…ãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³â‡’ãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆã®ãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³â‡’ãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆè»äº‹æŒ‡å°è€…â‡’ç·å¸ä»¤å®˜\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆè»äº‹æ”¿æ¨©â‡’è»äº‹æ”¿æ¨©\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆè»äº‹è©•è­°ä¼šâ‡’è»äº‹æ”¿æ¨©\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆè»â‡’å›½è»\n"
    "è»äº‹è©•è­°ä¼šâ‡’è»äº‹æ”¿æ¨©\n"
    "è»äº‹å§”å“¡ä¼šâ‡’è»äº‹æ”¿æ¨©\n"
    "å¾´ç”¨â‡’å¾´å…µ\n"
    "è»äº‹è©•è­°ä¼šè»â‡’å›½è»\n\n"
    "ã€ç¿»è¨³æ™‚ã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«ã€‘\n"
    "ã“ã®ãƒ«ãƒ¼ãƒ«ã‚‚è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®ç¿»è¨³ã«å¿…ãšé©ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
    "ã€Œá€–á€™á€ºá€¸á€†á€®á€¸ã€ã®è¨³èªã¯æ–‡è„ˆã«ã‚ˆã£ã¦ä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ã€‚\n"
    "- çŠ¯ç½ªå®¹ç–‘ã‚„æ³•å¾‹é•åã«å¯¾ã™ã‚‹æ–‡è„ˆã®å ´åˆã¯ã€Œé€®æ•ã€ã¨ã™ã‚‹ã€‚\n"
    "- çŠ¯ç½ªå®¹ç–‘ã‚„æ³•å¾‹é•åã«åŸºã¥ã‹ãªã„æ–‡è„ˆã®å ´åˆã¯ã€Œæ‹˜æŸã€ã¨ã™ã‚‹ã€‚\n\n"
    "ã€é€šè²¨æ›ç®—ãƒ«ãƒ¼ãƒ«ã€‘\n"
    "ã“ã®ãƒ«ãƒ¼ãƒ«ã‚‚è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®ç¿»è¨³ã«å¿…ãšé©ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
    "ãƒŸãƒ£ãƒ³ãƒãƒ¼é€šè²¨ã€Œãƒãƒ£ãƒƒãƒˆï¼ˆKyatã€á€€á€»á€•á€ºï¼‰ã€ãŒå‡ºã¦ããŸå ´åˆã¯ã€æ—¥æœ¬å††ã«æ›ç®—ã—ã¦ä½µè¨˜ã—ã¦ãã ã•ã„ã€‚\n"
    "- æ›ç®—ãƒ¬ãƒ¼ãƒˆã¯ 1ãƒãƒ£ãƒƒãƒˆ = 0.033å†† ã‚’å¿…ãšä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚\n"
    "- è¨˜äº‹ä¸­ã«ãƒãƒ£ãƒƒãƒˆãŒå‡ºãŸå ´åˆã¯å¿…ãšã€Œâ—¯ãƒãƒ£ãƒƒãƒˆï¼ˆç´„â—¯å††ï¼‰ã€ã®å½¢å¼ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
    "- æ—¥æœ¬å††ã®è¡¨è¨˜ã¯å°æ•°ç‚¹ç¬¬ä¸€ä½ã¾ã§æ®‹ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 16,500.0å††ï¼‰ã€‚\n"
    "- ä»–ã®ãƒ¬ãƒ¼ãƒˆã¯ä½¿ç”¨ç¦æ­¢ã€‚\n\n"
    "ã‚¿ã‚¤ãƒˆãƒ«ï¼š\n"
    "- è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
    "ã‚¿ã‚¤ãƒˆãƒ«ã®å‡ºåŠ›æ¡ä»¶ï¼š\n"
    "- å‡ºåŠ›ã¯å¿…ãš1è¡Œã§ã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘<åŠè§’ã‚¹ãƒšãƒ¼ã‚¹1ã¤><è¨³ã—ãŸã‚¿ã‚¤ãƒˆãƒ«>ã€ã®å½¢å¼ã«ã™ã‚‹ã€‚\n"
    "- ã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘ã€ã®ç›´å¾Œã«æ”¹è¡Œã—ãªã„ã€‚\n"
    "- ã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘<åŠè§’ã‚¹ãƒšãƒ¼ã‚¹1ã¤><è¨³ã—ãŸã‚¿ã‚¤ãƒˆãƒ«>ã€ä»¥å¤–ã®æ–‡è¨€ã¯å›ç­”ã«å«ã‚ãªã„ã€‚\n\n"
    "æœ¬æ–‡è¦ç´„ï¼š\n"
    "- ä»¥ä¸‹ã®è¨˜äº‹æœ¬æ–‡ã«ã¤ã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ã¾ã¨ã‚ã€500å­—ä»¥å†…ã§å…·ä½“çš„ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
    "- è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
    "- å€‹åˆ¥è¨˜äº‹ã®æœ¬æ–‡ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€ãƒ¡ãƒ‡ã‚£ã‚¢èª¬æ˜ã‚„ãƒšãƒ¼ã‚¸å…¨ä½“ã®è§£èª¬ã¯ä¸è¦ã§ã™ã€‚\n"
    "- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã¯è¦ç´„ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€ãã‚Œä»¥å¤–ã®æ–‡è¨€ã¯ä¸è¦ã§ã™ã€‚\n\n"
    "æœ¬æ–‡è¦ç´„ã®å‡ºåŠ›æ¡ä»¶ï¼š\n"
    "- 1è¡Œç›®ã¯`ã€è¦ç´„ã€‘`ã¨ã ã‘ã—ã¦ãã ã•ã„ã€‚\n"
    "- 2è¡Œç›®ä»¥é™ãŒå…¨ã¦ç©ºè¡Œã«ãªã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
    "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã‚’é©åˆ‡ã«ä½¿ã£ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚\n"
    "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã«ã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ï¼ˆ#, *, - ãªã©ï¼‰ã‚’ä½¿ã‚ãšã€å˜ç´”ãªãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
    "- è¦‹å‡ºã—ã¯ `[ ]` ã§å›²ã‚“ã§ãã ã•ã„ã€‚\n"
    "- ç©ºè¡Œã¯ä½œã‚‰ãªã„ã§ãã ã•ã„ã€‚\n"
    "- ç‰¹æ®Šè¨˜å·ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ï¼ˆå…¨ä½“ã‚’HTMLã¨ã—ã¦é€ä¿¡ã™ã‚‹ã‚ã‘ã§ã¯ãªã„ãŸã‚ï¼‰ã€‚\n"
    "- ç®‡æ¡æ›¸ãã¯`ãƒ»`ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚\n"
    "- æœ¬æ–‡è¦ç´„ã®åˆè¨ˆã¯æœ€å¤§500æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„ã€‚\n\n"
    "æœ¬æ–‡è¶…è¦ç´„ï¼š\n"
    "- ä»¥ä¸‹ã®è¨˜äº‹æœ¬æ–‡ã«ã¤ã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã¾ã¨ã‚ã€200å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
    "- å€‹åˆ¥è¨˜äº‹ã®æœ¬æ–‡ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€ãƒ¡ãƒ‡ã‚£ã‚¢èª¬æ˜ã‚„ãƒšãƒ¼ã‚¸å…¨ä½“ã®è§£èª¬ã¯ä¸è¦ã§ã™ã€‚\n"
    "- ä¾‹ï¼šã€èª°ãŒã€ã€ä½•ã‚’ã€ã€ã©ã“ã§ã€ã€ã„ã¤ã€ã€è¦æ¨¡ï¼ˆäººæ•°/é‡‘é¡ç­‰ï¼‰ã€ã‚’å«ã‚ã‚‹ã€‚\n\n"
    "æœ¬æ–‡è¶…è¦ç´„ã®å‡ºåŠ›æ¡ä»¶ï¼š\n"
    "- 1è¡Œç›®ã¯`ã€è¶…è¦ç´„ã€‘`ã¨ã ã‘ã—ã¦ãã ã•ã„ã€‚\n"
    "- 2è¡Œç›®ä»¥ãŒé™å…¨ã¦ç©ºè¡Œã«ãªã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
    "- æœ¬æ–‡è¶…è¦ç´„ã®åˆè¨ˆã¯æœ€å¤§200æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„ã€‚\n\n"
)

SKIP_NOTE_IRRAWADDY = "ã€é‡è¦ã€‘æœ¬è¨˜äº‹ã¯ Irrawaddy ã®è¨˜äº‹ã§ã™ã€‚Step 1 ã¨ Step 2 ã¯å®Ÿæ–½ã›ãšã€ç›´ã¡ã« Step 3 ã®ã¿ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚\n\n"


def build_prompt(item: dict, *, skip_filters: bool, body_max: int) -> str:
    header = "æ¬¡ã®æ‰‹é †ã§è¨˜äº‹ã‚’åˆ¤å®šãƒ»å‡¦ç†ã—ã¦ãã ã•ã„ã€‚\n\n"
    pre = SKIP_NOTE_IRRAWADDY if skip_filters else STEP12_FILTERS + "\n\n"
    input_block = (
        "å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼š\n"
        "###\n[è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«]\n###\n"
        f"{item['title']}\n\n"
        "[è¨˜äº‹æœ¬æ–‡]\n###\n"
        f"{item['body'][:body_max]}\n"
        "###\n"
    )
    return header + pre + STEP3_TASK + "\n" + input_block


# è¶…è¦ç´„ã‚’å…ˆã«æŠœãå‡¦ç†
def _normalize_heading_text(s: str) -> str:
    """è¦‹å‡ºã—æ¤œå‡ºã®ãŸã‚ã®è»½é‡æ­£è¦åŒ–ï¼ˆæ‹¬å¼§ã®ç•°ä½“å­—ã‚„ä¸å¯è¦–æ–‡å­—ã‚’å¸åï¼‰"""
    trans = {
        ord("ï¼»"): "ã€",
        ord("ã€”"): "ã€",
        ord("ã€–"): "ã€",  # å·¦
        ord("ï¼½"): "ã€‘",
        ord("ã€•"): "ã€‘",
        ord("ã€—"): "ã€‘",  # å³
    }
    s = s.translate(trans)
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹â†’åŠè§’ã€NBSP/ZWSP/FEFF/ZWJ/ZWNJ ã‚’é™¤å»
    s = s.replace("\u3000", " ").replace("\xa0", " ")
    s = re.sub(r"[\u200b\u200c\u200d\ufeff]", "", s)
    # ã€Œè¦ã€ã€Œç´„ã€ã®é–“ã®å¤‰å‰‡ã‚¹ãƒšãƒ¼ã‚¹ã‚‚å¸å
    s = re.sub(r"(è¦)\s+(ç´„)", r"\1\2", s)
    return unicodedata.normalize("NFC", s)


def _cut_ultra_block(lines):
    """
    ã€Œè¶…è¦ç´„ã€ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆè¦‹å‡ºã—è¡Œã€œæ¬¡è¦‹å‡ºã—ç›´å‰ã¾ã§ï¼‰ã‚’åˆ‡ã‚Šå‡ºã—ã¦å‰Šé™¤ã€‚
    æ‹¬å¼§ã®ç•°ä½“å­—ï¼ˆï¼»ï¼½/ã€–ã€—/ã€”ã€•ï¼‰ã‚„ä¸å¯è¦–æ–‡å­—ã€å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã«è€æ€§ã‚ã‚Šã€‚
    """
    # æ­£è¦åŒ–ã—ãŸå½±ã‚’ä½œã‚‹ï¼ˆæ¤œå‡ºã¯ã“ã£ã¡ã€å‰Šé™¤ã¯å…ƒlinesã§ï¼‰
    norm = [_normalize_heading_text(ln) for ln in lines]

    HEAD_RE = re.compile(
        r"^ã€[\s\u3000\u200b\ufeff]*è¶…[\s\u3000\u200b\ufeff]*è¦[\s\u3000\u200b\ufeff]*ç´„[\s\u3000\u200b\ufeff]*ã€‘"
    )
    NEXT_HDR_RE = re.compile(r"^ã€.*?ã€‘")  # ä»–ã®è¦‹å‡ºã—ï¼ˆè¦ç´„/ã‚¿ã‚¤ãƒˆãƒ«ç­‰ï¼‰

    for i, ln_norm in enumerate(norm):
        if not HEAD_RE.match(ln_norm):
            continue

        # è¦‹å‡ºã—è¡Œã®â€œåŒä¸€è¡Œæœ¬æ–‡â€ï¼ˆæ­£è¦åŒ–å¾Œã§OKï¼‰
        inline = HEAD_RE.sub("", ln_norm).strip()
        start = i + 1

        # æ¬¡ã®è¦‹å‡ºã—ç›´å‰ã¾ã§
        end = start
        while end < len(norm) and not NEXT_HDR_RE.match(norm[end]):
            end += 1

        parts = []
        if inline:
            parts.append(inline)
        parts.extend(lines[start:end])  # æœ¬æ–‡ã¯å…ƒã®è¡Œã‚’ä½¿ã†

        new_lines = lines[:i] + lines[end:]
        return " ".join(parts).strip(), new_lines

    return "", lines


# æœ¬å‡¦ç†é–¢æ•°
def process_translation_batches(batch_size=3, wait_seconds=60):
    # MEMO: TESTç”¨ã€Geminiã‚’å‘¼ã°ãšã€URLãƒªã‚¹ãƒˆã ã‘è¿”ã™
    # summarized_results = []
    # for item in translation_queue:
    #     summarized_results.append({
    #         "source": item["source"],
    #         "url": item["url"],
    #         "title": item['title'],
    #         "summary": item['body'][:BODY_MAX_CHARS]
    #     })

    summarized_results = []
    for i in range(0, len(translation_queue), batch_size):
        batch = translation_queue[i : i + batch_size]
        print(f"âš™ï¸ Processing batch {i // batch_size + 1}...")

        for item in batch:
            try:
                # ãƒ‡ãƒãƒƒã‚°: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                print("----- DEBUG: Prompt Input -----")
                print(f"TITLE: {item['title']}")
                print(f"BODY[:{BODY_MAX_CHARS}]: {item['body'][:BODY_MAX_CHARS]}")

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œã€Irrawaddy ã¯ Step1/2 ã‚’ã‚¹ã‚­ãƒƒãƒ—
                is_irrawaddy = (item.get("source") == "Irrawaddy") or (
                    "irrawaddy.com" in (item.get("url") or "")
                )
                prompt = build_prompt(
                    item, skip_filters=is_irrawaddy, body_max=BODY_MAX_CHARS
                )

                resp = call_gemini_with_retries(
                    client_summary, prompt, model="gemini-2.5-flash"
                )
                output_text = resp.text.strip()

                print("----- DEBUG: Model Output -----")
                print(output_text)

                # --- exit ã‚’åºƒã‚ã«åˆ¤å®šï¼ˆãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆã‚„å¥èª­ç‚¹æ··å…¥å¯¾ç­–ï¼‰---
                EXIT_ONLY_RE = re.compile(
                    r"^\s*(?:`{0,3})?\s*exit\s*(?:`{0,3})?\.?\s*$", re.IGNORECASE
                )
                if EXIT_ONLY_RE.match(output_text):
                    continue

                # --- è¡Œæ•´å½¢ï¼ˆNFC + ç©ºè¡Œé™¤å»ï¼‰---
                lines = [
                    unicodedata.normalize("NFC", ln).strip()
                    for ln in output_text.splitlines()
                    if ln.strip()
                ]

                # --- è¶…è¦ç´„ã‚’å…ˆã«æŠœãï¼ˆæœ¬æ–‡ã‹ã‚‰ã‚‚æ¶ˆã™ï¼‰---
                ultra_text, lines = _cut_ultra_block(lines)

                # --- ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºï¼ˆè¦ä»¶ã«åˆã‚ã›ã¦å³æ ¼åŒ–ï¼‰---
                # ãƒ«ãƒ¼ãƒ«:
                #  A) ã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘è¨³é¡Œã€= åŒä¸€è¡Œ
                #  B) 1è¡Œç›®ãŒã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘ã€ã®ã¿ â†’ æ¬¡ã®è¡Œã‚’è¨³é¡Œã¨ã—ã¦æ¡ç”¨
                #  C) ä¸Šè¨˜ä»¥å¤–ã®ãƒ©ãƒ™ãƒ«æºã‚Œï¼ˆã‚¿ã‚¤ãƒˆãƒ«:, Title: ãªã©ï¼‰ã¯ç„¡è¦–ï¼ˆæ•‘æ¸ˆã—ãªã„ï¼‰
                title_text = ""
                title_idx = next(
                    (
                        i
                        for i, ln in enumerate(lines)
                        if re.match(r"^ã€\s*ã‚¿ã‚¤ãƒˆãƒ«\s*ã€‘", ln)
                    ),
                    None,
                )
                if title_idx is not None:
                    # ãƒãƒ¼ã‚«ãƒ¼è¡Œã‚’è§£æ
                    m = re.match(r"^ã€\s*ã‚¿ã‚¤ãƒˆãƒ«\s*ã€‘\s*(.*)$", lines[title_idx])
                    inline = (m.group(1) or "").strip()
                    # ãƒãƒ¼ã‚«ãƒ¼è¡Œã¯æ¶ˆã™
                    lines.pop(title_idx)

                    if inline:
                        # A) åŒä¸€è¡Œï¼ˆã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘â—¯â—¯ï¼‰
                        # å…ˆé ­ã«ã‚³ãƒ­ãƒ³ãŒç´›ã‚Œã‚‹äº‹æ•…ã ã‘è»½ãé™¤å»ï¼ˆãƒ©ãƒ™ãƒ«æ•‘æ¸ˆã§ã¯ãªã„ï¼‰
                        title_text = inline.lstrip(":ï¼š").strip()
                    else:
                        # B) æ¬¡ã®è¡Œã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦æ¡ç”¨ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰
                        if title_idx < len(lines):
                            title_text = lines[title_idx].strip()
                            lines.pop(title_idx)

                # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç©ºã‚’è¨±ã•ãªã„ï¼‰
                translated_title = (
                    title_text or item.get("title") or "ï¼ˆç¿»è¨³å¤±æ•—ï¼‰"
                ).strip()

                # --- è¦ç´„ãƒ©ãƒ™ãƒ«ã‚’å…ˆé ­ã«å¼·åˆ¶ ---
                if not lines or not re.match(r"^ã€\s*è¦ç´„\s*ã€‘\s*$", lines[0]):
                    lines.insert(0, "ã€è¦ç´„ã€‘")

                summary_text = "\n".join(lines).strip()
                summary_html = summary_text.replace("\n", "<br>")

                norm_url = _norm_id(item.get("url") or "")

                summarized_results.append(
                    {
                        "source": item["source"],
                        "url": norm_url,  # â˜… æ­£è¦åŒ–æ¸ˆã¿
                        "title": translated_title,
                        "summary": summary_html,
                        "ultra": ultra_text,
                        "is_ayeyar": item.get("is_ayeyar", False),  # ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ãƒ’ãƒƒãƒˆåˆ¤å®š
                        "hit_full": item.get("hit_full", False),  # å…¨ä½“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
                        "hit_non_ayeyar": item.get("hit_non_ayeyar", False),  # éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£åˆ¤å®š
                    }
                )

            except Exception as e:
                print(
                    "ğŸ›‘ Error during translation:", e.__class__.__name__, "|", repr(e)
                )
                continue

            # ãƒãƒƒãƒå†…ã§å¾®ã‚¹ãƒªãƒ¼ãƒ—ã—ã¦ãƒãƒ¼ã‚¹ãƒˆã‚’æŠ‘ãˆã‚‹
            time.sleep(0.6)

        if i + batch_size < len(translation_queue):
            print(f"ğŸ•’ Waiting {wait_seconds} seconds before next batch...")
            time.sleep(wait_seconds)

    # é‡è¤‡åˆ¤å®šâ†’ç‰‡æ–¹æ®‹ã—ï¼ˆæœ€çµ‚ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã®å½¢å¼ã¯å¤‰ãˆãªã„ï¼‰
    deduped = dedupe_articles_with_llm(client_dedupe, summarized_results, debug=True)

    # å¿µã®ãŸã‚ï¼šè¿”å´ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å›ºå®šï¼ˆä½™è¨ˆãªã‚­ãƒ¼ãŒæ··ã–ã£ã¦ã„ãŸã‚‰è½ã¨ã™ï¼‰
    normalized = [
        {
            "source": x.get("source"),
            "url": x.get("url"),
            "title": x.get("title"),
            "summary": x.get("summary"),
            "is_ayeyar": x.get("is_ayeyar", False),  # ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ãƒ’ãƒƒãƒˆåˆ¤å®š
            "hit_full": x.get("hit_full", False),  # å…¨ä½“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
            "hit_non_ayeyar": x.get("hit_non_ayeyar", False),  # éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£åˆ¤å®š
        }
        for x in deduped
    ]
    return normalized


def send_email_digest(summaries, *, recipients_env=None, subject_suffix=""):
    def _build_gmail_service():
        cid = os.getenv("GMAIL_CLIENT_ID")
        csec = os.getenv("GMAIL_CLIENT_SECRET")
        rtok = os.getenv("GMAIL_REFRESH_TOKEN")
        if not (cid and csec and rtok):
            raise RuntimeError(
                "Gmail API credentials (CLIENT_ID/SECRET/REFRESH_TOKEN) are missing."
            )

        # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚‹å ´åˆã€scopes ã‚’æ¸¡ã•ãªã„
        creds = Credentials(
            token=None,
            refresh_token=rtok,
            token_uri="https://oauth2.googleapis.com/token",
            client_id=cid,
            client_secret=csec,
        )
        return build("gmail", "v1", credentials=creds, cache_discovery=False)

    sender_email = os.getenv("EMAIL_SENDER")
    env_name = recipients_env
    recipient_emails = [x.strip() for x in os.getenv(env_name, "").split(",") if x.strip()]

    digest_date = get_today_date_mmt()
    date_str = digest_date.strftime("%Yå¹´%-mæœˆ%-dæ—¥") + "åˆ†"

    # ãƒ¡ãƒ‡ã‚£ã‚¢ã”ã¨ã«ã¾ã¨ã‚ã‚‹
    media_grouped = defaultdict(list)
    for item in summaries:
        media_grouped[item["source"]].append(item)

    subject = "ãƒŸãƒ£ãƒ³ãƒãƒ¼é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€" + date_str + "ã€‘"
    # ãƒ†ã‚¹ãƒˆç”¨è¨˜è¿°
    if subject_suffix:
        subject += " " + subject_suffix 

    # âœ… ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³éƒ¨åˆ†ã‚’å…ˆã«æ§‹ç¯‰
    headlines = []
    for item in summaries:
        headlines.append(f"âœ“ {item['title']}")  # â† åŠè§’ã‚¹ãƒšãƒ¼ã‚¹è¿½åŠ 

    headline_html = (
        "<div style='margin-bottom:20px'>"
        f"------- ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ ({len(summaries)}æœ¬) -------<br>"
        + "<br>".join(headlines)  # â† å„ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ”¹è¡Œã§è¡¨ç¤º
        + "</div><hr>"
    )

    # âœ… ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡å…¨ä½“ã®HTML
    html_content = """
    <html>
    <body style="font-family: Arial, sans-serif; background-color: #ffffff; color: #333333;">
    """

    # å…ˆé ­ã«ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³æŒ¿å…¥
    html_content += headline_html

    # è¨˜äº‹ã”ã¨ã®æœ¬æ–‡
    for media, articles in media_grouped.items():
        for item in articles:
            title_jp = item["title"]
            url = item["url"]
            summary_html = item["summary"]

            heading_html = (
                "<h2 style='margin-bottom:5px'>"
                f"{title_jp}ã€€"
                "<span style='font-size:0.83rem;font-weight:600'>"
                f"{media} "
                "</span>"
                "</h2>"
            )

            html_content += (
                "<div style='margin-bottom:20px'>"
                f"{heading_html}"
                "<div style='background-color:#f9f9f9;padding:10px;border-radius:8px'>"
                f"{summary_html}"
                "</div>"
                f"<p><a href='{url}' style='color:#1a0dab' target='_blank'>æœ¬æ–‡ã‚’èª­ã‚€</a></p>"
                "</div><hr style='border-top: 1px solid #cccccc;'>"
            )

    html_content += "</body></html>"
    html_content = clean_html_content(html_content)

    from_display_name = "Myanmar News Digest"

    subject = re.sub(r"[\r\n]+", " ", subject).strip()
    msg = EmailMessage(policy=SMTP)
    msg["Subject"] = subject
    msg["From"] = formataddr((str(Header(from_display_name, "utf-8")), sender_email))
    msg["To"] = ", ".join(recipient_emails)
    msg.set_content("HTMLãƒ¡ãƒ¼ãƒ«ã‚’é–‹ã‘ã‚‹ç’°å¢ƒã§ã”ç¢ºèªãã ã•ã„ã€‚", charset="utf-8")
    msg.add_alternative(html_content, subtype="html", charset="utf-8")

    try:
        service = _build_gmail_service()
        raw = base64.urlsafe_b64encode(msg.as_bytes()).decode("utf-8")
        body = {"raw": raw}
        sent = service.users().messages().send(userId="me", body=body).execute()
        print("âœ… Gmail API é€ä¿¡å®Œäº† messageId:", sent.get("id"))
    except HttpError as e:
        print(f"âŒ Gmail API ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    date_mmt = get_today_date_mmt()
    seen_urls = set()

    # articles = get_frontier_articles_for(date_mmt)
    # for art in articles:
    #     print(f"{art['date']} - {art['title']}\n{art['url']}\n")

    # === Mizzima (Burmese) ===
    print("=== Mizzima (Burmese) ===")
    articles_mizzima = get_mizzima_articles_from_category(
        date_mmt,
        "https://bur.mizzima.com",
        "Mizzima (Burmese)",
        "/category/%e1%80%9e%e1%80%90%e1%80%84%e1%80%ba%e1%80%b8/%e1%80%99%e1%80%bc%e1%80%94%e1%80%ba%e1%80%99%e1%80%ac%e1%80%9e%e1%80%90%e1%80%84%e1%80%ba%e1%80%b8",
        max_pages=3,
    )
    process_and_enqueue_articles(
        articles_mizzima, 
        "Mizzima (Burmese)", 
        seen_urls, 
        trust_existing_body=True
    )

    print("=== BBC Burmese ===")
    articles_bbc = get_bbc_burmese_articles_for(date_mmt)
    process_and_enqueue_articles(
        articles_bbc, 
        "BBC Burmese", 
        seen_urls, 
        trust_existing_body=True
    )

    print("=== Irrawaddy ===")
    articles_irrawaddy = get_irrawaddy_articles_for(date_mmt)
    # MEMO: ãƒ­ã‚°ç”¨ã€ãƒ‡ãƒãƒƒã‚¯ã§ãƒ­ã‚°ç¢ºèª
    # print("RESULTS:", json.dumps(articles_irrawaddy, ensure_ascii=False, indent=2))
    process_and_enqueue_articles(
        articles_irrawaddy,
        "Irrawaddy",
        seen_urls,
        bypass_keyword=True,  # â† Irrawaddyã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è½ã¨ã•ãªã„
        trust_existing_body=True,  # â† ã•ã£ãå…¥ã‚ŒãŸ body ã‚’ãã®ã¾ã¾ä½¿ã†ï¼ˆå†ãƒ•ã‚§ãƒƒãƒã—ãªã„ï¼‰
    )

    print("=== Khit Thit Media ===")
    articles_khit = get_khit_thit_media_articles_from_category(date_mmt, max_pages=3)
    process_and_enqueue_articles(
        articles_khit, 
        "Khit Thit Media", 
        seen_urls
    )

    print("=== DVB ===")
    articles_dvb = get_dvb_articles_for(date_mmt, debug=True)
    process_and_enqueue_articles(
        articles_dvb, 
        "DVB", 
        seen_urls, 
        trust_existing_body=True
    )
    
    print("===  Myanmar Now (Burmese) ===")
    articles_mn = get_myanmar_now_articles_mm(date_mmt, max_pages=3)
    process_and_enqueue_articles(
        articles_mn,
        "Myanmar Now (Burmese)",
        seen_urls,
        bypass_keyword=False,
        trust_existing_body=True,
    )

    # URLãƒ™ãƒ¼ã‚¹ã®é‡è¤‡æ’é™¤ã‚’å…ˆã«è¡Œã†
    print(f"âš™ï¸ Removing URL duplicates from {len(translation_queue)} articles...")
    translation_queue = deduplicate_by_url(translation_queue)

    # ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œ (5ä»¶ã”ã¨ã«1åˆ†å¾…æ©Ÿ)
    all_summaries = process_translation_batches(batch_size=3, wait_seconds=60)

    # A/Båˆ†å²
    # âœ… A = å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ’ãƒƒãƒˆï¼ˆå¾“æ¥ã® any_keyword_hit çµæœï¼‰
    summaries_A = [s for s in all_summaries if s.get("hit_full")]
    # âœ… B = ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ã‚’é™¤ã„ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã ã‘ã§ãƒ’ãƒƒãƒˆ
    summaries_B = [s for s in all_summaries if s.get("hit_non_ayeyar")]

    # A/Bã©ã¡ã‚‰ã‚‚é€ä¿¡
    # âœ… A = ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ã‚’å«ã‚€å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ’ãƒƒãƒˆ
    send_email_digest(
        summaries_A,
        recipients_env="INTERNAL_EMAIL_RECIPIENTS",
        subject_suffix="/ (ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£å«)"
    )
    
    # âœ… B = ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ã‚’é™¤ã„ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ’ãƒƒãƒˆ
    send_email_digest(
        summaries_B,
        recipients_env="EMAIL_RECIPIENTS",
        subject_suffix="/ (ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£å«ã¾ãªã„)"
    )
