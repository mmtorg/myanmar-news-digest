import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, date, timezone
from dateutil.parser import parse as parse_date
import re
# Chat GPT
# from openai import OpenAI, OpenAIError
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import os
import sys
from email import policy  # â† è¿½åŠ 
from email.header import Header  # â† è¿½åŠ å¿…è¦
from email.message import EmailMessage
from email.policy import SMTPUTF8
from email.utils import formataddr
import unicodedata
from google import genai
from google.api_core.exceptions import GoogleAPICallError
from collections import defaultdict
import time
# è¨˜äº‹é‡è¤‡æ’é™¤ãƒ­ã‚¸ãƒƒã‚¯(BERTåŸ‹ã‚è¾¼ã¿ç‰ˆ)ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sentence_transformers import SentenceTransformer, util

# Gemini
client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# Chat GPT
# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ãƒŸãƒ£ãƒ³ãƒãƒ¼æ¨™æº–æ™‚ (UTC+6:30)
MMT = timezone(timedelta(hours=6, minutes=30))

# æ˜¨æ—¥ã®æ—¥ä»˜
# ãƒŸãƒ£ãƒ³ãƒãƒ¼æ™‚é–“ã§æ­£åˆã§ã‚‚BBCã®å½“æ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯é…ä¿¡ã•ã‚Œãªã„ã®ã§ã€æ˜¨æ—¥ã®æ—¥ä»˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã™ã‚‹ã“ã¨ã¨ã—ã¦ã‚‹
def get_yesterday_date_mmt():
    now_mmt = datetime.now(MMT)
    yesterday_mmt = now_mmt - timedelta(days=1)
    return yesterday_mmt.date()

# ä»Šæ—¥ã®æ—¥ä»˜
# def get_today_date_mmt():
#     now_mmt = datetime.now(MMT)
#     return now_mmt.date()

# å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆå…¨ãƒ¡ãƒ‡ã‚£ã‚¢å…±é€šã§ä½¿ç”¨ã™ã‚‹ï¼‰
NEWS_KEYWORDS = ["á€™á€¼á€”á€ºá€™á€¬", "á€™á€¼á€”á€ºá€™á€¬á€·", "á€—á€™á€¬", "á€¡á€±á€¬á€„á€ºá€†á€”á€ºá€¸á€…á€¯á€€á€¼á€Šá€º", "á€™á€„á€ºá€¸á€¡á€±á€¬á€„á€ºá€œá€¾á€­á€¯á€„á€º", "Myanmar", "Burma"]
NEWS_KEYWORDS = [unicodedata.normalize('NFC', kw) for kw in NEWS_KEYWORDS]

def clean_html_content(html: str) -> str:
    html = html.replace("\xa0", " ").replace("&nbsp;", " ")
    # åˆ¶å¾¡æ–‡å­—ï¼ˆã‚«ãƒ†ã‚´ãƒªCï¼‰ã‚’é™¤å¤–ã€å¯è¦–Unicodeã¯ãã®ã¾ã¾
    return ''.join(c for c in html if unicodedata.category(c)[0] != 'C')

def clean_text(text: str) -> str:
    import unicodedata
    if not text:
        return ""
    return ''.join(
        c if (unicodedata.category(c)[0] != 'C' and c != '\xa0') else ' '
        for c in text
    )

def get_frontier_articles_for(date_obj):
    base_url = "https://www.frontiermyanmar.net"
    list_url = base_url + "/en/news"
    res = requests.get(list_url, timeout=10)
    soup = BeautifulSoup(res.content, "html.parser")
    links = soup.select("div.teaser a")
    article_urls = [base_url + a["href"] for a in links if a.get("href", "").startswith("/")]

    filtered_articles = []
    for url in article_urls:
        try:
            res_article = requests.get(url, timeout=10)
            soup_article = BeautifulSoup(res_article.content, "html.parser")
            time_tag = soup_article.find("time")
            if not time_tag:
                continue
            date_str = time_tag.get("datetime", "")
            if not date_str:
                continue
            article_date = datetime.fromisoformat(date_str).date()
            if article_date == date_obj:
                title = soup_article.find("h1").get_text(strip=True)
                filtered_articles.append({
                    "url": url,
                    "title": title,
                    "date": article_date.isoformat()
                })
        except Exception:
            continue

    return filtered_articles

def get_mizzima_articles_for(date_obj, base_url, source_name):
    list_url = base_url  # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
    res = requests.get(list_url, timeout=10)
    soup = BeautifulSoup(res.content, "html.parser")
    links = soup.find_all("a", href=True)

    # URLã« /YYYY/MM/DD/ ãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ã®ã¿
    date_pattern = re.compile(r"/\d{4}/\d{2}/\d{2}/")
    article_urls = [a["href"] for a in links if date_pattern.search(a["href"])]

    target_date_str = date_obj.strftime("%Y/%m/%d")  # ä¾‹: "2025/08/02"

    filtered_articles = []
    for url in article_urls:
        if target_date_str not in url:
            continue  # URLã«æ˜¨æ—¥ã®æ—¥ä»˜ãŒç„¡ã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

        try:
            res_article = requests.get(url, timeout=10)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_tag = soup_article.find("h1")
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)

            # â˜… æŒ‡å®šdivå†…ã®pã‚¿ã‚°ã®ã¿å–å¾—
            content_divs = soup_article.select("div.mag-post-single, div.entry-content")
            paragraphs = []
            for div in content_divs:
                paragraphs += div.find_all("p")
                
            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            body_text = unicodedata.normalize('NFC', body_text)

            if not body_text.strip():
                continue  # æœ¬æ–‡ãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—

            # ã‚¿ã‚¤ãƒˆãƒ«oræœ¬æ–‡ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°å¯¾è±¡ã¨ã™ã‚‹
            if not any(keyword in title or keyword in body_text for keyword in NEWS_KEYWORDS):
                continue

            filtered_articles.append({
                "source": source_name,
                "url": url,
                "title": title,
                "date": date_obj.isoformat()
            })

        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return filtered_articles

def get_vom_articles_for(date_obj):
    base_url = "https://voiceofmyanmarnews.com"
    list_url = base_url + "/?cat=1"
    res = requests.get(list_url, timeout=10)
    soup = BeautifulSoup(res.content, "html.parser")
    links = soup.select("h2.entry-title a")
    article_urls = [a["href"] for a in links if a.get("href", "").startswith("https://")]

    filtered_articles = []
    for url in article_urls:
        try:
            res_article = requests.get(url, timeout=10)
            soup_article = BeautifulSoup(res_article.content, "html.parser")
            date_div = soup_article.select_one("time.entry-date")
            if not date_div:
                continue
            date_text = date_div.get_text(strip=True)
            # ä¾‹: "July 25, 2025" ã‚’ãƒ‘ãƒ¼ã‚¹
            try:
                article_date = datetime.strptime(date_text, "%B %d, %Y").date()
            except ValueError:
                continue
            if article_date == date_obj:
                title = soup_article.find("h1").get_text(strip=True)
                filtered_articles.append({
                    "url": url,
                    "title": title,
                    "date": article_date.isoformat()
                })
        except Exception:
            continue

    return filtered_articles

def get_ludu_articles_for(date_obj):
    base_url = "https://ludunwayoo.com"
    list_url = base_url + "/en/news"
    res = requests.get(list_url, timeout=10)
    soup = BeautifulSoup(res.content, "html.parser")
    links = soup.select("h2.entry-title a")
    article_urls = [a["href"] for a in links if a.get("href", "").startswith("http")]

    filtered_articles = []
    for url in article_urls:
        try:
            res_article = requests.get(url, timeout=10)
            soup_article = BeautifulSoup(res_article.content, "html.parser")
            time_tag = soup_article.find("time")
            if not time_tag:
                continue
            date_str = time_tag.get("datetime", "")
            if not date_str:
                continue
            article_date = datetime.fromisoformat(date_str).date()
            if article_date == date_obj:
                title = soup_article.find("h1").get_text(strip=True)
                filtered_articles.append({
                    "url": url,
                    "title": title,
                    "date": article_date.isoformat()
                })
        except Exception:
            continue

    return filtered_articles

# BCCã¯RSSã‚ã‚‹ã®ã§ãã‚Œä½¿ã†
def get_bbc_burmese_articles_for(target_date_mmt):
    rss_url = "https://feeds.bbci.co.uk/burmese/rss.xml"
    res = requests.get(rss_url, timeout=10)
    soup = BeautifulSoup(res.content, "xml")

    articles = []
    for item in soup.find_all("item"):
        pub_date_tag = item.find("pubDate")
        if not pub_date_tag:
            continue

        try:
            pub_date = parse_date(pub_date_tag.text)  # RSSã¯UTCåŸºæº–
            pub_date_mmt = pub_date.astimezone(MMT).date()  # â† MMTã«å¤‰æ›ã—ã¦æ—¥ä»˜æŠ½å‡º
        except Exception as e:
            print(f"âŒ pubDate parse error: {e}")
            continue

        if pub_date_mmt != target_date_mmt:
            continue  # æ˜¨æ—¥(MMTåŸºæº–)ã®æ—¥ä»˜ã¨ä¸€è‡´ã—ãªã„è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—

        title = item.find("title").text.strip()
        link = item.find("link").text.strip()

        try:
            article_res = requests.get(link, timeout=10)
            article_soup = BeautifulSoup(article_res.content, "html.parser")
            paragraphs = article_soup.find_all("p")
            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            # ã“ã“ã§NFCæ­£è¦åŒ–ã‚’è¿½åŠ 
            body_text = unicodedata.normalize('NFC', body_text)

            if not any(keyword in title or keyword in body_text for keyword in NEWS_KEYWORDS):
                continue  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ãªã‘ã‚Œã°é™¤å¤–

            print(f"âœ… æŠ½å‡ºè¨˜äº‹: {title} ({link})")  # ãƒ­ã‚°å‡ºåŠ›ã§æŠ½å‡ºè¨˜äº‹ç¢ºèª
            articles.append({
                "title": title,
                "url": link,
                "date": pub_date_mmt.isoformat()
            })

        except Exception as e:
            print(f"âŒ è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return articles

# def get_bbc_burmese_articles_for(date_obj):
#     base_url = "https://www.bbc.com"
#     list_url = base_url + "/burmese"
#     res = requests.get(list_url, timeout=10)
#     soup = BeautifulSoup(res.content, "html.parser")
#     links = soup.select("a[href^='/burmese/']")
#     article_urls = [
#         base_url + a["href"]
#         for a in links
#         if any(part in a["href"] for part in ["articles", "media"])
#     ]

#     seen = set()
#     filtered_articles = []
#     for url in article_urls:
#         if url in seen:
#             continue
#         seen.add(url)
#         try:
#             res_article = requests.get(url, timeout=10)
#             soup_article = BeautifulSoup(res_article.content, "html.parser")
#             time_tag = soup_article.find("time")
#             if not time_tag:
#                 continue
#             date_str = time_tag.get("datetime", "")
#             if not date_str:
#                 continue
#             article_date = datetime.fromisoformat(date_str).date()
#             if article_date == date_obj:
#                 title = soup_article.find("h1").get_text(strip=True)
#                 filtered_articles.append({
#                     "url": url,
#                     "title": title,
#                     "date": article_date.isoformat()
#                 })
#         except Exception:
#             continue

#     return filtered_articles

def get_yktnews_articles_for(date_obj):
    base_url = "https://yktnews.com"
    list_url = base_url + "/category/news/"
    res = requests.get(list_url, timeout=10)
    soup = BeautifulSoup(res.content, "html.parser")
    links = soup.find_all("a", href=True)

    # URLã« /YYYY/MM/ ãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ã®ã¿
    date_pattern = re.compile(r"/\d{4}/\d{2}/")
    article_urls = [a["href"] for a in links if date_pattern.search(a["href"])]

    print(article_urls)

    target_date_str = date_obj.strftime("%Y-%m-%d")  # ä¾‹: "2025-08-02"
    target_month_str = date_obj.strftime("%Y/%m")  # ä¾‹: "2025/08"

    filtered_articles = []
    for url in article_urls:
        if target_month_str not in url:
            continue  # URLã«å¯¾è±¡æœˆãŒç„¡ã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

        try:
            res_article = requests.get(url, timeout=10)
            
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            # æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
            time_tag = soup_article.select_one("div.tdb-block-inner time.entry-date")
            if not time_tag or not time_tag.has_attr("datetime"):
                continue

            date_str = time_tag["datetime"]
            article_date = datetime.fromisoformat(date_str).astimezone(MMT).date()
            if article_date != date_obj:
                continue  # æ˜¨æ—¥ã®æ—¥ä»˜ã§ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_tag = soup_article.find("h1")
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)

            # æœ¬æ–‡å–å¾— (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹å¼)
            paragraphs = soup_article.select("div.tdb-block-inner p")
            if not paragraphs:
                paragraphs = soup_article.select("div.tdb_single_content p")
            if not paragraphs:
                paragraphs = soup_article.select("article p")
            if not paragraphs:
                paragraphs = soup_article.find_all("p")  # æœ€çµ‚æ‰‹æ®µï¼šå…¨Pã‚¿ã‚°
            
            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            body_text = unicodedata.normalize('NFC', body_text)

            if not body_text.strip():
                continue  # æœ¬æ–‡ãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—

            # ã‚¿ã‚¤ãƒˆãƒ«oræœ¬æ–‡ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°å¯¾è±¡ã¨ã™ã‚‹
            if not any(keyword in title or keyword in body_text for keyword in NEWS_KEYWORDS):
                continue

            filtered_articles.append({
                "url": url,
                "title": title,
                "date": date_obj.isoformat()
            })

        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return filtered_articles


# Chat GPTä½¿ã†å ´åˆ
# def translate_and_summarize(text: str) -> str:
#     if not text or not text.strip():
#         print("âš ï¸ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™ã€‚")
#         return "ï¼ˆç¿»è¨³ãƒ»è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸï¼‰"

#     prompt = (
#         "ä»¥ä¸‹ã®è¨˜äº‹ã®å†…å®¹ã«ã¤ã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ã¾ã¨ã‚ã€å…·ä½“çš„ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚" 
#         "æ–‡å­—æ•°ã¯800æ–‡å­—ã¾ã§ã¨ã—ã¾ã™ã€‚è‡ªç„¶ãªæ—¥æœ¬èªã«è¨³ã—ã¦ãã ã•ã„ã€‚\n\n"
#         f"{text[:2000]}"  # å…¥åŠ›é•·ã‚’é©åˆ‡ã«åˆ¶é™ï¼ˆAPIã®å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚’è¶…ãˆãªã„ã‚ˆã†ã«ï¼‰
#     )

#     try:
#         response = client.chat.completions.create(
#             model="gpt-3.5-turbo",
#             messages=[{"role": "user", "content": prompt}]
#         )
#         return response.choices[0].message.content.strip()

#     except OpenAIError as api_err:
#         # OpenAIå…¨ä½“ã®ä¾‹å¤–ã‚’ç¶²ç¾…
#         print(f"ğŸ›‘ OpenAI API ã‚¨ãƒ©ãƒ¼: {api_err}")
#         return "ï¼ˆç¿»è¨³ãƒ»è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸï¼‰"
#     except Exception as e:
#         # ãã®ä»–ã®äºˆæœŸã—ãªã„ä¾‹å¤–
#         print(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
#         return "ï¼ˆç¿»è¨³ãƒ»è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸï¼‰"

# BERTåŸ‹ã‚è¾¼ã¿ã§é¡ä¼¼è¨˜äº‹åˆ¤å®š
def deduplicate_articles(articles, similarity_threshold=0.92):
    if not articles:
        return []

    # é‡è¤‡ã—ãŸå ´åˆã®è¨˜äº‹å„ªå…ˆåº¦
    media_priority = {
        "BBC Burmese": 1,
        "Mizzima (English)": 2,
        "Mizzima (Burmese)": 3,
        "YKT News": 4
    }

    model = SentenceTransformer('cl-tohoku/bert-base-japanese-v2')
    texts = [art['title'] + " " + art['body'][:300] for art in articles]  # æœ¬æ–‡ã¯å…ˆé ­300æ–‡å­—ã ã‘
    embeddings = model.encode(texts, convert_to_tensor=True)

    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings).cpu().numpy()

    visited = set()
    unique_articles = []

    # ã¾ãšã‚¿ã‚¤ãƒˆãƒ«å®Œå…¨ä¸€è‡´ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
    title_seen = {}
    for idx, art in enumerate(articles):
        if art['title'] in title_seen:
            # ãƒ‡ãƒãƒƒã‚¯
            print(f"ğŸ›‘ Duplicate Title Found: '{art['title']}'\n - Kept: {articles[title_seen[art['title']]]['source']} | {articles[title_seen[art['title']]]['url']}\n - Removed: {art['source']} | {art['url']}")
            continue  # ã™ã§ã«åŒã˜ã‚¿ã‚¤ãƒˆãƒ«ã®è¨˜äº‹ãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
        title_seen[art['title']] = idx
        unique_articles.append(art)
        visited.add(idx)

    # æ¬¡ã«BERTãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åˆ¤å®š
    for i in range(len(articles)):
        if i in visited:
            continue

        group = [i]
        for j in range(i + 1, len(articles)):
            if cosine_scores[i][j] > similarity_threshold:
                # ãƒ‡ãƒãƒƒã‚¯
                print(f"ğŸ›‘ BERT Duplicate Found:\n - Kept Candidate: {articles[i]['source']} | {articles[i]['title']} | {articles[i]['url']}\n - Removed Candidate: {articles[j]['source']} | {articles[j]['title']} | {articles[j]['url']}\n (Similarity: {cosine_scores[i][j]:.4f})")
                group.append(j)
                visited.add(j)

        group_sorted = sorted(group, key=lambda idx: media_priority.get(articles[idx]['source'], 99))
        unique_articles.append(articles[group_sorted[0]])
        visited.add(i)

    return unique_articles

# ç¿»è¨³å¯¾è±¡ã‚­ãƒ¥ãƒ¼
translation_queue = []

def process_and_enqueue_articles(articles, source_name, seen_urls=None):
    if seen_urls is None:
        seen_urls = set()

    queued_items = []
    for art in articles:
        if art['url'] in seen_urls:
            continue
        seen_urls.add(art['url'])

        try:
            res = requests.get(art['url'], timeout=10)
            soup = BeautifulSoup(res.content, "html.parser")
            paragraphs = soup.find_all("p")
            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)

            # â˜…ã“ã“ã§NEWS_KEYWORDSãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ã‹ã‘ã‚‹
            if not any(keyword in art['title'] or keyword in body_text for keyword in NEWS_KEYWORDS):
                continue  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«ã¾ã‚Œã¦ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

            queued_items.append({
                "source": source_name,
                "url": art["url"],
                "title": art["title"],  # ç¿»è¨³å‰ã‚¿ã‚¤ãƒˆãƒ«
                "body": body_text,      # ç¿»è¨³å‰æœ¬æ–‡
            })
        except Exception as e:
            print(f"Error processing {art['url']}: {e}")
            continue

    translation_queue.extend(queued_items)

def process_translation_batches(batch_size=10, wait_seconds=60):
    summarized_results = []

    for i in range(0, len(translation_queue), batch_size):
        batch = translation_queue[i:i + batch_size]
        print(f"âš™ï¸ Processing batch {i // batch_size + 1}...")

        for item in batch:
            prompt = (
                "ä»¥ä¸‹ã¯è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã§ã™ã€‚è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘ â—¯â—¯ã€ã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã¯è¿”ã—ã¦ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®æ–‡è¨€ã¯ä¸è¦ã§ã™ã€‚\n"
                "###\n"
                f"{item['title']}\n"
                "###\n\n"
                "ä»¥ä¸‹ã®è¨˜äº‹ã®æœ¬æ–‡ã«ã¤ã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ã¾ã¨ã‚å…·ä½“çš„ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚è‡ªç„¶ãªæ—¥æœ¬èªã«è¨³ã—ã¦ãã ã•ã„ã€‚\n"
                "å€‹åˆ¥è¨˜äº‹ã®æœ¬æ–‡ã®è¦ç´„ã®ã¿ã¨ã—ã¦ãã ã•ã„ã€‚ãƒ¡ãƒ‡ã‚£ã‚¢ã®èª¬æ˜ã‚„ãƒšãƒ¼ã‚¸å…¨ä½“ã®è§£èª¬ã¯ä¸è¦ã§ã™ã€‚\n"
                "ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã¯è¦ç´„ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€ãã‚Œä»¥å¤–ã®æ–‡è¨€ã¯ä¸è¦ã§ã™ã€‚\n"
                "ä»¥ä¸‹ã€å‡ºåŠ›ã®æ¡ä»¶ã§ã™ã€‚\n"
                "- 1è¡Œç›®ã¯ã€Œã€è¦ç´„ã€‘ã€ã¨ã ã‘ã—ã¦ãã ã•ã„ã€‚"
                "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã‚’é©åˆ‡ã«ä½¿ã£ã¦è¦‹ã‚„ã™ãæ•´ç†ã—ã¦ãã ã•ã„ã€‚\n"
                "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã«ã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ï¼ˆ#, *, - ãªã©ï¼‰ã¯ä½¿ã‚ãšã€å˜ç´”ãªãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
                "- è¦‹å‡ºã—ã¯ `[  ]` ã§å›²ã‚“ã§ãã ã•ã„ã€‚\n"
                "- ãƒ†ã‚­ã‚¹ãƒˆãŒå…¥ã£ã¦ã„ãªã„æ”¹è¡Œã¯ä½œã‚‰ãªã„ã§ãã ã•ã„ã€‚\n"
                "- å…¨ä½“ã‚’HTMLã§é€ã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã§ã€ç‰¹æ®Šè¨˜å·ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚\n"
                "- ç®‡æ¡æ›¸ãã¯ã€Œãƒ»ã€ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚\n"
                "- è¦ç´„ã®æ–‡å­—æ•°ã¯æœ€å¤§500æ–‡å­—ã‚’è¶…ãˆã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
                "###\n"
                f"{item['body'][:2000]}\n"
                "###"
            )

            try:
                resp = client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt
                )
                output_text = resp.text.strip()

                # ãƒ‘ãƒ¼ã‚¹
                lines = output_text.splitlines()
                title_line = next((line for line in lines if line.startswith("ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘")), None)
                summary_lines = [line for line in lines if line and not line.startswith("ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘")]

                if title_line:
                    translated_title = title_line.replace("ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘", "").strip()
                else:
                    translated_title = "ï¼ˆç¿»è¨³å¤±æ•—ï¼‰"

                summary_text = "\n".join(summary_lines).strip()
                summary_html = summary_text.replace("\n", "<br>")

                summarized_results.append({
                    "source": item["source"],
                    "url": item["url"],
                    "title": translated_title,
                    "summary": summary_html,
                })

            except Exception as e:
                print(f"ğŸ›‘ Error during translation: {e}")
                continue

        if i + batch_size < len(translation_queue):
            print(f"ğŸ•’ Waiting {wait_seconds} seconds before next batch...")
            time.sleep(wait_seconds)

    return summarized_results

def send_email_digest(summaries):
    sender_email = os.getenv("EMAIL_SENDER")
    sender_pass = os.getenv("GMAIL_APP_PASSWORD")
    recipient_emails = os.getenv("EMAIL_RECIPIENTS", "").split(",")

    # âœ… ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    digest_date = get_yesterday_date_mmt()
    # digest_date = get_today_date_mmt()
    date_str = digest_date.strftime("%Yå¹´%-mæœˆ%-dæ—¥") + "åˆ†"

    # ãƒ¡ãƒ‡ã‚£ã‚¢ã”ã¨ã«ã¾ã¨ã‚ã‚‹
    media_grouped = defaultdict(list)
    for item in summaries:
        media_grouped[item["source"]].append(item)

    # ãƒ¡ãƒ¼ãƒ«ã‚¿ã‚¤ãƒˆãƒ«
    subject = "ãƒŸãƒ£ãƒ³ãƒãƒ¼é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€" + date_str + "ã€‘"

    # ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã®HTMLç”Ÿæˆ
    html_content = f"""
    <html>
    <body style="font-family: Arial, sans-serif; background-color: #ffffff; color: #333333;">
    """

    for media, articles in media_grouped.items():
        html_content += f"<h3 style='color: #2a2a2a; margin-top: 30px;'>{media} ã‹ã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹</h3>"

        for item in articles:
            title_jp = "ã‚¿ã‚¤ãƒˆãƒ«: " + item["title"]
            url = item["url"]

            summary_html = item["summary"]  # ã™ã§ã«HTMLæ•´å½¢æ¸ˆã¿ã‚’ãã®ã¾ã¾ä½¿ã†
            html_content += (
                f"<div style='margin-bottom: 20px;'>"
                f"<h4 style='margin-bottom: 5px;'>{title_jp}</h4>"
                f"<p><a href='{url}' style='color: #1a0dab;'>æœ¬æ–‡ã‚’èª­ã‚€</a></p>"
                f"<div style='background-color: #f9f9f9; padding: 10px; border-radius: 8px;'>"
                f"{summary_html}"
                f"</div></div><hr style='border-top: 1px solid #cccccc;'>"
            )

    html_content += "</body></html>"
    html_content = clean_html_content(html_content)

    from_display_name = "Myanmar News Digest"

    msg = EmailMessage(policy=SMTPUTF8)
    msg["Subject"] = subject
    msg["From"] = formataddr((from_display_name, sender_email))
    msg["To"] = ", ".join(recipient_emails)
    msg.set_content("HTMLãƒ¡ãƒ¼ãƒ«ã‚’é–‹ã‘ã‚‹ç’°å¢ƒã§ã”ç¢ºèªãã ã•ã„ã€‚", charset="utf-8")
    msg.add_alternative(html_content, subtype="html", charset="utf-8")

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender_email, sender_pass)
            server.send_message(msg)
            print("âœ… ãƒ¡ãƒ¼ãƒ«é€ä¿¡å®Œäº†")
    except Exception as e:
        print(f"âŒ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    date_mmt = get_yesterday_date_mmt()
    # date_mmt = get_today_date_mmt()
    seen_urls = set()
    
    # articles = get_frontier_articles_for(date_mmt)
    # for art in articles:
    #     print(f"{art['date']} - {art['title']}\n{art['url']}\n")

    # è¨˜äº‹å–å¾—ï¼†ã‚­ãƒ¥ãƒ¼ã«è²¯ã‚ã‚‹
    print("=== Mizzima (English) ===")
    articles_eng = get_mizzima_articles_for(date_mmt, "https://eng.mizzima.com", "Mizzima (English)")
    process_and_enqueue_articles(articles_eng, "Mizzima (English)", seen_urls)
    
    print("=== Mizzima (Burmese) ===")
    articles_bur = get_mizzima_articles_for(date_mmt, "https://bur.mizzima.com", "Mizzima (Burmese)")
    process_and_enqueue_articles(articles_bur, "Mizzima (Burmese)", seen_urls)

    # print("=== Voice of Myanmar ===")
    # articles4 = get_vom_articles_for(date_mmt)
    # for art in articles4:
    #     print(f"{art['date']} - {art['title']}\n{art['url']}\n")

    # print("=== Ludu Wayoo ===")
    # articles5 = get_ludu_articles_for(date_mmt)
    # for art in articles5:
    #     print(f"{art['date']} - {art['title']}\n{art['url']}\n")

    print("=== BBC Burmese ===")
    articles6 = get_bbc_burmese_articles_for(date_mmt)
    process_and_enqueue_articles(articles6, "BBC Burmese", seen_urls)

    print("=== YKT News ===")
    articles7 = get_yktnews_articles_for(date_mmt)
    process_and_enqueue_articles(articles7, "YKT News", seen_urls)

    # âœ… å…¨è¨˜äº‹å–å¾—å¾Œ â†’ BERTé¡ä¼¼åº¦ã§é‡è¤‡æ’é™¤
    print(f"âš™ï¸ Deduplicating {len(translation_queue)} articles...")
    deduplicated_articles = deduplicate_articles(translation_queue)

    # translation_queue ã‚’é‡è¤‡æ’é™¤å¾Œã®ãƒªã‚¹ãƒˆã«ç½®ãæ›ãˆ
    translation_queue.clear()
    translation_queue.extend(deduplicated_articles)

    # ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œ (10ä»¶ã”ã¨ã«1åˆ†å¾…æ©Ÿ)
    all_summaries = process_translation_batches(batch_size=10, wait_seconds=60)

    send_email_digest(all_summaries)
