import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, date, timezone
from dateutil.parser import parse as parse_date
import re
# Chat GPT
# from openai import OpenAI, OpenAIError
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import os
import sys
from email import policy  # â† è¿½åŠ 
from email.header import Header  # â† è¿½åŠ å¿…è¦
from email.message import EmailMessage
from email.policy import SMTPUTF8
from email.utils import formataddr
import unicodedata
from google import genai
from google.api_core.exceptions import GoogleAPICallError
from collections import defaultdict
import time
import json
import pprint

# è¨˜äº‹é‡è¤‡æ’é™¤ãƒ­ã‚¸ãƒƒã‚¯(BERTåŸ‹ã‚è¾¼ã¿ç‰ˆ)ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sentence_transformers import SentenceTransformer, util

# Geminiæœ¬ç•ªç”¨
# client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# GeminiTESTç”¨
client = genai.Client(api_key=os.getenv("GEMINI_TEST_API_KEY"))

# Chat GPT
# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ãƒŸãƒ£ãƒ³ãƒãƒ¼æ¨™æº–æ™‚ (UTC+6:30)
MMT = timezone(timedelta(hours=6, minutes=30))

# ä»Šæ—¥ã®æ—¥ä»˜
# ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é€Ÿå ±æ€§é‡è¦–ã§ä»Šæ—¥åˆ†ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã®æ–¹é‡
def get_today_date_mmt():
    # æœ¬ç•ªç”¨ã€ä»Šæ—¥ã®æ—¥ä»˜
    # now_mmt = datetime.now(MMT)
    # ãƒ†ã‚¹ãƒˆç”¨ã€æ˜¨æ—¥ã®æ—¥ä»˜ã«ã™ã‚‹
    now_mmt = datetime.now(MMT) - timedelta(days=1)
    return now_mmt.date()

# å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆå…¨ãƒ¡ãƒ‡ã‚£ã‚¢å…±é€šã§ä½¿ç”¨ã™ã‚‹ï¼‰
NEWS_KEYWORDS = [
    # ãƒŸãƒ£ãƒ³ãƒãƒ¼ï¼ˆå›½åãƒ»ç¾è¡Œåç§°ï¼‰
    "á€™á€¼á€”á€ºá€™á€¬", "á€™á€¼á€”á€ºá€™á€¬á€·", "Myanmar", "myanmar",
    
    # ãƒ“ãƒ«ãƒï¼ˆæ—§å›½åãƒ»é€šç§°ï¼‰
    "á€—á€™á€¬", "Burma", "burma",
    
    # ã‚¢ã‚¦ãƒ³ã‚µãƒ³ã‚¹ãƒ¼ãƒãƒ¼ï¼ˆAung San Suu Kyiï¼‰
    "á€¡á€±á€¬á€„á€ºá€†á€”á€ºá€¸á€…á€¯á€€á€¼á€Šá€º", "Aung San Suu Kyi", "aung san suu kyi",
    
    # ãƒŸãƒ³ã‚¢ã‚¦ãƒ³ãƒ•ãƒ©ã‚¤ãƒ³ï¼ˆMin Aung Hlaingï¼‰
    "á€™á€„á€ºá€¸á€¡á€±á€¬á€„á€ºá€œá€¾á€­á€¯á€„á€º", "Min Aung Hlaing", "min aung hlaing",
    
    # ãƒãƒ£ãƒƒãƒˆï¼ˆKyatï¼‰
    "Kyat", "kyat",
    
    # å¾´å…µåˆ¶ï¼ˆConscription / Military Draftï¼‰, å¾´å…µ, å…µå½¹
    "Conscription", "conscription", "Military Draft", "military draft", "military service", "military service", "á€…á€…á€ºá€á€•á€ºá€á€„á€ºá€á€±á€«á€ºá€›á€±á€¸", "á€…á€…á€ºá€™á€¾á€¯á€‘á€™á€ºá€¸", "á€¡á€á€„á€ºá€¸á€á€•á€ºá€á€„á€ºá€á€±á€«á€ºá€á€¼á€„á€ºá€¸", "á€á€•á€ºá€á€„á€ºá€á€±á€«á€º "
]

# Unicodeæ­£è¦åŒ–ï¼ˆNFCï¼‰ã‚’é©ç”¨
NEWS_KEYWORDS = [unicodedata.normalize('NFC', kw) for kw in NEWS_KEYWORDS]

# ãƒãƒ£ãƒƒãƒˆã¯æ•°å­—ã«ç¶šãã‚‚ã®ã®ã¿ï¼ˆé€šè²¨åˆ¤å®šï¼‰
KYAT_PATTERN = re.compile(
    r'(?<=[0-9á€-á‰])[\s,\.]*(?:á€á€±á€¬á€„á€ºá€¸|á€á€­á€”á€ºá€¸|á€á€”á€ºá€¸)?\s*á€€á€»á€•á€º'
)

def any_keyword_hit(title: str, body: str) -> bool:
    # é€šå¸¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´
    if any(kw in title or kw in body for kw in NEWS_KEYWORDS):
        return True
    # é€šè²¨ã€Œá€€á€»á€•á€ºã€ã ã‘ã¯æ­£è¦è¡¨ç¾ã§åˆ¤å®š
    if KYAT_PATTERN.search(title) or KYAT_PATTERN.search(body):
        return True
    return False

def clean_html_content(html: str) -> str:
    html = html.replace("\xa0", " ").replace("&nbsp;", " ")
    # åˆ¶å¾¡æ–‡å­—ï¼ˆã‚«ãƒ†ã‚´ãƒªCï¼‰ã‚’é™¤å¤–ã€å¯è¦–Unicodeã¯ãã®ã¾ã¾
    return ''.join(c for c in html if unicodedata.category(c)[0] != 'C')

# æœ¬æ–‡ãŒå–å¾—ã§ãã‚‹ã¾ã§ã€Œrequestsã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€
def fetch_with_retry(url, retries=3, wait_seconds=2):
    for attempt in range(retries):
        try:
            res = requests.get(url, timeout=10)
            if res.status_code == 200 and res.text.strip():
                return res
        except Exception as e:
            print(f"Attempt {attempt + 1} failed for {url}: {e}")
        time.sleep(wait_seconds)
    raise Exception(f"Failed to fetch {url} after {retries} attempts.")

# æœ¬æ–‡ãŒç©ºãªã‚‰ã€Œä¸€å®šç§’æ•°å¾…ã£ã¦å†å–å¾—ã€
def extract_paragraphs_with_wait(soup_article, retries=2, wait_seconds=2):
    for attempt in range(retries + 1):
        paragraphs = soup_article.select("div.entry-content p")
        if not paragraphs:
            paragraphs = soup_article.select("div.node-content p")
        if not paragraphs:
            paragraphs = soup_article.select("article p")
        if not paragraphs:
            paragraphs = soup_article.find_all("p")

        if paragraphs:
            return paragraphs
        
        print(f"Paragraphs not found, waiting {wait_seconds}s and retrying...")
        time.sleep(wait_seconds)
    return []


# Mizzimaã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸å·¡å›ã§å–å¾—
def get_mizzima_articles_from_category(date_obj, base_url, source_name, category_path, max_pages=3):
    # ==== ãƒ­ãƒ¼ã‚«ãƒ«å®šæ•° Mizzimaé™¤å¤–å¯¾è±¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼‰====
    EXCLUDE_TITLE_KEYWORDS = [
        # æ˜¥ã®é©å‘½æ—¥èªŒ
        "á€”á€½á€±á€¦á€¸á€á€±á€¬á€ºá€œá€¾á€”á€ºá€›á€±á€¸ á€”á€±á€·á€…á€‰á€ºá€™á€¾á€á€ºá€…á€¯",
        # å†™çœŸãƒ‹ãƒ¥ãƒ¼ã‚¹
        "á€“á€¬á€á€ºá€•á€¯á€¶á€á€á€„á€ºá€¸"
    ]

    article_urls = []

    for page_num in range(1, max_pages + 1):
        if page_num == 1:
            url = f"{base_url}{category_path}"
        else:
            url = f"{base_url}{category_path}/page/{page_num}/"

        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                continue

            soup = BeautifulSoup(res.content, "html.parser")
            links = [a['href'] for a in soup.select("main.site-main article a.post-thumbnail[href]")]
            article_urls.extend(links)

        except Exception as e:
            print(f"Error crawling category page {url}: {e}")
            continue

    filtered_articles = []
    for url in article_urls:
        try:
            res_article = fetch_with_retry(url)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            meta_tag = soup_article.find("meta", property="article:published_time")
            if not meta_tag or not meta_tag.has_attr("content"):
                continue

            date_str = meta_tag["content"]
            article_datetime_utc = datetime.fromisoformat(date_str)
            article_datetime_mmt = article_datetime_utc.astimezone(MMT)
            article_date = article_datetime_mmt.date()

            if article_date != date_obj:
                continue

            title_tag = soup_article.find("meta", attrs={"property": "og:title"})
            if not title_tag or not title_tag.has_attr("content"):
                continue
            title = title_tag["content"].strip()

            # === é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‚’NFCæ­£è¦åŒ–ã—ã¦ã‹ã‚‰ï¼‰ ===
            title_nfc = unicodedata.normalize('NFC', title)
            if any(kw in title_nfc for kw in EXCLUDE_TITLE_KEYWORDS):
                print(f"SKIP: excluded keyword in title â†’ {url} | TITLE: {title_nfc}")
                continue

            content_div = soup_article.find("div", class_="entry-content")
            if not content_div:
                continue

            paragraphs = []
            for p in content_div.find_all("p"):
                if p.find_previous("h2", string=re.compile("Related Posts", re.I)):
                    break
                paragraphs.append(p)

            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            body_text = unicodedata.normalize('NFC', body_text)

            if not body_text.strip():
                continue

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šã¯æ­£è¦åŒ–æ¸ˆã¿ã‚¿ã‚¤ãƒˆãƒ«ã§è¡Œã†
            if not any_keyword_hit(title, body_text):
                continue

            filtered_articles.append({
                "source": source_name,
                "url": url,
                "title": title,
                "date": article_date.isoformat()
            })

        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return filtered_articles

# BCCã¯RSSã‚ã‚‹ã®ã§ãã‚Œä½¿ã†
def get_bbc_burmese_articles_for(target_date_mmt):
    # ==== ãƒ­ãƒ¼ã‚«ãƒ«å®šæ•° ====
    NOISE_PATTERNS = [
        r"BBC\s*News\s*á€™á€¼á€”á€ºá€™á€¬",  # å›ºå®šç½²åï¼ˆBurmeseè¡¨è¨˜ï¼‰
        r"BBC\s*Burmese"        # è‹±èªè¡¨è¨˜
    ]

    # ==== ãƒ­ãƒ¼ã‚«ãƒ«é–¢æ•° ====
    def _remove_noise_phrases(text: str) -> str:
        """BBCç½²åãªã©ã®ãƒã‚¤ã‚ºãƒ•ãƒ¬ãƒ¼ã‚ºã‚’é™¤å»"""
        if not text:
            return text
        for pat in NOISE_PATTERNS:
            text = re.sub(pat, "", text, flags=re.IGNORECASE)
        return text.strip()

    # ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆä¸­ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã“ã«ãƒ’ãƒƒãƒˆã—ãŸã‹ã‚’è¿”ã™ï¼ˆå‘¨è¾ºæ–‡è„ˆã¤ãï¼‰
    def _find_hits(text: str, keywords):
        hits = []
        for kw in keywords:
            start = 0
            while True:
                i = text.find(kw, start)
                if i == -1:
                    break
                s = max(0, i-30); e = min(len(text), i+len(kw)+30)
                ctx = text[s:e].replace("\n", " ")
                hits.append({"kw": kw, "pos": i, "ctx": ctx})
                start = i + len(kw)
        return hits

    rss_url = "https://feeds.bbci.co.uk/burmese/rss.xml"
    session = requests.Session()

    try:
        res = session.get(rss_url, timeout=10)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    soup = BeautifulSoup(res.content, "xml")
    articles = []

    for item in soup.find_all("item"):
        pub_date_tag = item.find("pubDate")
        if not pub_date_tag:
            continue

        # RSSã¯UTC â†’ MMTã¸å¤‰æ›ã—ã€å¯¾è±¡æ—¥ã ã‘é€šã™
        try:
            pub_date = parse_date(pub_date_tag.text)
            pub_date_mmt = pub_date.astimezone(MMT).date()
        except Exception as e:
            print(f"âŒ pubDate parse error: {e}")
            continue

        if pub_date_mmt != target_date_mmt:
            continue

        title = (item.find("title") or {}).get_text(strip=True) if item.find("title") else ""
        link = (item.find("link") or {}).get_text(strip=True) if item.find("link") else ""
        if not link:
            continue

        try:
            article_res = session.get(link, timeout=10)
            article_res.raise_for_status()
            article_soup = BeautifulSoup(article_res.content, "html.parser")

            # ===== ã“ã“ã§é™¤å¤–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã¾ã¨ã‚ã¦å‰Šé™¤ =====
            # è¨˜äº‹ç½²åã‚„ãƒ¡ã‚¿æƒ…å ±
            for node in article_soup.select('section[role="region"][aria-labelledby="article-byline"]'):
                node.decompose()
            # ã€ŒãŠã™ã™ã‚ï¼æœ€ã‚‚èª­ã¾ã‚ŒãŸã€ãƒ–ãƒ­ãƒƒã‚¯
            for node in article_soup.select('section[data-e2e="recommendations-heading"][role="region"]'):
                node.decompose()
            # ã¤ã„ã§ã«ãƒ˜ãƒƒãƒ€ãƒ¼/ãƒŠãƒ“/ãƒ•ãƒƒã‚¿ãƒ¼ç­‰ã®ãƒã‚¤ã‚ºã‚‚è½ã¨ã—ã¦ãŠãï¼ˆä»»æ„ï¼‰
            for node in article_soup.select('header[role="banner"], nav[role="navigation"], footer[role="contentinfo"], aside'):
                node.decompose()
            # ============================================

            # æœ¬æ–‡ã¯ main å†…ã® <p> ã«é™å®š
            main = article_soup.select_one('main[role="main"]') or article_soup
            paragraphs = [p.get_text(strip=True) for p in main.find_all('p')]
            # ç©ºè¡Œã‚„ãƒã‚¤ã‚ºã‚’å‰Šã‚‹
            paragraphs = [t for t in paragraphs if t]
            body_text = "\n".join(paragraphs)

            # ãƒŸãƒ£ãƒ³ãƒãƒ¼æ–‡å­—ã®åˆæˆå·®ç•°ã‚’é¿ã‘ã‚‹ãŸã‚ NFC æ­£è¦åŒ–
            title_nfc = unicodedata.normalize('NFC', title)
            title_nfc = _remove_noise_phrases(title_nfc)
            body_text_nfc = unicodedata.normalize('NFC', body_text)
            body_text_nfc = _remove_noise_phrases(body_text_nfc)

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
            if not any_keyword_hit(title_nfc, body_text_nfc):
                print(f"SKIP: no keyword hits â†’ {link} | TITLE: {title_nfc}")
                continue

            # # === ãƒ‡ãƒãƒƒã‚°: åˆ¤å®šå‰ã«ã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡ã®è¦ç´„ã‚’å‡ºã™ ===
            # print("----- DEBUG CANDIDATE -----")
            # print("URL:", link)
            # print("TITLE:", repr(title_nfc))
            # print("BODY_HEAD:", repr(body_text_nfc[:500]))
            # print("BODY_LEN:", len(body_text_nfc))

            # # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆãƒ’ãƒƒãƒˆè©³ç´°ã‚‚å–ã‚‹ï¼‰
            # title_hits = _find_hits(title_nfc, NEWS_KEYWORDS)
            # body_hits  = _find_hits(body_text_nfc, NEWS_KEYWORDS)
            # total_hits = title_hits + body_hits

            # if not total_hits:
            #     print("SKIP: no keyword hits.")
            #     continue

            # # === ãƒ‡ãƒãƒƒã‚°: ã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã“ã§å½“ãŸã£ãŸã‹ ===
            # print("HITS:", len(total_hits))
            # if title_hits:
            #     print(" - in TITLE:")
            #     for h in title_hits[:10]:
            #         print(f"   kw={repr(h['kw'])} ctx=â€¦{h['ctx']}â€¦")
            # if body_hits:
            #     print(" - in BODY:")
            #     for h in body_hits[:10]:  # é•·ããªã‚‹ã®ã§æœ€å¤§10ä»¶
            #         print(f"   kw={repr(h['kw'])} ctx=â€¦{h['ctx']}â€¦")

            print(f"âœ… æŠ½å‡ºè¨˜äº‹: {title_nfc} ({link})")
            articles.append({
                "title": title_nfc,
                "url": link,
                "date": pub_date_mmt.isoformat(),
            })

        except Exception as e:
            print(f"âŒ è¨˜äº‹å–å¾—/è§£æã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return articles

# khit_thit_ediaã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸å·¡å›ã§å–å¾—
def get_khit_thit_edia_articles_from_category(date_obj, max_pages=3):
    base_url="https://yktnews.com/category/news/"
    article_urls = []

    for page in range(1, max_pages + 1):
        url = f"{base_url}page/{page}/" if page > 1 else base_url
        print(f"Fetching {url}")
        res = fetch_with_retry(url)
        soup = BeautifulSoup(res.content, "html.parser")

        # è¨˜äº‹ãƒªãƒ³ã‚¯æŠ½å‡º
        entry_links = soup.select('p.entry-title.td-module-title a[href]')
        page_article_urls = [a['href'] for a in entry_links if a.has_attr('href')]
        article_urls.extend(page_article_urls)

    filtered_articles = []
    for url in article_urls:
        try:
            res_article = fetch_with_retry(url)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            # æ—¥ä»˜å–å¾—
            meta_tag = soup_article.find("meta", property="article:published_time")
            if not meta_tag or not meta_tag.has_attr("content"):
                continue
            date_str = meta_tag["content"]
            article_datetime_utc = datetime.fromisoformat(date_str)
            article_datetime_mmt = article_datetime_utc.astimezone(MMT)
            article_date = article_datetime_mmt.date()

            if article_date != date_obj:
                continue  # å¯¾è±¡æ—¥ã§ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_tag = soup_article.find("h1")
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)

            # æœ¬æ–‡å–å¾— (khit_thit_ediaç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³)
            paragraphs = soup_article.select("div.tdb-block-inner p")
            if not paragraphs:
                paragraphs = soup_article.select("div.tdb_single_content p")
            if not paragraphs:
                paragraphs = soup_article.select("article p")
            if not paragraphs:
                paragraphs = soup_article.find_all("p")
            
            paragraphs = extract_paragraphs_with_wait(soup_article)
            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            body_text = unicodedata.normalize('NFC', body_text)

            if not body_text.strip():
                continue  # æœ¬æ–‡ãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—

            if not any_keyword_hit(title, body_text):
                continue  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç„¡ã—ã¯é™¤å¤–

            filtered_articles.append({
                "url": url,
                "title": title,
                "date": date_obj.isoformat()
            })

        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return filtered_articles

# irrawaddy
def get_irrawaddy_articles_for(date_obj, debug=True):
    """
    æŒ‡å®šã® Irrawaddy ã‚«ãƒ†ã‚´ãƒªURLç¾¤ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ï¼‰ã‚’1å›ãšã¤å·¡å›ã—ã€
    MMTã®æŒ‡å®šæ—¥(æ—¢å®š: ä»Šæ—¥)ã‹ã¤ any_keyword_hit ã«ãƒ’ãƒƒãƒˆã™ã‚‹è¨˜äº‹ã®ã¿è¿”ã™ã€‚

    - /category/news/asia, /category/news/world ã¯é™¤å¤–ï¼ˆå…ˆé ­ä¸€è‡´ãƒ»å¤§å°ç„¡è¦–ï¼‰
    - ä¸€è¦§ã§ã¯ã€Œæ™‚è¨ˆã‚¢ã‚¤ã‚³ãƒ³ä»˜ãã®æ—¥ä»˜ãƒªãƒ³ã‚¯ã€ã‹ã‚‰å½“æ—¥å€™è£œã‚’æŠ½å‡º
    - è¨˜äº‹å´ã§ã¯ <meta property="article:published_time"> ã‚’ MMT ã«å¤‰æ›ã—ã¦å†ç¢ºèª
    - æœ¬æ–‡ã¯ <div class="content-inner "> é…ä¸‹ã® <p> ã‹ã‚‰æŠ½å‡ºï¼ˆç‰¹å®šãƒ–ãƒ­ãƒƒã‚¯é…ä¸‹ã¯é™¤å¤–ï¼‰
    è¿”ã‚Šå€¤: [{url, title, date}]
    ä¾å­˜: MMT, get_today_date_mmt, fetch_with_retry, any_keyword_hit
    """

    session = requests.Session()

    # ==== å·¡å›å¯¾è±¡ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã€é‡è¤‡ã‚ã‚Šã§ã‚‚OKï¼šå†…éƒ¨ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰ ====
    CATEGORY_PATHS_RAW = [
        "/category/news/",
        "/category/politics",
        "/category/news/war-against-the-junta",
        "/category/news/conflicts-in-numbers",
        "/category/news/junta-crony",
        "/category/news/ethnic-issues",
        "/category/business",
        "/category/business/economy",
        # "/category/election-2020", # 2021å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        "/category/Features",
        "/category/Opinion",
        "/category/Opinion/editorial",
        "/category/Opinion/commentary",
        "/category/Opinion/guest-column",
        "/category/Opinion/analysis",
        # "/category/Opinion/letters", # 2014å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        "/category/in-person",
        "/category/in-person/interview",
        "/category/in-person/profile",
        # "/category/Dateline", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        "/category/Specials",
        "/category/specials/women",
        # "/category/specials/places-in-history", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/specials/on-this-day", # 2023å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        "/category/from-the-archive",
        # "/category/Specials/myanmar-covid-19", # 2022å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        "/category/Specials/myanmar-china-watch",
        # "/category/Lifestyle", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Travel", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle/Food", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle/fashion-design", # 2019å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/photo", # 2016å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/photo-essay", # 2021å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
    ]
    BASE = "https://www.irrawaddy.com"
    EXCLUDE_PREFIXES = ["/category/news/asia", "/category/news/world"]  # å…ˆé ­ä¸€è‡´ãƒ»å¤§å°ç„¡è¦–

    # ==== æ­£è¦åŒ–ãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ãƒ»é™¤å¤– ====
    norm = lambda p: re.sub(r"/{2,}", "/", p.strip())
    paths, seen = [], set()
    for p in CATEGORY_PATHS_RAW:
        q = norm(p)
        if any(q.lower().startswith(x) for x in EXCLUDE_PREFIXES):
            continue
        if q not in seen:
            seen.add(q)
            paths.append(q)

    # ==== ãƒ­ãƒ¼ã‚«ãƒ«é–¢æ•° ====
    def _norm_text(text: str) -> str:
        return unicodedata.normalize('NFC', text)

    def _parse_category_date_text(text: str):
        # ä¾‹: 'August 9, 2025'
        text = re.sub(r"\s+", " ", text.strip())
        return datetime.strptime(text, "%B %d, %Y").date()

    def _article_date_from_meta_mmt(soup):
        meta = soup.find("meta", attrs={"property": "article:published_time"})
        if not meta or not meta.get("content"):
            return None
        iso = meta["content"].replace("Z", "+00:00")  # æœ«å°¾Zå¯¾ç­–
        dt = datetime.fromisoformat(iso)
        return dt.astimezone(MMT).date()

    def _extract_title(soup):
        t = soup.find("title")
        return _norm_text(t.get_text(strip=True)) if t else None

    def _is_excluded_by_ancestor(node) -> bool:
        excluded = {
            "jnews_inline_related_post",
            "jeg_postblock_21",
            "widget", "widget_jnews_popular",
            "jeg_postblock_5",
            "jnews_related_post_container",
            "jeg_footer_primary",
        }
        for anc in node.parents:
            classes = anc.get("class", [])
            if any(c in excluded for c in classes):
                return True
        return False

    def _extract_body_irrawaddy(soup):
        # <div class="content-inner "> é…ä¸‹ã® <p>ã®ã¿ï¼ˆé™¤å¤–ãƒ–ãƒ­ãƒƒã‚¯é…ä¸‹ã¯é™¤å¤–ï¼‰
        paragraphs = []
        content_inners = soup.select("div.content-inner")
        if not content_inners:
            content_inners = [div for div in soup.find_all("div")
                            if "content-inner" in (div.get("class") or [])]
        for root in content_inners:
            for p in root.find_all("p"):
                if _is_excluded_by_ancestor(p):
                    continue
                txt = p.get_text(strip=True)
                if txt:
                    paragraphs.append(_norm_text(txt))
        return "\n".join(paragraphs).strip()
    
    def _fetch_with_retry_irrawaddy(url, retries=3, wait_seconds=2, session=None):
        """
        Irrawaddyå°‚ç”¨ãƒ•ã‚§ãƒƒãƒãƒ£ï¼šæœ€åˆã‹ã‚‰ cloudscraper ã§å–å¾—ã—ã€403/429/503 ã¯æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§å†è©¦è¡Œã€‚
        æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ requests ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã»ã¼åˆ°é”ã—ãªã„æƒ³å®šï¼‰ã€‚
        """
        import random
        try:
            import cloudscraper
        except ImportError:
            raise RuntimeError("cloudscraper ãŒå¿…è¦ã§ã™ã€‚pip install cloudscraper ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

        sess = session or requests.Session()

        UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/126.0.0.0 Safari/537.36")
        HEADERS = {
            "User-Agent": UA,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Referer": "https://www.irrawaddy.com/",
            "Connection": "keep-alive",
        }

        # cloudscraper ã‚’æœ€åˆã«ä½¿ã†ï¼ˆæ—¢å­˜ Session ã‚’ãƒ©ãƒƒãƒ—ã—ã¦ã‚¯ãƒƒã‚­ãƒ¼å…±æœ‰ï¼‰
        scraper = cloudscraper.create_scraper(
            sess=sess,
            browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
        )

        for attempt in range(retries):
            try:
                r = scraper.get(url, headers=HEADERS, timeout=30, allow_redirects=True)
                print(f"[fetch-cs] {attempt+1}/{retries}: HTTP {r.status_code} len={len(getattr(r,'text',''))} â†’ {url}")
                if r.status_code == 200 and getattr(r, "text", "").strip():
                    return r
                if r.status_code in (403, 429, 503):
                    time.sleep(wait_seconds * (2 ** attempt) + random.uniform(0, 0.8))
                    continue
                break
            except Exception as e:
                print(f"[fetch-cs] {attempt+1}/{retries} EXC: {e} â†’ {url}")
                time.sleep(wait_seconds * (2 ** attempt) + random.uniform(0, 0.8))

        # éå¸¸ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã»ã¼ä¸è¦ï¼‰ã€‚æˆåŠŸã™ã‚Œã°è¿”ã™ã€‚
        try:
            r2 = sess.get(url, headers=HEADERS, timeout=20, allow_redirects=True)
            print(f"[fetch-rq] final: HTTP {r2.status_code} len={len(r2.text)} â†’ {url}")
            if r2.status_code == 200 and r2.text.strip():
                return r2
        except Exception as e:
            print(f"[fetch-rq] EXC final: {e} â†’ {url}")

        raise Exception(f"Failed to fetch {url} after {retries} attempts.")

    # 2) ç°¡æ˜“ãƒ­ã‚¬ãƒ¼ï¼ˆæ¶ˆã™æ™‚ã¯ã“ã®1è¡Œã¨ dbg(...) ã‚’æ¶ˆã™ã ã‘ï¼‰
    dbg = (lambda *a, **k: print(*a, **k)) if debug else (lambda *a, **k: None)

    results = []
    seen_urls = set()
    candidate_urls = []

    # âœ… ã“ã‚Œã‚’è¿½åŠ ï¼ˆã¾ãŸã¯å…¥ã‚Œç›´ã™ï¼‰
    _shown_parsefail = 0
    _shown_mismatch  = 0

    # ==== 1) ã‚«ãƒ†ã‚´ãƒªå·¡å› ====
    for rel_path in paths:
        url = f"{BASE}{rel_path}"
        print(f"Fetching {url}")
        try:
            res = _fetch_with_retry_irrawaddy(url, session=session)
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            continue

        soup = BeautifulSoup(res.content, "html.parser")
        wrapper = soup.select_one("div.jnews_category_content_wrapper")
        scope = wrapper if wrapper else soup

        links = scope.select("div.jeg_postblock_content .jeg_meta_date a[href]")
        if not links:
            links = scope.select(".jeg_post_meta .jeg_meta_date a[href]")
        if not links:
            links = [a for a in scope.select("div.jeg_postblock_content a[href]")
                    if a.find("i", class_="fa fa-clock-o")]

        # ãƒ‡ãƒãƒƒã‚°ï¼šä½•ä»¶æ‹¾ãˆãŸã‹ï¼†å…ˆé ­2ä»¶ã®ä¸­èº«
        dbg(f"[cat] date-links={len(links)} @ {url}")
        for a in links[:2]:
            _txt = re.sub(r"\s+", " ", a.get_text(" ", strip=True))
            dbg("   â†’", _txt, "|", a.get("href"))

        if not links:
            dbg(f"[cat] no date links @ {url}")
            continue

        for a in links:
            href = a.get("href")
            raw = a.get_text(" ", strip=True)
            try:
                shown_date = _parse_category_date_text(raw)
            except Exception:
                if _shown_parsefail < 3:
                    dbg("[cat] date-parse-fail:", re.sub(r"\s+", " ", raw)[:120])
                    _shown_parsefail += 1
                continue

            if shown_date == date_obj:
                if href and href not in seen_urls:
                    candidate_urls.append(href)
                    seen_urls.add(href)
            else:
                if _shown_mismatch < 3:
                    dbg("[cat] date-mismatch:", shown_date, "target:", date_obj, "â†’", href)
                    _shown_mismatch += 1

    dbg(f"[cat] candidates={len(candidate_urls)}")

    # ==== 2) è¨˜äº‹ç¢ºèª ====
    for url in candidate_urls:
        try:
            res_article = _fetch_with_retry_irrawaddy(url, session=session)
        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

        soup_article = BeautifulSoup(res_article.content, "html.parser")

        meta_date = _article_date_from_meta_mmt(soup_article)
        if meta_date is None:
            dbg("[art] meta-missing:", url)
            continue
        if meta_date != date_obj:
            dbg("[art] meta-mismatch:", meta_date, "target:", date_obj, "â†’", url)
            continue

        title = _extract_title(soup_article)
        if not title:
            dbg("[art] title-missing:", url)
            continue

        body = _extract_body_irrawaddy(soup_article)
        if not body:
            dbg("[art] body-empty:", url)
            continue

        if not any_keyword_hit(title, body):
            dbg("[art] keyword-not-hit:", url)
            continue

        results.append({
            "url": url,
            "title": title,
            "date": date_obj.isoformat(),
        })

    dbg(f"[final] kept={len(results)}")

    # results = []
    # seen_urls = set()
    # candidate_urls = []

    # # ==== 1) å„ã‚«ãƒ†ã‚´ãƒªURLã‚’1å›ãšã¤å·¡å› â†’ å½“æ—¥å€™è£œæŠ½å‡º ====
    # for rel_path in paths:
    #     url = f"{BASE}{rel_path}"
    #     print(f"Fetching {url}")
    #     try:
    #         res = _fetch_with_retry_irrawaddy(url, session=session)
    #     except Exception as e:
    #         print(f"Error fetching {url}: {e}")
    #         continue

    #     soup = BeautifulSoup(res.content, "html.parser")
    #     wrapper = soup.select_one("div.jnews_category_content_wrapper")
    #     scope = wrapper if wrapper else soup

    #     links = scope.select("div.jeg_postblock_content .jeg_meta_date a[href]")
    #     if not links:
    #         # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ™‚è¨ˆã‚¢ã‚¤ã‚³ãƒ³ã‚’å«ã‚€ a
    #         links = [a for a in scope.select("div.jeg_postblock_content a[href]")
    #                 if a.find("i", class_="fa fa-clock-o")]

    #     for a in links:
    #         if not a.find("i", class_="fa fa-clock-o"):
    #             continue
    #         href = a.get("href")
    #         if not href or href in seen_urls:
    #             continue
    #         try:
    #             shown_date = _parse_category_date_text(a.get_text(" ", strip=True))
    #         except Exception:
    #             continue
    #         if shown_date == date_obj:
    #             candidate_urls.append(href)
    #             seen_urls.add(href)

    # # ==== 2) å€™è£œè¨˜äº‹ã§å³å¯†ç¢ºèªï¼ˆmetaæ—¥ä»˜/æœ¬æ–‡/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰ ====
    # for url in candidate_urls:
    #     try:
    #         res_article = _fetch_with_retry_irrawaddy(url, session=session)
    #         soup_article = BeautifulSoup(res_article.content, "html.parser")

    #         if _article_date_from_meta_mmt(soup_article) != date_obj:
    #             continue

    #         title = _extract_title(soup_article)
    #         if not title:
    #             continue

    #         body = _extract_body_irrawaddy(soup_article)
    #         if not body:
    #             continue

    #         if not any_keyword_hit(title, body):
    #             continue

    #         results.append({
    #             "url": url,
    #             "title": title,
    #             "date": date_obj.isoformat(),
    #         })
    #     except Exception as e:
    #         print(f"Error processing {url}: {e}")
    #         continue

    return results

# åŒã˜URLã®é‡è¤‡å‰Šé™¤
def deduplicate_by_url(articles):
    seen_urls = set()
    unique_articles = []
    for art in articles:
        if art['url'] in seen_urls:
            print(f"ğŸ›‘ URL Duplicate Removed: {art['source']} | {art['title']} | {art['url']}")
            continue
        seen_urls.add(art['url'])
        unique_articles.append(art)
    return unique_articles

# ç¿»è¨³å¯¾è±¡ã‚­ãƒ¥ãƒ¼
translation_queue = []

def process_and_enqueue_articles(articles, source_name, seen_urls=None):
    if seen_urls is None:
        seen_urls = set()

    queued_items = []
    for art in articles:
        if art['url'] in seen_urls:
            continue
        seen_urls.add(art['url'])

        try:
            res = requests.get(art['url'], timeout=10)
            soup = BeautifulSoup(res.content, "html.parser")
            # æœ¬æ–‡pã‚¿ã‚°å–å¾— (ãƒªãƒˆãƒ©ã‚¤ä»˜ã)
            paragraphs = extract_paragraphs_with_wait(soup, retries=2, wait_seconds=2)
            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)

            title_nfc = unicodedata.normalize('NFC', art['title'])
            body_nfc  = unicodedata.normalize('NFC', body_text)

            # â˜…ã“ã“ã§NEWS_KEYWORDSãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ã‹ã‘ã‚‹
            if not any(keyword in title_nfc or keyword in body_nfc for keyword in NEWS_KEYWORDS):
                continue  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«ã¾ã‚Œã¦ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

            queued_items.append({
                "source": source_name,
                "url": art["url"],
                "title": art["title"],  # ç¿»è¨³å‰ã‚¿ã‚¤ãƒˆãƒ«
                "body": body_text,      # ç¿»è¨³å‰æœ¬æ–‡
            })
        except Exception as e:
            print(f"Error processing {art['url']}: {e}")
            continue

    translation_queue.extend(queued_items)

# ãƒ‡ãƒãƒƒã‚¯ç”¨é–¢æ•°
# def process_translation_batches(batch_size=10, wait_seconds=60):
#     summarized_results = []

#     # ãƒ†ã‚¹ãƒˆç”¨ã« translation_queue ã®ä¸­èº«ã‚’ãã®ã¾ã¾ summarized_results ã«è©°ã‚ã‚‹
#     for item in translation_queue:
#         summarized_results.append({
#             "source": item["source"],
#             "url": item["url"],
#             "title": item["title"],      # ç¿»è¨³å‰ã‚¿ã‚¤ãƒˆãƒ«
#             "summary": item["body"][:2000]  # è¦ç´„ã®ä»£ã‚ã‚Šã«æœ¬æ–‡å†’é ­
#         })

#     # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆsummarized_results ã®ä¸­èº«ã‚’çœç•¥ã›ãšç¢ºèªï¼‰
#     print("===== DEBUG: summarized_results =====")
#     pprint.pprint(summarized_results, width=120, compact=False)
#     print("===== END DEBUG =====")

#     # ã“ã“ã§å‡¦ç†çµ‚äº†
#     return summarized_results

# é‡è¤‡è¨˜äº‹å‰Šé™¤å‡¦ç†ã‚»ãƒƒãƒˆ
def _strip_tags(text: str) -> str:
    # è¦ç´„ã«å«ã‚ãŸ <br> ãªã©ã‚’ç´ ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼ˆæœ€ä½é™ï¼‰
    text = text.replace("<br>", "\n")
    return re.sub(r"<[^>]+>", "", text)

def _safe_json_loads_maybe_extract(text: str):
    """
    ç”ŸæˆAIãŒå‰å¾Œã«ä½™è¨ˆãªæ–‡ã‚’ä»˜ã‘ãŸå ´åˆã§ã‚‚JSONéƒ¨åˆ†ã ã‘æŠ½å‡ºã—ã¦èª­ã‚€ä¿é™ºã€‚
    """
    try:
        return json.loads(text)
    except Exception:
        # æœ€å¾Œã® { ... } ã‚’ç´ æœ´ã«æŠ½å‡º
        m = re.search(r'\{.*\}', text, flags=re.DOTALL)
        if m:
            return json.loads(m.group(0))
        raise

def dedupe_articles_with_llm(client, summarized_results):
    """
    summarized_results (list[dict]) ã‚’å—ã‘å–ã‚Šã€é‡è¤‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã”ã¨ã«1æœ¬ã ã‘æ®‹ã—ãŸé…åˆ—ã‚’è¿”ã™ã€‚
    è¿”å´å½¢å¼ã¯å…ƒã¨åŒã˜ï¼ˆsource, url, title, summary ã®ã¿ï¼‰ã€‚
    """
    if not summarized_results:
        return summarized_results

    # ===== â‘  summarized_results ã®ã¾ã¾è¡¨ç¤º =====
    print("===== DEBUG 1: summarized_results BEFORE DEDUPE =====")
    pprint.pprint(summarized_results, width=120, compact=False)
    print("===== END DEBUG 1 =====\n")

    # LLMå…¥åŠ›ç”¨ã« articles ã‚’æ§‹ç¯‰ï¼ˆid ã¯URLå„ªå…ˆã€ãªã‘ã‚Œã°é€£ç•ªï¼‰
    articles = []
    id_map = {}
    for idx, it in enumerate(summarized_results):
        _id = it.get("url") or f"idx-{idx}"
        # å†…éƒ¨ç”¨ã®åŸæœ¬ï¼ˆè¿”å´æ™‚ã«ãã®ã¾ã¾ä½¿ã†ï¼‰
        id_map[_id] = it

        # æœ¬æ–‡ç›¸å½“ã¨ã—ã¦ summary ã‚’æ¸¡ã™ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®ä¸¡æ–¹ã‚’æ¯”è¼ƒã•ã›ã‚‹ï¼‰
        articles.append({
            "id": _id,
            "source": it.get("source"),
            "title": it.get("title"),
            "body": _strip_tags(it.get("summary", "")),
        })

    # ===== LLMã«æ¸¡ã™articlesã‚‚ç¢ºèª =====
    print("===== DEBUG 2: articles SENT TO LLM =====")
    pprint.pprint(articles, width=120, compact=False)
    print("===== END DEBUG 2 =====\n")

    prompt = (
        "ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡è¤‡åˆ¤å®šãƒ•ã‚£ãƒ«ã‚¿ã§ã™ã€‚\n"
        "ç›®çš„ï¼šã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’æ¯”è¼ƒã—ã€ã€ŒåŒä¸€ã®å‡ºæ¥äº‹ã€ã‚’å ±ã˜ã‚‹è¨˜äº‹ã‚’é‡è¤‡ã¨ã—ã¦æŸã­ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰1æœ¬ã ã‘æ®‹ã—ã¾ã™ã€‚\n"
        "å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ã€‚\n\n"
        "åˆ¤å®šæ–¹é‡:\n"
        "1) åŒä¸€å‡ºæ¥äº‹ï¼ã€Œèª°ã€ã€Œä½•ã‚’ã€ã€Œã©ã“/å¯¾è±¡ã€ã€Œã„ã¤ã€ã®å°‘ãªãã¨ã‚‚3è¦ç´ ãŒä¸€è‡´ã—ã€ã‚³ã‚¢äº‹å®ŸãŒåŒã˜ï¼ˆè¨€ã„æ›ãˆãƒ»è¨€èªå·®ã¯åŒä¸€æ‰±ã„ã€‚æ—¥ä»˜ã¯Â±14æ—¥ã¾ã§åŒä¸€æ‰±ã„ï¼‰ã€‚\n"
        "2) ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–ï¼šæœ€ã‚‚ä¸€è‡´åº¦ãŒé«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã®ã¿æ‰€å±ã€‚\n"
        "3) æ®‹ã™åŸºæº–ï¼ša)å›ºæœ‰æƒ…å ±é‡ãŒå¤šã„ b)å…·ä½“æ€§/æ˜ç­ã• c)æœ¬æ–‡ãŒé•·ã„ d)åŒç‚¹ãªã‚‰sourceã®æ–‡å­—åˆ—æ˜‡é †ã€‚\n"
        "4) çµ±åˆè¨˜äº‹ã¯ä½œã‚‰ãªã„ã€‚å…¥åŠ›å¤–ã®äº‹å®Ÿã¯åŠ ãˆãªã„ã€‚\n\n"
        "å…¥åŠ›:\n"
        "{\n  \"articles\": " + json.dumps(articles, ensure_ascii=False) + "\n}\n\n"
        "å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSONã®ã¿ï¼‰:\n"
        "{\n"
        "  \"kept\": [\n"
        "    {\"id\": \"<æ®‹ã™è¨˜äº‹ID>\", \"cluster_id\": \"<ID>\", \"why\": \"<1-2æ–‡>\"}\n"
        "  ],\n"
        "  \"removed\": [\n"
        "    {\"id\": \"<é™¤å¤–è¨˜äº‹ID>\", \"duplicate_of\": \"<æ®‹ã—ãŸè¨˜äº‹ID>\", \"why\": \"<1-2æ–‡>\"}\n"
        "  ],\n"
        "  \"clusters\": [\n"
        "    {\"cluster_id\": \"<ID>\", \"member_ids\": [\"<id1>\", \"<id2>\", \"...\"], \"event_key\": \"<å‡ºæ¥äº‹ã®çŸ­æ–‡>\"}\n"
        "  ]\n"
        "}\n"
    )

    try:
        resp = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
        data = _safe_json_loads_maybe_extract(resp.text)
        kept_ids = [x.get("id") for x in data.get("kept", []) if x.get("id") in id_map]

        # å…ƒã®é †åºã‚’ä¿ã£ãŸã¾ã¾ãƒ•ã‚£ãƒ«ã‚¿
        kept_set = set(kept_ids)
        if kept_set:
            filtered = [obj for obj in summarized_results if (obj.get("url") or f"idx-{summarized_results.index(obj)}") in kept_set]
            return filtered

        # ã†ã¾ãåˆ¤å®šã§ããªã‹ã£ãŸã‚‰åŸæœ¬ã‚’è¿”ã™
        return summarized_results
    except Exception as e:
        print(f"ğŸ›‘ Dedupe failed, returning original list: {e}")
        return summarized_results

# æœ¬å‡¦ç†é–¢æ•°
def process_translation_batches(batch_size=10, wait_seconds=60):

    # âš ï¸ TEST: Geminiã‚’å‘¼ã°ãšã€URLãƒªã‚¹ãƒˆã ã‘è¿”ã™
    # summarized_results = []
    # for item in translation_queue:
    #     summarized_results.append({
    #         "source": item["source"],
    #         "url": item["url"],
    #         "title": item['title'],
    #         "summary": item['body'][:2000]
    #     })

    summarized_results = []
    for i in range(0, len(translation_queue), batch_size):
        batch = translation_queue[i:i + batch_size]
        print(f"âš™ï¸ Processing batch {i // batch_size + 1}...")

        for item in batch:
            prompt = (
                "æ¬¡ã®æ‰‹é †ã§è¨˜äº‹ã‚’åˆ¤å®šãƒ»å‡¦ç†ã—ã¦ãã ã•ã„ã€‚\n\n"
                "Step 1: ä¾‹å¤–ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰\n"
                "Q1. è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯æœ¬æ–‡ãŒ `Myawaddy`, `á€™á€¼á€á€á€®`, `Muse`, `á€™á€°á€†á€šá€º`ã«é–¢ã™ã‚‹å†…å®¹ã§ã™ã‹ï¼Ÿ\n"
                "â†’ Yes ã®å ´åˆã€ã“ã®å¾Œã®åˆ¤å®šã¯è¡Œã‚ãš Step 3 ã«é€²ã‚“ã§ãã ã•ã„ã€‚\n"
                "â†’ No ã®å ´åˆã¯ Step 2 ã¸é€²ã‚“ã§ãã ã•ã„ã€‚\n\n"
                "Step 2: é™¤å¤–æ¡ä»¶ãƒã‚§ãƒƒã‚¯\n"
                "Q2. ç‰¹å®šã®åœ°åŸŸï¼ˆéƒ¡åŒºã€ã‚¿ã‚¦ãƒ³ã‚·ãƒƒãƒ—ã€å¸‚ã€æ‘ï¼‰ã§ç™ºç”Ÿã—ãŸå±€åœ°çš„ãªæˆ¦é—˜ã€ç´›äº‰ã€æ”»æ’ƒã€è¡çªã€çˆ†æ’ƒã€å¼·ç›—ã€æŠ—è­°æ´»å‹•ã«é–¢ã™ã‚‹è¨˜äº‹ã§ã™ã‹ï¼Ÿï¼ˆåœ°åŸŸå…¨ä½“ã®è¢«å®³å ±å‘Šãƒ»çµ±è¨ˆã‚‚å«ã‚€ï¼‰\n"
                "â†’ Yes ã®å ´åˆã¯å‡¦ç†ã‚’çµ‚äº†ã—ã¦ãã ã•ã„ã€Step 3 ã«ã¯é€²ã¾ãªã„ã§ãã ã•ã„ã€`exit`ã ã‘ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\n"
                "â†’ No ã®å ´åˆã¯ Step 3 ã¸é€²ã‚“ã§ãã ã•ã„ã€‚\n\n"
                "Step 3: ç¿»è¨³ã¨è¦ç´„å‡¦ç†\n"
                "ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã€æœ¬æ–‡ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\n"
                "ã‚¿ã‚¤ãƒˆãƒ«ï¼š\n"
                "- è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
                "- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã¯å¿…ãšã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘ â—¯â—¯ã€ã®å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚\n"
                "- ãã‚Œä»¥å¤–ã®æ–‡è¨€ã¯ä¸è¦ã§ã™ã€‚\n\n"
                "æœ¬æ–‡è¦ç´„ï¼š\n"
                "- ä»¥ä¸‹ã®è¨˜äº‹æœ¬æ–‡ã«ã¤ã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ã¾ã¨ã‚ã€å…·ä½“çš„ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
                "- è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
                "- å€‹åˆ¥è¨˜äº‹ã®æœ¬æ–‡ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€ãƒ¡ãƒ‡ã‚£ã‚¢èª¬æ˜ã‚„ãƒšãƒ¼ã‚¸å…¨ä½“ã®è§£èª¬ã¯ä¸è¦ã§ã™ã€‚\n"
                "- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã¯è¦ç´„ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€ãã‚Œä»¥å¤–ã®æ–‡è¨€ã¯ä¸è¦ã§ã™ã€‚\n\n"
                "å‡ºåŠ›æ¡ä»¶ï¼š\n"
                "- 1è¡Œç›®ã¯`ã€è¦ç´„ã€‘`ã¨ã ã‘ã—ã¦ãã ã•ã„ã€‚\n"
                "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã‚’é©åˆ‡ã«ä½¿ã£ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚\n"
                "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã«ã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ï¼ˆ#, *, - ãªã©ï¼‰ã‚’ä½¿ã‚ãšã€å˜ç´”ãªãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
                "- è¦‹å‡ºã—ã¯ `[ ]` ã§å›²ã‚“ã§ãã ã•ã„ã€‚\n"
                "- ç©ºè¡Œã¯ä½œã‚‰ãªã„ã§ãã ã•ã„ã€‚\n"
                "- ç‰¹æ®Šè¨˜å·ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ï¼ˆå…¨ä½“ã‚’HTMLã¨ã—ã¦é€ä¿¡ã™ã‚‹ã‚ã‘ã§ã¯ãªã„ãŸã‚ï¼‰ã€‚\n"
                "- ç®‡æ¡æ›¸ãã¯`ãƒ»`ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚\n"
                "- è¦ç´„ã®æ–‡å­—æ•°ã¯æœ€å¤§500æ–‡å­—ã¨ã—ã¦ãã ã•ã„ã€‚\n\n"
                "å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼š\n"
                "###\n"
                "[è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«]\n"
                "###\n"
                f"{item['title']}\n\n"
                "[è¨˜äº‹æœ¬æ–‡]\n"
                "###\n"
                f"{item['body'][:2000]}\n"
                "###\n"
            )

            try:
                # ãƒ‡ãƒãƒƒã‚°: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                print("----- DEBUG: Prompt Input -----")
                print(f"TITLE: {item['title']}")
                print(f"BODY[:2000]: {item['body'][:2000]}")

                resp = client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt
                )
                output_text = resp.text.strip()

                # ãƒ‡ãƒãƒƒã‚°: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’ç¢ºèª
                print("----- DEBUG: Model Output -----")
                print(output_text)

                # exitãŒè¿”ã£ã¦ããŸã‚‰ã‚¹ã‚­ãƒƒãƒ—
                if output_text.strip().lower() == "exit":
                    continue

                # ã‚¿ã‚¤ãƒˆãƒ«è¡Œã¨è¦ç´„ã®æŠ½å‡º
                lines = output_text.splitlines()
                title_line = next((line for line in lines if line.startswith("ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘")), None)
                summary_lines = [line for line in lines if line and not line.startswith("ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘")]

                if title_line:
                    translated_title = title_line.replace("ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘", "").strip()
                else:
                    translated_title = "ï¼ˆç¿»è¨³å¤±æ•—ï¼‰"

                summary_text = "\n".join(summary_lines).strip()

                # å‡ºåŠ›æ¡ä»¶ã«æ²¿ã£ã¦HTMLã«å¤‰æ›ï¼ˆæ”¹è¡Œâ†’<br>ï¼‰
                summary_html = summary_text.replace("\n", "<br>")

                summarized_results.append({
                    "source": item["source"],
                    "url": item["url"],
                    "title": translated_title,
                    "summary": summary_html,
                })

            except Exception as e:
                print(f"ğŸ›‘ Error during translation: {e}")
                continue

        if i + batch_size < len(translation_queue):
            print(f"ğŸ•’ Waiting {wait_seconds} seconds before next batch...")
            time.sleep(wait_seconds)

    # é‡è¤‡åˆ¤å®šâ†’ç‰‡æ–¹æ®‹ã—ï¼ˆæœ€çµ‚ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã®å½¢å¼ã¯å¤‰ãˆãªã„ï¼‰
    deduped = dedupe_articles_with_llm(client, summarized_results)

    # å¿µã®ãŸã‚ï¼šè¿”å´ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å›ºå®šï¼ˆä½™è¨ˆãªã‚­ãƒ¼ãŒæ··ã–ã£ã¦ã„ãŸã‚‰è½ã¨ã™ï¼‰
    normalized = [
        {
            "source": x.get("source"),
            "url": x.get("url"),
            "title": x.get("title"),
            "summary": x.get("summary"),
        } for x in deduped
    ]
    return normalized

def send_email_digest(summaries):
    sender_email = os.getenv("EMAIL_SENDER")
    sender_pass = os.getenv("GMAIL_APP_PASSWORD")
    # ãƒ¡ãƒ¼ãƒ«é€ä¿¡å…ˆæœ¬ç•ªç”¨
    # recipient_emails = os.getenv("EMAIL_RECIPIENTS", "").split(",")
    # ãƒ¡ãƒ¼ãƒ«é€ä¿¡å…ˆãƒ†ã‚¹ãƒˆç”¨
    recipient_emails = ["yasu.23721740311@gmail.com"]


    # âœ… ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    digest_date = get_today_date_mmt()
    date_str = digest_date.strftime("%Yå¹´%-mæœˆ%-dæ—¥") + "åˆ†"

    # ãƒ¡ãƒ‡ã‚£ã‚¢ã”ã¨ã«ã¾ã¨ã‚ã‚‹
    media_grouped = defaultdict(list)
    for item in summaries:
        media_grouped[item["source"]].append(item)

    # ãƒ¡ãƒ¼ãƒ«ã‚¿ã‚¤ãƒˆãƒ«
    subject = "ãƒŸãƒ£ãƒ³ãƒãƒ¼é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€" + date_str + "ã€‘"

    # ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã®HTMLç”Ÿæˆ
    html_content = """
    <html>
    <body style="font-family: Arial, sans-serif; background-color: #ffffff; color: #333333;">
    """

    # ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã¯ä½¿ã†ãŒã€è¦‹å‡ºã—ã¯å„è¨˜äº‹ã®ä¸­ã«å…¥ã‚Œã‚‹
    for media, articles in media_grouped.items():
        for item in articles:
            
            title_jp = item["title"]          # ã€Œã‚¿ã‚¤ãƒˆãƒ«: ã€ã®æ¥é ­è¾ã¯å¤–ã™
            url = item["url"]
            summary_html = item["summary"]    # æ—¢ã« <br> æ•´å½¢æ¸ˆã¿

            # å‚è€ƒHTMLæº–æ‹ ï¼šè¦‹å‡ºã—(h2)ã®å³å´ã«ãƒ¡ãƒ‡ã‚£ã‚¢åã€‚
            heading_html = (
                "<h2 style='margin-bottom:5px'>"
                f"{title_jp}ã€€"
                "<span style='font-size:0.83rem;font-weight:600'>"  # â† h5ç›¸å½“
                f"{media} "
                "</span>"
                "</h2>"
            )

            html_content += (
                "<div style='margin-bottom:20px'>"
                f"{heading_html}"
                "<div style='background-color:#f9f9f9;padding:10px;border-radius:8px'>"
                f"{summary_html}"
                "</div>"
                f"<p><a href='{url}' style='color:#1a0dab' target='_blank'>æœ¬æ–‡ã‚’èª­ã‚€</a></p>"
                "</div><hr style='border-top: 1px solid #cccccc;'>"
                "</div>"
            )

    html_content += "</body></html>"
    html_content = clean_html_content(html_content)

    from_display_name = "Myanmar News Digest"

    msg = EmailMessage(policy=SMTPUTF8)
    msg["Subject"] = subject
    msg["From"] = formataddr((from_display_name, sender_email))
    msg["To"] = ", ".join(recipient_emails)
    msg.set_content("HTMLãƒ¡ãƒ¼ãƒ«ã‚’é–‹ã‘ã‚‹ç’°å¢ƒã§ã”ç¢ºèªãã ã•ã„ã€‚", charset="utf-8")
    msg.add_alternative(html_content, subtype="html", charset="utf-8")

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender_email, sender_pass)
            server.send_message(msg)
            print("âœ… ãƒ¡ãƒ¼ãƒ«é€ä¿¡å®Œäº†")
    except Exception as e:
        print(f"âŒ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    date_mmt = get_today_date_mmt()
    seen_urls = set()
    
    # articles = get_frontier_articles_for(date_mmt)
    # for art in articles:
    #     print(f"{art['date']} - {art['title']}\n{art['url']}\n")

    # è¨˜äº‹å–å¾—ï¼†ã‚­ãƒ¥ãƒ¼ã«è²¯ã‚ã‚‹
    # Mizzima (English)å¤–ã™
    # print("=== Mizzima (English) ===")
    # articles_eng = get_mizzima_articles_from_category(
    #     date_mmt,
    #     "https://eng.mizzima.com",
    #     "Mizzima (English)",
    #     "/category/news/myanmar_news",
    #     max_pages=3
    # )
    # process_and_enqueue_articles(articles_eng, "Mizzima (English)", seen_urls)
    
    # === Mizzima (Burmese) ===
    print("=== Mizzima (Burmese) ===")
    articles_bur = get_mizzima_articles_from_category(
        date_mmt,
        "https://bur.mizzima.com",
        "Mizzima (Burmese)",
        "/category/%e1%80%9e%e1%80%90%e1%80%84%e1%80%ba%e1%80%b8/%e1%80%99%e1%80%bc%e1%80%94%e1%80%ba%e1%80%99%e1%80%ac%e1%80%9e%e1%80%90%e1%80%84%e1%80%ba%e1%80%b8",
        max_pages=3
    )
    process_and_enqueue_articles(articles_bur, "Mizzima (Burmese)", seen_urls)

    # print("=== Voice of Myanmar ===")
    # articles4 = get_vom_articles_for(date_mmt)
    # for art in articles4:
    #     print(f"{art['date']} - {art['title']}\n{art['url']}\n")

    # print("=== Ludu Wayoo ===")
    # articles5 = get_ludu_articles_for(date_mmt)
    # for art in articles5:
    #     print(f"{art['date']} - {art['title']}\n{art['url']}\n")

    print("=== BBC Burmese ===")
    articles6 = get_bbc_burmese_articles_for(date_mmt)
    process_and_enqueue_articles(articles6, "BBC Burmese", seen_urls)

    print("=== Khit Thit Media ===")
    articles7 = get_khit_thit_edia_articles_from_category(date_mmt, max_pages=3)
    process_and_enqueue_articles(articles7, "Khit Thit Media", seen_urls)

    print("=== Irrawaddy ===")
    articles8 = get_irrawaddy_articles_for(date_mmt)

    # ãƒ‡ãƒãƒƒã‚¯ã§ãƒ­ã‚°ç¢ºèª
    print("RESULTS:", json.dumps(articles8, ensure_ascii=False, indent=2))
    sys.exit(1)

    process_and_enqueue_articles(articles8, "Irrawaddy", seen_urls)

    # URLãƒ™ãƒ¼ã‚¹ã®é‡è¤‡æ’é™¤ã‚’å…ˆã«è¡Œã†
    print(f"âš™ï¸ Removing URL duplicates from {len(translation_queue)} articles...")
    translation_queue = deduplicate_by_url(translation_queue)

    # ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œ (10ä»¶ã”ã¨ã«1åˆ†å¾…æ©Ÿ)
    all_summaries = process_translation_batches(batch_size=10, wait_seconds=60)

    send_email_digest(all_summaries)
