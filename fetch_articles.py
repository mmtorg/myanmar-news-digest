import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone, date
from dateutil.parser import parse as parse_date
import re

import os
import sys
from email.message import EmailMessage
from email.utils import formataddr
import unicodedata
from google import genai
from collections import defaultdict
import time
import json
import pprint as _pprint
import random
from typing import List, Dict, Optional
from urllib.parse import urlparse  # è¿½åŠ 
from collections import deque
import base64
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.oauth2.credentials import Credentials
from email.policy import SMTP
from email.header import Header
import xml.etree.ElementTree as ET
from urllib.parse import urljoin

try:
    import httpx
except Exception:
    httpx = None
try:
    import urllib3
except Exception:
    urllib3 = None

try:
    from google.api_core.exceptions import (
        ServiceUnavailable,
        ResourceExhausted,
        DeadlineExceeded,
        InternalServerError,
    )
except Exception:
    ServiceUnavailable = ResourceExhausted = DeadlineExceeded = InternalServerError = (
        Exception
    )


# ========= Gemini ãƒªãƒˆãƒ©ã‚¤èª¿æ•´ç”¨ã®å®šæ•° =========
GEMINI_MAX_RETRIES = 7          # æ—¢å®š 5 â†’ 7
GEMINI_BASE_DELAY = 10.0        # æ—¢å®š 2.0 â†’ 10.0
GEMINI_MAX_DELAY = 120.0        # æ—¢å®š 30.0 â†’ 120.0

# ç¿»è¨³ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆç¬é–“è² è·ã‚’ä¸‹ã’ã‚‹ï¼‰
TRANSLATION_BATCH_SIZE = 2      # æ—¢å®š 3 â†’ 2

# ä¹±æ•°ã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
def _exp_backoff_sleep(attempt: int, base_delay: float, max_delay: float) -> float:
    """
    attempt: 0 origin (0,1,2,...)
    return: sleep seconds (æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• + 0ã€œ1ç§’ã®ã‚¸ãƒƒã‚¿ãƒ¼), max_delayã§ã‚¯ãƒªãƒƒãƒ—
    """
    import math, random
    # 2^attempt * base_delay ã‚’ä¸Šé™ max_delay ã§ã‚¯ãƒªãƒƒãƒ—
    delay = min(max_delay, (2 ** attempt) * base_delay)
    # 0ã€œ1ç§’ã®å°ã•ãªã‚¸ãƒƒã‚¿ãƒ¼ã‚’åŠ ãˆã‚‹ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯å›é¿ï¼‰
    return min(max_delay, delay + random.random())


# Geminiæœ¬ç•ªç”¨
client_summary = genai.Client(api_key=os.getenv("GEMINI_API_SUMMARY_KEY"))
client_dedupe = genai.Client(api_key=os.getenv("GEMINI_API_DEDUPE_KEY"))


def _is_retriable_exc(e: Exception) -> bool:
    msg = (str(e) or "").lower()
    name = e.__class__.__name__.lower()

    # Googleç³»ã®æ˜ç¤ºçš„ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡
    if isinstance(
        e,
        (ServiceUnavailable, ResourceExhausted, DeadlineExceeded, InternalServerError),
    ):
        return True

    # httpx/urllib3ç³»ï¼ˆç’°å¢ƒã«ç„¡ã‘ã‚Œã°ç„¡è¦–ï¼‰
    if httpx and isinstance(
        e,
        (
            getattr(httpx, "RemoteProtocolError", Exception),
            getattr(httpx, "ReadTimeout", Exception),
            getattr(httpx, "ConnectError", Exception),
        ),
    ):
        return True
    if urllib3 and isinstance(
        e,
        (
            urllib3.exceptions.ProtocolError,
            urllib3.exceptions.ReadTimeoutError,
            urllib3.exceptions.MaxRetryError,
        ),
    ):
        return True

    # æ–‡å­—åˆ—ã§ã®åˆ¤å®šï¼ˆå®Ÿè£…å·®åˆ†å¸åï¼‰
    hints = [
        "remoteprotocolerror",
        "servererror",
        "internal",  # "500 internal", "internal error" ãªã©
        "server disconnected",
        "unavailable",
        "500",
        "503",
        "502",
        "504",
        "gateway",
        "timeout",
        "temporar",
        "overload",
    ]
    if any(h in msg or h in name for h in hints):
        return True
    return False


# === Gemini ä½¿ç”¨é‡ãƒ­ã‚°ï¼ˆå…¥å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ ======================================
def _usage_from_resp(resp):
    """
    google-genai ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ usage ã‚’å–ã‚Šå‡ºã™ï¼ˆsnake/camelåŒæ–¹ã«è€æ€§ï¼‰ã€‚
    æˆ»ã‚Šå€¤: dict(prompt_token_count, candidates_token_count, total_token_count,
                cache_creation_input_token_count, cache_read_input_token_count)
    """
    usage = (
        getattr(resp, "usage_metadata", None)
        or getattr(resp, "usageMetadata", None)
        or {}
    )
    ud = {}
    if usage:
        get = usage.get if isinstance(usage, dict) else lambda k, d=None: getattr(usage, k, d)
        ud["prompt_token_count"] = get(
            "prompt_token_count", get("input_token_count", get("input_tokens", 0))
        )
        ud["candidates_token_count"] = get(
            "candidates_token_count",
            get("output_token_count", get("output_tokens", 0)),
        )
        ud["total_token_count"] = get(
            "total_token_count",
            get(
                "total_tokens",
                (ud.get("prompt_token_count", 0) or 0)
                + (ud.get("candidates_token_count", 0) or 0),
            ),
        )
        ud["cache_creation_input_token_count"] = get(
            "cache_creation_input_token_count", 0
        )
        ud["cache_read_input_token_count"] = get("cache_read_input_token_count", 0)
    return ud


def _log_gemini_usage(resp, *, tag: str = "gen", model: str = ""):
    """æ¨™æº–å‡ºåŠ›ï¼‹JSONLãƒ•ã‚¡ã‚¤ãƒ«(gemini_usage.log)ã¸å…¥å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨˜éŒ²"""
    try:
        u = _usage_from_resp(resp) or {}
        rec = {
            "ts": datetime.utcnow().isoformat(timespec="seconds") + "Z",
            "tag": tag,
            "model": model,
            **u,
        }
        print(
            "ğŸ“Š TOKENS[{tag}] in={in_} out={out} total={tot} (cache create/read={cc}/{cr})".format(
                tag=tag,
                in_=rec.get("prompt_token_count", 0),
                out=rec.get("candidates_token_count", 0),
                tot=rec.get("total_token_count", 0),
                cc=rec.get("cache_creation_input_token_count", 0),
                cr=rec.get("cache_read_input_token_count", 0),
            )
        )
        try:
            with open("gemini_usage.log", "a", encoding="utf-8") as f:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        except Exception:
            pass
    except Exception as e:
        print(f"âš ï¸ usage log failed: {e}")


# === Free tier monitor (10 Requests per Minute / 250 Requests per Day / 250k Tokens per Minute[input]) ======
# è¿½åŠ ãƒ­ã‚°: å‡ºåŠ›å´ã® Tokens per Minute (output) ã‚‚é›†è¨ˆã—ã¦è¡¨ç¤ºã™ã‚‹
class _FreeTierWatch:
    def __init__(self, rpm_limit=10, rpd_limit=250, tpm_limit=250_000):
        self.rpm_limit = int(os.getenv("GEMINI_FREE_RPM", rpm_limit))
        # â€œRPDâ€ ã®ç•¥ç§°ã¯ä½¿ã‚ãšã€æ­£å¼åç§°ã§æ‰±ã†
        self.requests_per_day_limit = int(os.getenv("GEMINI_FREE_RPD", rpd_limit))
        # ç„¡æ–™æ ã®TPMåˆ¤å®šã¯å…¥åŠ›ãŒåŸºæº–
        self.tpm_limit = int(os.getenv("GEMINI_FREE_TPM", tpm_limit))

        self.req_times = deque()  # ç›´è¿‘60ç§’ã®æˆåŠŸãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†æ™‚åˆ»
        self.tpm_in_points = deque()  # ç›´è¿‘60ç§’ã® (æ™‚åˆ», å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³)
        self.tpm_out_points = deque()  # ç›´è¿‘60ç§’ã® (æ™‚åˆ», å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³)
        self.day_key = None  # MMT æ—¥ä»˜ã‚­ãƒ¼ï¼ˆUTC+6:30ï¼‰
        self.requests_per_day_count = 0

        # â€œè¶ŠãˆãŸç¬é–“ã ã‘â€é€šçŸ¥ã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ
        self._over_rpm = False
        self._over_tpm_in = False
        self._over_rpd = False

        # æ¯å›ã®ãƒ¬ãƒ¼ãƒˆçª“ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå‡ºåŠ›ï¼ˆæ¨™æº–å‡ºåŠ›ã®ã¿ï¼æ—¢å®šONï¼‰
        self._rate_window_log_enabled = str(
            os.getenv("GEMINI_RATE_WINDOW_LOG", "1")
        ).lower() not in ("0", "false", "off")

    def _mmt_today(self, now_utc):
        mmt = timezone(timedelta(hours=6, minutes=30))
        return now_utc.astimezone(mmt).date()

    def record(
        self,
        prompt_tokens: int,
        output_tokens: int = 0,
        *,
        tag: str = "gen",
        model: str = "",
    ):
        now = datetime.utcnow().replace(tzinfo=timezone.utc)

        # ç›´è¿‘60ç§’çª“ï¼ˆRequests per Minute / Tokens per Minuteï¼‰
        self.req_times.append(now)
        self.tpm_in_points.append((now, int(prompt_tokens or 0)))
        self.tpm_out_points.append((now, int(output_tokens or 0)))
        cutoff = now - timedelta(seconds=60)
        while self.req_times and self.req_times[0] < cutoff:
            self.req_times.popleft()
        while self.tpm_in_points and self.tpm_in_points[0][0] < cutoff:
            self.tpm_in_points.popleft()
        while self.tpm_out_points and self.tpm_out_points[0][0] < cutoff:
            self.tpm_out_points.popleft()

        rpm = len(self.req_times)
        tpm_in = sum(tok for _, tok in self.tpm_in_points)
        tpm_out = sum(tok for _, tok in self.tpm_out_points)

        # Requests per Day â€” MMTæ—¥ä»˜ã§ã‚«ã‚¦ãƒ³ãƒˆ
        today_mmt = self._mmt_today(now)
        if self.day_key != today_mmt:
            self.day_key = today_mmt
            self.requests_per_day_count = 0
            self._over_rpd = False  # æ—¥ã¾ãŸãã§ãƒªã‚»ãƒƒãƒˆ
        self.requests_per_day_count += 1  # ã“ã®æˆåŠŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨ˆä¸Š

        # è¶…éåˆ¤å®šï¼ˆå…¥åŠ›TPM/RPM/Requests per Dayï¼‰
        over_rpm = rpm > self.rpm_limit
        over_tpm_in = tpm_in > self.tpm_limit
        over_rpd = self.requests_per_day_count > self.requests_per_day_limit

        def _emit_exceeded(kind_label: str, detail: str):
            # kind_label ã¯æ­£å¼åç§°ã§ï¼š "Requests per Minute" / "Tokens per Minute (input)" / "Requests per Day"
            print(
                f"ğŸš© FREE-TIER EXCEEDED [{kind_label}] {detail} | tag={tag} model={model}"
            )

        # è¶…éé€šçŸ¥ï¼ˆæ­£å¼åç§°ï¼‰
        if over_rpm and not self._over_rpm:
            self._over_rpm = True
            _emit_exceeded(
                "Requests per Minute", f"{rpm}>{self.rpm_limit} within last 60s"
            )
        elif not over_rpm:
            self._over_rpm = False

        if over_tpm_in and not self._over_tpm_in:
            self._over_tpm_in = True
            _emit_exceeded(
                "Tokens per Minute (input)",
                f"input={tpm_in} > {self.tpm_limit} in last 60s",
            )
        elif not over_tpm_in:
            self._over_tpm_in = False

        if over_rpd and not self._over_rpd:
            self._over_rpd = True
            _emit_exceeded(
                "Requests per Day",
                f"{self.requests_per_day_count}>{self.requests_per_day_limit} (MMT day {today_mmt})",
            )

        # æ¯å›ã®ãƒ¬ãƒ¼ãƒˆçª“ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆäººé–“å¯èª­ã€JSONå‡ºåŠ›ãªã—ï¼‰
        if self._rate_window_log_enabled:
            print(
                "â„¹ï¸ WINDOW [rate] "
                f"Requests per Minute={rpm} | "
                f"Tokens per Minute (input)={tpm_in} | "
                f"Tokens per Minute (output)={tpm_out} | "
                f"Requests per Day={self.requests_per_day_count} "
                f"(MMT day {today_mmt}) | tag={tag} model={model}"
            )


# æœ‰åŠ¹/ç„¡åŠ¹ãƒˆã‚°ãƒ«ï¼ˆæ—¢å®š=æœ‰åŠ¹ï¼‰
_FREE_TIER_CHECK_ENABLED = str(os.getenv("GEMINI_FREE_TIER_CHECK", "1")).lower() not in (
    "0",
    "false",
    "off",
)
_FREE_TIER_MON = _FreeTierWatch() if _FREE_TIER_CHECK_ENABLED else None


def call_gemini_with_retries(
    client,
    prompt: str,
    model: str = "gemini-2.5-flash",
    max_retries: int = GEMINI_MAX_RETRIES,
    base_delay: float = GEMINI_BASE_DELAY,
    max_delay: float = GEMINI_MAX_DELAY,
    usage_tag: str = "generic",
):
    """
    Gemini å‘¼ã³å‡ºã—ã®å…±é€šãƒªãƒˆãƒ©ã‚¤ãƒ©ãƒƒãƒ‘ãƒ¼ã€‚
    - 503/UNAVAILABLE/ä¸€æ™‚çš„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¯æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•+ã‚¸ãƒƒã‚¿ãƒ¼ã§å†è©¦è¡Œ
    - 429/ãƒ¬ãƒ¼ãƒˆç³»ã¯å¾…æ©Ÿã—ã¦å†è©¦è¡Œï¼ˆGemini Freeã®ç¬é–“ä¸Šé™ã«å½“ãŸã‚‹ã“ã¨ãŒå¤šã„ï¼‰
    - ãã‚Œä»¥å¤–ã®æ’ä¹…çš„ã‚¨ãƒ©ãƒ¼ã¯å³æ™‚raise
    """
    last_exc = None
    for attempt in range(max_retries):
        try:
            # å®Ÿéš›ã®å‘¼ã³å‡ºã—ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®å‘¼ã³æ–¹ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            resp = client.models.generate_content(model=model, contents=prompt)
            # ä½¿ç”¨é‡ãƒ­ã‚°
            try:
                _log_gemini_usage(resp, tag=(usage_tag or "gen"), model=model)
            except Exception:
                pass
            # Free tier ç›£è¦–ï¼ˆMMTæ—¥æ¬¡ / RPM / å…¥åŠ›TPMï¼‰
            try:
                if _FREE_TIER_MON:
                    u = _usage_from_resp(resp) or {}
                    _FREE_TIER_MON.record(
                        int(u.get("prompt_token_count") or 0),
                        output_tokens=int(u.get("candidates_token_count") or 0),
                        tag=(usage_tag or "gen"),
                        model=model,
                    )
            except Exception:
                pass
            return resp
        except Exception as e:
            msg = str(e)
            last_exc = e

            # ä¾‹å¤–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç°¡æ˜“åˆ¤å®šï¼ˆSDKå·®ç•°ã‚’å¸åã™ã‚‹ãŸã‚æ–‡å­—åˆ—ãƒ™ãƒ¼ã‚¹ï¼‰
            is_503 = "503" in msg or "UNAVAILABLE" in msg or "overloaded" in msg
            is_429 = "429" in msg or "RESOURCE_EXHAUSTED" in msg or "rate" in msg.lower()

            # å†è©¦è¡Œå¯¾è±¡
            if is_503 or is_429 or "timeout" in msg.lower() or "temporarily" in msg.lower():
                # ãƒ­ã‚°ï¼ˆæ—¢å­˜ã®printä½“è£ã«åˆã‚ã›ã¦ã„ã¾ã™ï¼‰
                print(f"âš ï¸ Gemini retry {attempt+1}/{max_retries} after: {e}")
                sleep_sec = _exp_backoff_sleep(attempt, base_delay, max_delay)
                # 429ã®å ´åˆã¯å°‘ã—å¤šã‚ã«å¾…ã¤ï¼ˆFree tierã®ç¬é–“ä¸Šé™å›é¿ï¼‰
                if is_429:
                    sleep_sec = min(GEMINI_MAX_DELAY, sleep_sec + 5.0)
                try:
                    import time
                    time.sleep(sleep_sec)
                except KeyboardInterrupt:
                    raise
                continue

            # éãƒªãƒˆãƒ©ã‚¤ç³»ã¯å³raise
            raise

    # ã™ã¹ã¦å¤±æ•—
    raise last_exc if last_exc else RuntimeError("Gemini call failed with unknown error.")


# è¦ç´„ç”¨ã«é€ã‚‹æœ¬æ–‡ã®æœ€å¤§æ–‡å­—æ•°ï¼ˆå›ºå®šï¼‰
# Irrawaddyè‹±èªè¨˜äº‹ãŒ3500æ–‡å­—ãã‚‰ã„ã‚ã‚‹
BODY_MAX_CHARS = 3500

# ãƒŸãƒ£ãƒ³ãƒãƒ¼æ¨™æº–æ™‚ (UTC+6:30)
MMT = timezone(timedelta(hours=6, minutes=30))


# ä»Šæ—¥ã®æ—¥ä»˜
# ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é€Ÿå ±æ€§é‡è¦–ã§ä»Šæ—¥åˆ†ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã®æ–¹é‡
def get_today_date_mmt():
    # æœ¬ç•ªç”¨ã€ä»Šæ—¥ã®æ—¥ä»˜
    now_mmt = datetime.now(MMT)
    return (now_mmt - timedelta(days=1)).date()


# å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆå…¨ãƒ¡ãƒ‡ã‚£ã‚¢å…±é€šã§ä½¿ç”¨ã™ã‚‹ï¼‰
NEWS_KEYWORDS = [
    # ãƒŸãƒ£ãƒ³ãƒãƒ¼ï¼ˆå›½åãƒ»ç¾è¡Œåç§°ï¼‰
    "á€™á€¼á€”á€ºá€™á€¬",
    "á€™á€¼á€”á€ºá€™á€¬á€·",
    "Myanmar",
    "myanmar",
    # ãƒ“ãƒ«ãƒï¼ˆæ—§å›½åãƒ»é€šç§°ï¼‰
    "á€—á€™á€¬",
    "Burma",
    "burma",
    # ã‚¢ã‚¦ãƒ³ã‚µãƒ³ã‚¹ãƒ¼ãƒãƒ¼ï¼ˆAung San Suu Kyiï¼‰
    "á€¡á€±á€¬á€„á€ºá€†á€”á€ºá€¸á€…á€¯á€€á€¼á€Šá€º",
    "Aung San Suu Kyi",
    "aung san suu kyi",
    # ãƒŸãƒ³ã‚¢ã‚¦ãƒ³ãƒ•ãƒ©ã‚¤ãƒ³ï¼ˆMin Aung Hlaingï¼‰
    "á€™á€„á€ºá€¸á€¡á€±á€¬á€„á€ºá€œá€¾á€­á€¯á€„á€º",
    "Min Aung Hlaing",
    "min aung hlaing",
    # ãƒãƒ£ãƒƒãƒˆï¼ˆKyatï¼‰
    "Kyat",
    "kyat",
    # å¾´å…µåˆ¶ï¼ˆConscription / Military Draftï¼‰, å¾´å…µ, å…µå½¹
    "á€…á€…á€ºá€á€•á€ºá€á€„á€ºá€á€±á€«á€ºá€›á€±á€¸",
    "á€…á€…á€ºá€™á€¾á€¯á€‘á€™á€ºá€¸",
    "á€¡á€á€„á€ºá€¸á€á€•á€ºá€á€„á€ºá€á€±á€«á€ºá€á€¼á€„á€ºá€¸",
    "á€á€•á€ºá€á€„á€ºá€á€±á€«á€º ",
    "Conscription",
    "conscription",
    "Military Draft",
    "Military draft",
    "military draft",
    "Military Service",
    "Military service",
    "military service",
    # ãƒ­ãƒ’ãƒ³ã‚®ãƒ£ ãŠé¡˜ã„ã•ã‚Œã¦ãªã„
    # "á€›á€­á€¯á€Ÿá€„á€ºá€‚á€»á€¬",
    # "Rohingya",
    # "rohingya",
    # å›½å¢ƒè²¿æ˜“ãƒ»äº¤æ˜“
    "á€”á€šá€ºá€…á€•á€ºá€€á€¯á€”á€ºá€á€½á€šá€ºá€›á€±á€¸",
    # ãƒ¤ãƒ³ã‚´ãƒ³ç®¡åŒº
    # "á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸",
    # ãƒ¤ãƒ³ã‚´ãƒ³
    "á€›á€”á€ºá€€á€¯á€”á€º",
    "Yangon Region",
    "Yangon region",
    "yangon region",
]

# Unicodeæ­£è¦åŒ–ï¼ˆNFCï¼‰ã‚’é©ç”¨
NEWS_KEYWORDS = [unicodedata.normalize("NFC", kw) for kw in NEWS_KEYWORDS]

# --- Ayeyarwady (ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£) ç³»ã ã‘ã‚’æŠœãå‡ºã™ã‚µãƒ–ã‚»ãƒƒãƒˆ ---
AYEYARWADY_KEYWORDS = [
    "á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸",
    "Ayeyarwady Region",
    "Ayeyarwady region",
    "ayeyarwady region",
]
AYEYARWADY_KEYWORDS = [unicodedata.normalize("NFC", kw) for kw in AYEYARWADY_KEYWORDS]

def is_ayeyarwady_hit(title: str, body: str) -> bool:
    """ã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡ã«ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹"""
    return any(kw in title or kw in body for kw in AYEYARWADY_KEYWORDS)

# ã€Œãƒãƒ£ãƒƒãƒˆã€èªã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šè²¨èªã¨ã—ã¦æ‹¾ã†
CURRENCY_WORD = r"(?:á€™á€¼á€”á€ºá€™á€¬(?:á€·)?(?:á€”á€­á€¯á€„á€ºá€„á€¶)?\s*)?(?:á€„á€½á€±\s*)?á€€á€»á€•á€º(?:á€„á€½á€±)?"

_DIGITS = r"[0-9á€-á‰][0-9á€-á‰,\.]*"
_SCALE = r"(?:á€á€±á€¬á€„á€ºá€¸|á€á€­á€”á€ºá€¸|á€á€”á€ºá€¸)"
_TRAIL = r"(?:\s*(?:á€€á€»á€±á€¬á€º|á€œá€±á€¬á€€á€º|á€á€”á€·á€º))?"

# 1) æ•°å­—â†’é€šè²¨ï¼ˆnum/scale ã¯ã“ã“ã§1å›ã ã‘å®šç¾©ï¼‰
_KYAT_NUM_FIRST = re.compile(
    rf"""
    (?P<num>{_DIGITS})\s*(?P<scale>{_SCALE})?\s*(?:{CURRENCY_WORD})
    {_TRAIL}
    """,
    re.VERBOSE,
)

# 2) é€šè²¨â†’æ•°å­—ï¼ˆåŒã˜ã‚°ãƒ«ãƒ¼ãƒ—åã‚’ã“ã“ã§ã‚‚1å›ã ã‘å®šç¾©ï¼‰
_KYAT_CCY_FIRST = re.compile(
    rf"""
    (?:{CURRENCY_WORD})\s*(?P<scale>{_SCALE})?\s*(?P<num>{_DIGITS})
    {_TRAIL}
    """,
    re.VERBOSE,
)


class _OrPattern:
    """è¤‡æ•°ã® compiled regex ã‚’ã¾ã¨ã‚ã€.search ã§æœ€åˆã«å½“ãŸã£ãŸ Match ã‚’è¿”ã™è–„ã„ãƒ©ãƒƒãƒ‘"""

    def __init__(self, *compiled):
        self._compiled = compiled
        self.pattern = " | ".join(p.pattern for p in compiled)  # å‚è€ƒç”¨
        self.flags = compiled[0].flags if compiled else 0

    def search(self, string, pos=0):
        for p in self._compiled:
            m = p.search(string, pos)
            if m:
                return m
        return None


KYAT_PATTERN = _OrPattern(_KYAT_NUM_FIRST, _KYAT_CCY_FIRST)


def any_keyword_hit(title: str, body: str) -> bool:
    # é€šå¸¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´
    if any(kw in title or kw in body for kw in NEWS_KEYWORDS):
        return True
    # é€šè²¨ã€Œá€€á€»á€•á€ºã€ã ã‘ã¯æ­£è¦è¡¨ç¾ã§åˆ¤å®š
    if KYAT_PATTERN.search(title) or KYAT_PATTERN.search(body):
        return True
    return False



def clean_html_content(html: str) -> str:
    html = html.replace("\xa0", " ").replace("&nbsp;", " ")
    # åˆ¶å¾¡æ–‡å­—ï¼ˆã‚«ãƒ†ã‚´ãƒªCï¼‰ã‚’é™¤å¤–ã€å¯è¦–Unicodeã¯ãã®ã¾ã¾
    return "".join(c for c in html if unicodedata.category(c)[0] != "C")


# æœ¬æ–‡ãŒå–å¾—ã§ãã‚‹ã¾ã§ã€Œrequestsã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€
def fetch_with_retry(url, retries=3, wait_seconds=2):
    for attempt in range(retries):
        try:
            res = requests.get(url, timeout=10)
            if res.status_code == 200 and res.text.strip():
                return res
        except Exception as e:
            print(f"Attempt {attempt + 1} failed for {url}: {e}")
        time.sleep(wait_seconds)
    raise Exception(f"Failed to fetch {url} after {retries} attempts.")


# æœ¬æ–‡ãŒç©ºãªã‚‰ã€Œä¸€å®šç§’æ•°å¾…ã£ã¦å†å–å¾—ã€
def extract_paragraphs_with_wait(soup_article, retries=2, wait_seconds=2):
    for attempt in range(retries + 1):
        paragraphs = soup_article.select("div.entry-content p")
        if not paragraphs:
            paragraphs = soup_article.select("div.node-content p")
        if not paragraphs:
            paragraphs = soup_article.select("article p")
        if not paragraphs:
            paragraphs = soup_article.find_all("p")

        if paragraphs:
            return paragraphs

        print(f"Paragraphs not found, waiting {wait_seconds}s and retrying...")
        time.sleep(wait_seconds)
    return []


# === æ±ç”¨ã® <p> æŠ½å‡ºå™¨ï¼ˆã‚µã‚¤ãƒˆå…±é€šï¼‰ ===
def extract_body_generic_from_soup(soup):
    for sel in ["div.entry-content p", "div.node-content p", "article p"]:
        ps = soup.select(sel)
        if ps:
            break
    else:
        ps = soup.find_all("p")
    txts = [p.get_text(strip=True) for p in ps if p.get_text(strip=True)]
    return "\n".join(txts).strip()


# === requests ã‚’ä½¿ã†ã‚·ãƒ³ãƒ—ãƒ«ãª fetch_onceï¼ˆ1å›ï¼‰ ===
def fetch_once_requests(url, timeout=15):
    r = requests.get(url, timeout=timeout)
    r.raise_for_status()
    # æ–‡å­—åŒ–ã‘å›é¿ã®ãŸã‚ bytes ã‚’è¿”ã™ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ã¯ BeautifulSoup ã«ä»»ã›ã‚‹ï¼‰
    return r.content


# === å†ãƒ•ã‚§ãƒƒãƒä»˜ããƒ»æœ¬æ–‡å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===
def get_body_with_refetch(
    url, fetcher, extractor, retries=3, wait_seconds=2, quiet=False
):
    """
    fetcher(url) -> html(bytes or str)
    extractor(soup) -> body(str)
    """
    last_err = None
    for attempt in range(retries + 1):
        try:
            html = fetcher(url)
            # bytes/str ã©ã¡ã‚‰ã§ã‚‚ BeautifulSoup ã«æ¸¡ã›ã‚‹
            soup = BeautifulSoup(html, "html.parser")

            # èª¤ã£ã¦ latin-1 ç³»ã§è§£é‡ˆã•ã‚ŒãŸå ´åˆã¯ UTF-8 ã§å†è§£é‡ˆã—ã¦ä¿é™ºã‚’ã‹ã‘ã‚‹
            enc = (getattr(soup, "original_encoding", None) or "").lower()
            if enc in ("iso-8859-1", "latin-1", "windows-1252"):
                soup = BeautifulSoup(html, "html.parser", from_encoding="utf-8")

            body = extractor(soup)
            if body:
                return unicodedata.normalize("NFC", body)

            if not quiet:
                print(f"[refetch] body empty, retrying {attempt+1}/{retries} â†’ {url}")
        except Exception as e:
            last_err = e
            if not quiet:
                print(f"[refetch] EXC {attempt+1}/{retries}: {e} â†’ {url}")
        time.sleep(wait_seconds)

    if not quiet and last_err:
        print(f"[refetch] give up after {retries+1} tries â†’ {url}")
    return ""


# === Irrawaddyå°‚ç”¨ ===
# æœ¬æ–‡ãŒå–å¾—ã§ãã‚‹ã¾ã§ã€Œrequestsã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€
def fetch_with_retry_irrawaddy(url, retries=3, wait_seconds=2, session=None):
    """
    Irrawaddy å°‚ç”¨ãƒ•ã‚§ãƒƒãƒãƒ£ï¼ˆå˜ç™ºãƒˆãƒ©ã‚¤ç‰ˆï¼‰ã€‚
    - curl_cffi ã§1å› â†’ å¤±æ•—ãªã‚‰ cloudscraper ã§1å› â†’ å¤±æ•—ãªã‚‰ requests ã§1å›ã€‚
    - è¿½åŠ ã®ãƒªãƒˆãƒ©ã‚¤ã‚„ /amp ã¸ã®å†è©¦è¡Œã¯å®Ÿæ–½ã—ãªã„ã€‚
    """
    import os
    import random
    import time
    import urllib.parse

    UA = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/128.0.0.0 Safari/537.36"
    )
    HEADERS = {
        "User-Agent": UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-User": "?1",
        "Sec-Fetch-Dest": "document",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": "https://www.irrawaddy.com/",
        "Connection": "keep-alive",
    }

    def _amp_url(u: str) -> str:
        # https://.../path/ ãªã‚‰ https://.../path/amp
        # https://.../path  ãªã‚‰ https://.../path/amp
        if not u.endswith("/"):
            u = u + "/"
        return urllib.parse.urljoin(u, "amp")

    # --- Try 1: curl_cffi (Chrome æŒ‡ç´‹) å˜ç™º ---
    try:
        from curl_cffi import requests as cfr  # type: ignore[import-not-found]
        proxies = {
            "http": os.getenv("HTTP_PROXY") or os.getenv("http_proxy"),
            "https": os.getenv("HTTPS_PROXY") or os.getenv("https_proxy"),
        }
        r = cfr.get(
            url,
            headers=HEADERS,
            impersonate="chrome124",
            timeout=30,
            allow_redirects=True,
            proxies={k: v for k, v in proxies.items() if v},
        )
        if r.status_code == 200 and (r.text or "").strip():
            return r
    except Exception as e:
        print(f"[fetch-cffi] EXC: {e} â†’ {url}")

    # --- Try 2: cloudscraper å˜ç™º ---
    try:
        import cloudscraper
        import requests as rq
        sess = session or rq.Session()
        scraper = cloudscraper.create_scraper(
            sess=sess,
            browser={"browser": "chrome", "platform": "windows", "mobile": False},
            delay=7,
        )
        r = scraper.get(url, headers=HEADERS, timeout=30, allow_redirects=True)
        if r.status_code == 200 and getattr(r, "text", "").strip():
            return r
    except Exception as e:
        print(f"[fetch-cs] EXC: {e} â†’ {url}")

    # --- Try 3: requests å˜ç™ºï¼ˆ/news/ ã®ã¨ãã®ã¿ /amp ã‚’1å›ã ã‘è©¦ã™ï¼‰ ---
    try:
        import requests
        sess = session or requests.Session()
        r2 = sess.get(url, headers=HEADERS, timeout=20, allow_redirects=True)
        print(
            f"[fetch-rq] final: HTTP {r2.status_code} len={len(getattr(r2,'text',''))} â†’ {url}"
        )
        if r2.status_code == 200 and getattr(r2, "text", "").strip():
            return r2
        # 403/503 ã‹ã¤ /news/ ã®è¨˜äº‹URLã«é™ã‚Šã€/amp ã‚’â€œ1å›ã ã‘â€è©¦ã™
        if r2.status_code in (403, 503) and "/news/" in url and "/category/" not in url:
            amp = _amp_url(url)
            r3 = sess.get(amp, headers=HEADERS, timeout=20, allow_redirects=True)
            print(
                f"[fetch-rq] amp: HTTP {r3.status_code} len={len(getattr(r3,'text',''))} â†’ {amp}"
            )
            if r3.status_code == 200 and getattr(r3, "text", "").strip():
                return r3
        try:
            svr = r2.headers.get("server") or r2.headers.get("Server")
            ray = r2.headers.get("cf-ray")
            sucuri = r2.headers.get("x-sucuri-id") or r2.headers.get("x-sucuri-block")
            print(f"[fetch-rq] headers: server={svr} cf-ray={ray} sucuri={sucuri}")
        except Exception:
            pass
    except Exception as e:
        print(f"[fetch-rq] EXC final: {e} â†’ {url}")

    raise Exception(f"Failed to fetch {url} after {retries} attempts.")


# ==== Playwright çµŒç”±ãƒ•ã‚§ãƒƒãƒï¼ˆIrrawaddyç”¨ã®æœ€å°å®Ÿè£…ï¼‰ ====
def _bool_env(name: str, default=False) -> bool:
    v = (os.getenv(name) or "").strip().lower()
    if not v:
        return bool(default)
    return v not in ("0", "false", "off", "no")

IRRAWADDY_USE_PLAYWRIGHT = _bool_env("IRRAWADDY_USE_PLAYWRIGHT", False)

def fetch_once_irrawaddy_playwright(url: str, timeout_ms: int = 45000) -> bytes:
    """
    Headless Chromium + JS å®Ÿè¡Œã§ HTML ã‚’å–å¾—ã€‚
    - ã‚«ãƒ†ã‚´ãƒª/è¨˜äº‹ã©ã¡ã‚‰ã‚‚å¯
    - 403/å¾…æ©Ÿä¸è¶³ã«å‚™ãˆã€ä¸»è¦ã‚»ãƒ¬ã‚¯ã‚¿ã‚’çŸ­æ™‚é–“å¾…æ©Ÿ
    - å¤±æ•—æ™‚ã¯ä¾‹å¤–ã‚’æŠ•ã’ã‚‹ï¼ˆä¸Šä½ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    ä¾å­˜: pip install playwright && python -m playwright install --with-deps chromium
    """
    from playwright.sync_api import sync_playwright

    UA = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/128.0.0.0 Safari/537.36"
    )
    EXTRA_HEADERS = {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9,my;q=0.8,ja;q=0.7",
        "Upgrade-Insecure-Requests": "1",
        "Referer": "https://www.irrawaddy.com/",
    }
    ARTICLE_SELECTORS = [
        "div.content-inner",
        "article .content-inner",
        ".jeg_post_content",
        ".entry-content",
    ]
    LISTING_SELECTORS = [
        ".jnews_category_hero_container",
        "div.jeg_postblock_content",
        ".jeg_post_meta .jeg_meta_date a",
        "h2.jeg_post_title a",
        "h3.jeg_post_title a",
        ".jeg_content",
    ]

    with sync_playwright() as p:
        # Playwright: ã‚ˆã‚Šäººé–“ã‚‰ã—ã„ç’°å¢ƒã«è¿‘ã¥ã‘ã‚‹
        browser = p.chromium.launch(headless=True, args=["--no-sandbox"])
        # Storage state ã®æ°¸ç¶šåŒ–ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿ã€æœ€å¾Œã«ä¿å­˜ï¼‰
        _PW_STATE_PATH = os.getenv("IRRAWADDY_PW_STATE_PATH", "pw_state.json")
        _has_state = False
        try:
            _has_state = os.path.exists(_PW_STATE_PATH)
        except Exception:
            _has_state = False
        context = browser.new_context(
            user_agent=UA,
            java_script_enabled=True,
            locale="en-US",
            timezone_id="Asia/Yangon",
            viewport={"width": 1366, "height": 768},
            storage_state=(_PW_STATE_PATH if _has_state else None),
        )
        context.set_extra_http_headers(EXTRA_HEADERS)
        page = context.new_page()
        try:
            # webdriverãƒ•ãƒ©ã‚°ã®ç·©å’Œ
            page.add_init_script(
                """
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                """
            )
        except Exception:
            pass

        status = 0
        ready = False
        try:
            resp = page.goto(url, wait_until="networkidle", timeout=timeout_ms)
            status = (resp.status if resp else 0) or 0
        except Exception:
            # networkidle ãŒæˆç«‹ã—ãªã„å ´åˆã¯ domcontentloaded ã§å†è©¦è¡Œ
            try:
                resp = page.goto(url, wait_until="domcontentloaded", timeout=timeout_ms)
                status = (resp.status if resp else 0) or 0
            except Exception:
                status = 0

        for sel in ARTICLE_SELECTORS + LISTING_SELECTORS:
            try:
                page.wait_for_selector(sel, timeout=6000)
                ready = True
                break
            except Exception:
                continue

        if (status in (403, 503)) or not ready:
            try:
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(2500)
                for sel in ARTICLE_SELECTORS + LISTING_SELECTORS:
                    try:
                        page.wait_for_selector(sel, timeout=6000)
                        ready = True
                        break
                    except Exception:
                        pass
            except Exception:
                pass

        html = page.content()
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³/Cookie ã‚’ä¿å­˜ï¼ˆæ¬¡å›ä»¥é™ã«æ´»ç”¨ï¼‰
        try:
            context.storage_state(path=_PW_STATE_PATH)
        except Exception:
            pass
        browser.close()

    if not html or not html.strip():
        raise RuntimeError(f"Playwright empty content: {url}")
    return html.encode("utf-8", errors="ignore")


def fetch_once_irrawaddy_auto(url: str, session=None) -> bytes:
    """
    1) IRRAWADDY_USE_PLAYWRIGHT=1 ãªã‚‰ Playwright ã‚’å„ªå…ˆ
    2) å¤±æ•—æ™‚ã¯æ—¢å­˜ã® curl_cffi / cloudscraper / requests ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    if IRRAWADDY_USE_PLAYWRIGHT:
        try:
            return fetch_once_irrawaddy_playwright(url)
        except Exception as e:
            print(f"[pw] fallback due to: {e} â†’ {url}")
    return fetch_once_irrawaddy(url, session=session)

# ==== Irrawaddy å®‰å®šåŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====
SEEN_CACHE_PATH_IRRAWADDY = os.getenv("IRRAWADDY_SEEN_CACHE", "irrawaddy_seen.json")  # æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥

def _sleep_jitter(min_s: float = 8.0, max_s: float = 12.0):
    """å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã« 8â€“12 ç§’ã®ã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãã‚¹ãƒªãƒ¼ãƒ—ã‚’å…¥ã‚Œã‚‹"""
    try:
        time.sleep(random.uniform(min_s, max_s))
    except KeyboardInterrupt:
        raise

def _exp_backoff(attempt: int, base: float = 8.0, cap: float = 90.0) -> float:
    """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• + å¾®å°ã‚¸ãƒƒã‚¿ãƒ¼"""
    return min(cap, (2 ** attempt) * base + random.random())

def _norm_id(u: str) -> str:
    try:
        from urllib.parse import urlparse
        p = urlparse(u)
        return (p.scheme + "://" + p.netloc + p.path).rstrip("/")
    except Exception:
        return (u or "").strip().rstrip("/")

def _load_seen_set(path: str = SEEN_CACHE_PATH_IRRAWADDY) -> set:
    """å‰å›ã¾ã§ã«å–å¾—æ¸ˆã¿ã® URL ã‚’æ°¸ç¶šä¿ç®¡ï¼ˆå®Ÿè¡Œã‚’ã¾ãŸã„ã§å·®åˆ†å–å¾—ã™ã‚‹ï¼‰"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f) or {}
        urls = data.get("urls", []) or []
        return set(_norm_id(u) for u in urls if u)
    except Exception:
        return set()

def _save_seen_set(seen: set, path: str = SEEN_CACHE_PATH_IRRAWADDY, max_urls: int = 5000):
    """ä¿å­˜ã‚µã‚¤ã‚ºã‚’æŠ‘ãˆã¤ã¤ JSON ã§ä¿å­˜"""
    try:
        urls = list(seen)
        if len(urls) > max_urls:
            urls = urls[-max_urls:]
        with open(path, "w", encoding="utf-8") as f:
            json.dump({"urls": urls, "ts": datetime.utcnow().isoformat() + "Z"}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[cache] save failed: {e}")

def fetch_irrawaddy_resilient(
    url: str,
    session=None,
    max_attempts: int = None,
    base_delay: float = None,
    cap: float = None,
    max_seconds: int = None,
) -> bytes:
    """
    Irrawaddyå‘ã‘ã®å …ç‰¢ãƒ•ã‚§ãƒƒãƒï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ç‰ˆï¼‰
    - å¤±æ•—æ™‚ã¯æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤
    - æˆåŠŸæ™‚ã®ã¿ 8â€“12 ç§’ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ï¼ˆæ¬¡ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’é–“å¼•ãï¼‰
    - è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ï¼ˆ404/410/æ˜ç™½ãªDNS/URLä¸æ­£ï¼‰ã¯å³çµ‚äº†
    - å›æ•°ãƒ»æ™‚é–“ã®ä¸¡æ–¹ã«ä¸Šé™ã‚¬ãƒ¼ãƒ‰
    ç’°å¢ƒå¤‰æ•°:
      IRRAWADDY_MAX_ATTEMPTS  (æ—¢å®š: 4)
      IRRAWADDY_BASE_DELAY_S  (æ—¢å®š: 8)
      IRRAWADDY_BACKOFF_CAP_S (æ—¢å®š: 90)
      IRRAWADDY_MAX_SECONDS   (æ—¢å®š: 180)
    """
    if max_attempts is None:
        max_attempts = int(os.getenv("IRRAWADDY_MAX_ATTEMPTS", "4"))
    if base_delay is None:
        base_delay = float(os.getenv("IRRAWADDY_BASE_DELAY_S", "8"))
    if cap is None:
        cap = float(os.getenv("IRRAWADDY_BACKOFF_CAP_S", "90"))
    if max_seconds is None:
        max_seconds = int(os.getenv("IRRAWADDY_MAX_SECONDS", "180"))

    deadline = time.monotonic() + max_seconds
    last = None
    attempt = 0

    while attempt < max_attempts and time.monotonic() < deadline:
        try:
            buf = fetch_once_irrawaddy_auto(url, session=session)
            _sleep_jitter(8.0, 12.0)  # æ¬¡URLã¸è¡Œãå‰ã®é–“å¼•ã
            return buf
        except Exception as e:
            last = e
            msg = str(e) if e else ""

            fatal_signatures = (
                " 404", " 410", "Not Found", "not found",
                "Invalid URL", "invalid url",
                "Name or service not known", "getaddrinfo failed", "DNS", "TLSV1_ALERT"
            )
            if any(sig in msg for sig in fatal_signatures):
                break

            wait = _exp_backoff(attempt, base=base_delay, cap=cap)
            remaining = deadline - time.monotonic()
            if wait > max(0, remaining):
                break

            print(f"[irrawaddy] retry {attempt+1}/{max_attempts} after: {e} (sleep {wait:.1f}s) â†’ {url}")
            time.sleep(wait)
            attempt += 1

    raise last or RuntimeError(f"irrawaddy fetch failed (exhausted): {url}")

# ==== RSS ä¸€æ¬¡çµŒè·¯ï¼ˆå½“æ—¥è¨˜äº‹URLã®åé›†ï¼‰ ====
def _collect_irrawaddy_candidates_via_rss(target_date_mmt) -> list:
    """
    RSS/Atom ã‚’ä¸€æ¬¡çµŒè·¯ã¨ã—ã¦åˆ©ç”¨ã—ã€MMTåŸºæº–ã§å½“æ—¥è¨˜äº‹ã® URL ã‚’é›†ã‚ã‚‹ã€‚
    - WordPress æ¨™æº–ã®ãƒ•ã‚£ãƒ¼ãƒ‰ã‚„ä¸»è¦ã‚«ãƒ†ã‚´ãƒªã®ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’é †æ¬¡ç¢ºèª
    - å–å¾—å¤±æ•—ã¯é»™ã£ã¦ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå …ç‰¢æ€§é‡è¦–ï¼‰
    """
    FEEDS = [
        "https://www.irrawaddy.com/feed",
        "https://www.irrawaddy.com/category/news/feed",
        "https://www.irrawaddy.com/category/politics/feed",
        "https://www.irrawaddy.com/category/business/feed",
        "https://www.irrawaddy.com/category/Features/feed",
        "https://www.irrawaddy.com/category/Opinion/feed",
        "https://www.irrawaddy.com/category/in-person/feed",
        "https://www.irrawaddy.com/category/Specials/feed",
    ]
    urls = []
    for feed_url in FEEDS:
        try:
            r = requests.get(feed_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
            if r.status_code != 200 or not (r.text or "").strip():
                continue
            root = ET.fromstring(r.text)
            items = root.findall(".//item")
            if not items:
                items = root.findall(".//{http://www.w3.org/2005/Atom}entry")
            for it in items:
                pub = (it.findtext("pubDate") or it.findtext("{http://www.w3.org/2005/Atom}updated") or "").strip()
                link = (it.findtext("link") or "").strip()
                if not link:
                    l = it.find("{http://www.w3.org/2005/Atom}link")
                    if l is not None:
                        link = (l.get("href") or "").strip()
                if not link or not pub:
                    continue
                try:
                    dt = parse_date(pub)
                    if dt.astimezone(MMT).date() == target_date_mmt:
                        urls.append(link)
                except Exception:
                    continue
        except Exception as e:
            print(f"[rss] skip {feed_url}: {e}")

    uniq, seen = [], set()
    for u in urls:
        k = _norm_id(u)
        if k not in seen:
            seen.add(k)
            uniq.append(u)
    return uniq

# === DVBå°‚ç”¨ ===
def fetch_with_retry_dvb(url, retries=4, wait_seconds=2, session=None):
    """
    DVB (https://burmese.dvb.no) å‘ã‘ã®å¤šæ®µãƒ•ã‚§ãƒƒãƒãƒ£ã€‚
    1) curl_cffi(ChromeæŒ‡ç´‹) â†’ 2) cloudscraper â†’ 3) requests ã®é †ã€‚
    403/429/503 ã¯æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã€‚/post/* ã§ã¯ /amp / ?output=amp ã‚‚è©¦ã™ã€‚
    """
    import os
    import time
    import random

    UA = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/128.0.0.0 Safari/537.36"
    )
    BASE = "https://burmese.dvb.no"
    HEADERS = {
        "User-Agent": UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9,my;q=0.8,ja;q=0.7",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-User": "?1",
        "Sec-Fetch-Dest": "document",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": f"{BASE}/",
        "Connection": "keep-alive",
    }

    def _amp_candidates(u: str):
        u = u.strip()
        q = "&" if "?" in u else "?"
        return [u.rstrip("/") + "/amp", u + f"{q}output=amp"]

    # --- Try 1: curl_cffi ---
    try:
        from curl_cffi import requests as cfr  # type: ignore

        proxies = {
            "http": os.getenv("HTTP_PROXY") or os.getenv("http_proxy"),
            "https": os.getenv("HTTPS_PROXY") or os.getenv("https_proxy"),
        }
        for attempt in range(retries):
            r = cfr.get(
                url,
                headers=HEADERS,
                impersonate="chrome124",
                timeout=30,
                allow_redirects=True,
                proxies={k: v for k, v in proxies.items() if v},
            )
            if r.status_code == 200 and (r.text or "").strip():
                return r
            # è¨˜äº‹URLã¯AMPç³»ã‚‚è©¦ã™
            if r.status_code in (403, 503) and "/post/" in url:
                for amp in _amp_candidates(url):
                    r2 = cfr.get(
                        amp,
                        headers=HEADERS,
                        impersonate="chrome124",
                        timeout=30,
                        allow_redirects=True,
                        proxies={k: v for k, v in proxies.items() if v},
                    )
                    if r2.status_code == 200 and (r2.text or "").strip():
                        return r2
            if r.status_code in (403, 429, 503):
                time.sleep(wait_seconds * (2**attempt) + random.uniform(0, 0.8))
                continue
            break
    except Exception as e:
        print(f"[dvb-cffi] EXC: {e} â†’ {url}")

    # --- Try 2: cloudscraper ---
    try:
        import cloudscraper
        import requests as rq

        sess = session or rq.Session()
        scraper = cloudscraper.create_scraper(
            sess=sess,
            browser={"browser": "chrome", "platform": "windows", "mobile": False},
            delay=7,
        )
        for attempt in range(retries):
            try:
                r = scraper.get(url, headers=HEADERS, timeout=30, allow_redirects=True)
                if r.status_code == 200 and getattr(r, "text", "").strip():
                    return r
                if r.status_code in (403, 503) and "/post/" in url:
                    for amp in _amp_candidates(url):
                        r2 = scraper.get(
                            amp, headers=HEADERS, timeout=30, allow_redirects=True
                        )
                        if r2.status_code == 200 and getattr(r2, "text", "").strip():
                            return r2
                if r.status_code in (403, 429, 503):
                    time.sleep(wait_seconds * (2**attempt) + random.uniform(0, 0.8))
                    continue
                break
            except Exception as e:
                print(f"[dvb-cs] {attempt+1}/{retries} EXC: {e} â†’ {url}")
                time.sleep(wait_seconds * (2**attempt) + random.uniform(0, 0.8))
    except Exception as e:
        print(f"[dvb-cs] INIT EXC: {e} â†’ {url}")

    # --- Try 3: requests ---
    try:
        import requests

        sess = session or requests.Session()
        r2 = sess.get(url, headers=HEADERS, timeout=30, allow_redirects=True)
        if r2.status_code == 200 and getattr(r2, "text", "").strip():
            return r2
        if r2.status_code in (403, 503) and "/post/" in url:
            for amp in _amp_candidates(url):
                r3 = sess.get(amp, headers=HEADERS, timeout=30, allow_redirects=True)
                if r3.status_code == 200 and getattr(r3, "text", "").strip():
                    return r3
    except Exception as e:
        print(f"[dvb-rq] EXC final: {e} â†’ {url}")

    raise Exception(f"Failed to fetch DVB {url} after {retries} attempts.")


def _norm_text(text: str) -> str:
    return unicodedata.normalize("NFC", text)


## removed duplicate _norm_id (prefer earlier normalized form)


def _parse_category_date_text(text: str):
    # ä¾‹: 'August 9, 2025'
    text = re.sub(r"\s+", " ", text.strip())
    return datetime.strptime(text, "%B %d, %Y").date()


def _article_date_from_meta_mmt(soup):
    meta = soup.find("meta", attrs={"property": "article:published_time"})
    if not meta or not meta.get("content"):
        return None
    iso = meta["content"].replace("Z", "+00:00")  # æœ«å°¾Zå¯¾ç­–
    dt = datetime.fromisoformat(iso)
    return dt.astimezone(MMT).date()


def _extract_title(soup):
    # 1) æ˜ç¤ºã®è¦‹å‡ºã—
    h = soup.select_one("h1.jeg_post_title") or soup.select_one("h1.entry-title") or soup.find("h1")
    if h and h.get_text(strip=True):
        return _norm_text(h.get_text(strip=True))
    # 2) og:title
    og = soup.find("meta", attrs={"property": "og:title"})
    if og and og.get("content"):
        return _norm_text(og.get("content", "").strip())
    # 3) <title>
    t = soup.find("title")
    if t and t.get_text(strip=True):
        return _norm_text(t.get_text(strip=True))
    return None


def _title_from_slug(u: str) -> str:
    try:
        from urllib.parse import urlparse, unquote
        path = urlparse(u).path or ""
        seg = path.rstrip("/").split("/")[-1]
        seg = seg.replace(".html", "")
        seg = unquote(seg)
        seg = seg.replace("-", " ")
        # å…ˆé ­å¤§æ–‡å­—åŒ–ï¼ˆè‹±æ–‡ã‚¿ã‚¤ãƒˆãƒ«ç”¨ã®ç°¡æ˜“æ•´å½¢ï¼‰
        return _norm_text(seg.title())
    except Exception:
        return ""


def resolve_gnews_url(u: str) -> str:
    """Resolve a Google News redirect URL to an Irrawaddy direct URL, if possible.
    Tries requests â†’ curl_cffi â†’ cloudscraper, and falls back to scanning HTML anchors.
    Returns the best-effort resolved URL (or the original on failure).
    """
    try:
        if (not u) or ("irrawaddy.com" in u) or ("news.google.com" not in u):
            return u
        UA = (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/128.0.0.0 Safari/537.36"
        )

        def _scan_html_for_anchor(html: str) -> str:
            if not html:
                return ""
            soup_tmp = BeautifulSoup(html, "html.parser")
            anchors = []
            for a in soup_tmp.find_all("a", href=True):
                href = a.get("href") or ""
                # www.irrawaddy.com ã®ã¿ã‚’æ¡ç”¨
                if "www.irrawaddy.com" not in href:
                    continue
                anchors.append(href)
            def _score(href: str) -> int:
                try:
                    from urllib.parse import urlparse as _urlparse
                    p = _urlparse(href)
                    path = (p.path or "").lower()
                except Exception:
                    path = href.lower()
                s = 0
                if "/news/" in path:
                    s += 5
                if re.search(r"/20\d{2}/\d{2}/\d{2}/", path):
                    s += 3
                if path.endswith(".html"):
                    s += 2
                if any(x in path for x in ("/tag/", "/category/", "/author/", "/search/")):
                    s -= 5
                return s
            if anchors:
                return max(anchors, key=_score)
            return ""

        # Try 1: requests
        try:
            r = requests.get(
                u,
                headers={"User-Agent": UA},
                timeout=12,
                allow_redirects=True,
            )
            fu = getattr(r, "url", "") or ""
            if fu and "www.irrawaddy.com" in fu:
                return fu
            hit = _scan_html_for_anchor(getattr(r, "text", "") or "")
            if hit:
                return hit
        except Exception:
            pass

        # Try 2: curl_cffi
        try:
            from curl_cffi import requests as cfr  # type: ignore
            r2 = cfr.get(u, headers={"User-Agent": UA}, impersonate="chrome124", timeout=20, allow_redirects=True)
            fu2 = getattr(r2, "url", "") or ""
            if fu2 and "www.irrawaddy.com" in fu2:
                return fu2
            hit2 = _scan_html_for_anchor(getattr(r2, "text", "") or "")
            if hit2:
                return hit2
        except Exception:
            pass

        # Try 3: cloudscraper
        try:
            import cloudscraper
            sc = cloudscraper.create_scraper(
                browser={"browser": "chrome", "platform": "windows", "mobile": False},
                delay=7,
            )
            r3 = sc.get(u, headers={"User-Agent": UA}, timeout=20, allow_redirects=True)
            fu3 = getattr(r3, "url", "") or ""
            if fu3 and "www.irrawaddy.com" in fu3:
                return fu3
            hit3 = _scan_html_for_anchor(getattr(r3, "text", "") or "")
            if hit3:
                return hit3
        except Exception:
            pass
    except Exception:
        pass
    return u


def is_irrawaddy_tag_url(u: str) -> bool:
    try:
        from urllib.parse import urlparse as _urlparse
        p = _urlparse(u)
        host = (p.netloc or "").lower()
        path = (p.path or "").lower()
        return ("irrawaddy.com" in host) and (path.startswith("/tag") or "/tag/" in path)
    except Exception:
        return False


def pick_article_from_irrawaddy_tag(u: str, target_date: date) -> str:
    """Given a tag page URL on Irrawaddy (including burma.irrawaddy.com),
    try to pick an article URL for the target MMT date.
    Prefers /news/YYYY/MM/DD/*.html pattern.
    Returns original URL on failure.
    """
    try:
        UA = (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/128.0.0.0 Safari/537.36"
        )
        def _fetch_any(url: str):
            try:
                r = requests.get(url, headers={"User-Agent": UA}, timeout=15)
                if r.status_code == 200 and (r.text or "").strip():
                    return r.text
            except Exception:
                pass
            try:
                from curl_cffi import requests as cfr  # type: ignore
                r2 = cfr.get(url, headers={"User-Agent": UA}, impersonate="chrome124", timeout=25)
                if r2.status_code == 200 and (r2.text or "").strip():
                    return r2.text
            except Exception:
                pass
            try:
                import cloudscraper
                sc = cloudscraper.create_scraper(
                    browser={"browser": "chrome", "platform": "windows", "mobile": False},
                    delay=7,
                )
                r3 = sc.get(url, headers={"User-Agent": UA}, timeout=25)
                if r3.status_code == 200 and (r3.text or "").strip():
                    return r3.text
            except Exception:
                pass
            return ""

        html = _fetch_any(u)
        if not html:
            return u
        soup = BeautifulSoup(html, "html.parser")
        anchors = [a.get("href") or "" for a in soup.find_all("a", href=True)]
        anchors = [h for h in anchors if "irrawaddy.com" in h]

        y = f"{target_date.year:04d}"
        m = f"{target_date.month:02d}"
        d = f"{target_date.day:02d}"
        date_pat = rf"/{y}/{m}/{d}/"

        def _score(h: str) -> int:
            try:
                from urllib.parse import urlparse as _urlparse
                path = (_urlparse(h).path or "").lower()
            except Exception:
                path = h.lower()
            s = 0
            if "/news/" in path:
                s += 5
            if re.search(date_pat, path):
                s += 4
            if path.endswith(".html"):
                s += 2
            if any(x in path for x in ("/tag/", "/category/", "/author/", "/search/")):
                s -= 5
            return s

        cands = sorted(anchors, key=_score, reverse=True)
        if cands and _score(cands[0]) > 0:
            return cands[0]
    except Exception:
        pass
    return u


def _oembed_title_irrawaddy(u: str) -> str:
    try:
        api = (
            "https://www.irrawaddy.com/wp-json/oembed/1.0/embed?url="
            + requests.utils.requote_uri(u)
        )
        # æ©Ÿæ¢°APIç³»ã«ã¯é€£çµ¡å…ˆå…¥ã‚Šã®UAã‚’ä½¿ç”¨ï¼ˆHTMLç›´å–å¾—ã¨ã¯åˆ†é›¢ï¼‰
        contact = os.getenv("CONTACT_EMAIL") or os.getenv("CONTACT_URL")
        ua_contact = (
            f"MyanmarNewsDigestBot/1.0 (+{contact})" if contact else "MyanmarNewsDigestBot/1.0"
        )
        r = requests.get(
            api,
            headers={
                "User-Agent": ua_contact,
                "Accept": "application/json, text/javascript, */*;q=0.1",
            },
            timeout=10,
        )
        if r.status_code == 200 and r.text.strip():
            try:
                data = r.json()
            except Exception:
                # JSONã¨ã—ã¦èª­ã‚ãªã„å ´åˆã¯è«¦ã‚ã‚‹
                return ""
            t = (data.get("title") or "").strip()
            return _norm_text(t)
    except Exception:
        pass
    return ""


def _is_excluded_by_ancestor(node) -> bool:
    excluded = {
        "jnews_inline_related_post",
        "jeg_postblock_21",
        "widget",
        "widget_jnews_popular",
        "jeg_postblock_5",
        "jnews_related_post_container",
        "widget widget_jnews_popular",
        "jeg_footer_primary clearfix",
    }
    for anc in node.parents:
        classes = anc.get("class", [])
        if any(c in excluded for c in classes):
            return True
    return False


# æœ¬æ–‡æŠ½å‡º
def extract_body_irrawaddy(soup):
    # <div class="content-inner "> é…ä¸‹ã® <p>ã®ã¿ï¼ˆé™¤å¤–ãƒ–ãƒ­ãƒƒã‚¯é…ä¸‹ã¯é™¤å¤–ï¼‰
    paragraphs = []
    content_inners = soup.select("div.content-inner")
    if not content_inners:
        content_inners = [
            div
            for div in soup.find_all("div")
            if "content-inner" in (div.get("class") or [])
        ]
    for root in content_inners:
        for p in root.find_all("p"):
            if _is_excluded_by_ancestor(p):
                continue
            txt = p.get_text(strip=True)
            if txt:
                paragraphs.append(_norm_text(txt))
    return "\n".join(paragraphs).strip()


#  Irrawaddy ç”¨ fetch_onceï¼ˆæ—¢å­˜ã® fetch_with_retry_irrawaddy ã‚’1å›ãƒ©ãƒƒãƒ—ï¼‰
def fetch_once_irrawaddy(url, session=None):
    r = fetch_with_retry_irrawaddy(url, retries=1, wait_seconds=0, session=session)
    # cloudscraper ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚‚ bytes ã‚’è¿”ã™ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ã¯ BeautifulSoup ã«ä»»ã›ã‚‹ï¼‰
    return r.content


# === ã“ã“ã¾ã§ ===


# ===== ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªãƒ’ãƒƒãƒˆæ™‚ã®å…±é€šãƒ­ã‚¬ãƒ¼ï¼ˆç°¡ç´ ç‰ˆï¼‰ =====
LOG_NO_KEYWORD_MISSES = True


def log_no_keyword_hit(source: str, url: str, title: str, body: str, stage: str):
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªãƒ’ãƒƒãƒˆã®è¨˜äº‹ã‚’æ¨™æº–å‡ºåŠ›ã«å‡ºã™ï¼ˆstageãƒ»æœ¬æ–‡æŠœç²‹ã¯å‡ºåŠ›ã—ãªã„ï¼‰ã€‚
    """
    if not LOG_NO_KEYWORD_MISSES:
        return
    try:
        title = unicodedata.normalize("NFC", title or "")
    except Exception:
        pass

    print("\n----- NO KEYWORD HIT -----")
    print(f"[source] {source}")
    print(f"[url]    {url}")
    print(f"[title]  {title}")
    print("----- END NO KEYWORD HIT -----\n")


# Mizzimaã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸å·¡å›ã§å–å¾—
def get_mizzima_articles_from_category(
    date_obj, base_url, source_name, category_path, max_pages=3
):
    # ==== ãƒ­ãƒ¼ã‚«ãƒ«å®šæ•° Mizzimaé™¤å¤–å¯¾è±¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼‰====
    EXCLUDE_TITLE_KEYWORDS = [
        # æ˜¥ã®é©å‘½æ—¥èªŒ
        "á€”á€½á€±á€¦á€¸á€á€±á€¬á€ºá€œá€¾á€”á€ºá€›á€±á€¸ á€”á€±á€·á€…á€‰á€ºá€™á€¾á€á€ºá€…á€¯",
        # å†™çœŸãƒ‹ãƒ¥ãƒ¼ã‚¹
        "á€“á€¬á€á€ºá€•á€¯á€¶á€á€á€„á€ºá€¸",
    ]

    article_urls = []

    for page_num in range(1, max_pages + 1):
        if page_num == 1:
            url = f"{base_url}{category_path}"
        else:
            url = f"{base_url}{category_path}/page/{page_num}/"

        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                continue

            soup = BeautifulSoup(res.content, "html.parser")
            links = [
                a["href"]
                for a in soup.select("main.site-main article a.post-thumbnail[href]")
            ]
            article_urls.extend(links)

        except Exception as e:
            print(f"Error crawling category page {url}: {e}")
            continue

    filtered_articles = []
    for url in article_urls:
        try:
            res_article = fetch_with_retry(url)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            meta_tag = soup_article.find("meta", property="article:published_time")
            if not meta_tag or not meta_tag.has_attr("content"):
                continue

            date_str = meta_tag["content"]
            article_datetime_utc = datetime.fromisoformat(date_str)
            article_datetime_mmt = article_datetime_utc.astimezone(MMT)
            article_date = article_datetime_mmt.date()

            if article_date != date_obj:
                continue

            title_tag = soup_article.find("meta", attrs={"property": "og:title"})
            if not title_tag or not title_tag.has_attr("content"):
                continue
            title = title_tag["content"].strip()

            # === é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‚’NFCæ­£è¦åŒ–ã—ã¦ã‹ã‚‰ï¼‰ ===
            title_nfc = unicodedata.normalize("NFC", title)
            if any(kw in title_nfc for kw in EXCLUDE_TITLE_KEYWORDS):
                print(f"SKIP: excluded keyword in title â†’ {url} | TITLE: {title_nfc}")
                continue

            content_div = soup_article.find("div", class_="entry-content")
            if not content_div:
                continue

            paragraphs = []
            for p in content_div.find_all("p"):
                if p.find_previous("h2", string=re.compile("Related Posts", re.I)):
                    break
                paragraphs.append(p)

            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            body_text = unicodedata.normalize("NFC", body_text)

            if not body_text.strip():
                continue

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šã¯æ­£è¦åŒ–æ¸ˆã¿ã‚¿ã‚¤ãƒˆãƒ«ã§è¡Œã†
            if not any_keyword_hit(title, body_text):
                log_no_keyword_hit(
                    source_name, url, title, body_text, "mizzima:category"
                )
                continue

            filtered_articles.append(
                {
                    "source": source_name,
                    "url": url,
                    "title": title,
                    "date": article_date.isoformat(),
                    "body": body_text,
                }
            )

        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return filtered_articles


# BCCã¯RSSã‚ã‚‹ã®ã§ãã‚Œä½¿ã†
def get_bbc_burmese_articles_for(target_date_mmt):
    
    # ==== ãƒ­ãƒ¼ã‚«ãƒ«å®šæ•° ====
    NOISE_PATTERNS = [
        r"BBC\s*News\s*á€™á€¼á€”á€ºá€™á€¬",  # å›ºå®šç½²åï¼ˆBurmeseè¡¨è¨˜ï¼‰
        r"BBC\s*Burmese",  # è‹±èªè¡¨è¨˜
    ]

    # BBC Burmese ã®ã€Œã‚¿ã‚¤ãƒˆãƒ«é™¤å¤–ã€ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå°†æ¥è¿½åŠ ã—ã‚„ã™ã„ã‚ˆã†é…åˆ—ã§ä¿æŒï¼‰
    EXCLUDE_TITLE_KEYWORDS = [
        "á€”á€­á€¯á€„á€ºá€„á€¶á€á€á€”á€ºá€¸á€á€á€„á€ºá€¸á€™á€»á€¬á€¸ á€¡á€”á€¾á€…á€ºá€á€»á€¯á€•á€º",
    ]
    # åˆæˆå·®ç•°ã‚’é¿ã‘ã‚‹ãŸã‚ NFC ã«æ­£è¦åŒ–ã—ã¦ãŠã
    EXCLUDE_TITLE_KEYWORDS = [
        unicodedata.normalize("NFC", kw) for kw in EXCLUDE_TITLE_KEYWORDS
    ]

    def _remove_noise_phrases(text: str) -> str:
        """BBCç½²åãªã©ã®ãƒã‚¤ã‚ºãƒ•ãƒ¬ãƒ¼ã‚ºã‚’é™¤å»"""
        if not text:
            return text
        for pat in NOISE_PATTERNS:
            text = re.sub(pat, "", text, flags=re.IGNORECASE)
        return text.strip()

    # MEMO: ãƒ­ã‚°ç”¨
    # ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆä¸­ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã“ã«ãƒ’ãƒƒãƒˆã—ãŸã‹ã‚’è¿”ã™ï¼ˆå‘¨è¾ºæ–‡è„ˆã¤ãï¼‰
    # def _find_hits(text: str, keywords):
    #     hits = []
    #     for kw in keywords:
    #         start = 0
    #         while True:
    #             i = text.find(kw, start)
    #             if i == -1:
    #                 break
    #             s = max(0, i - 30)
    #             e = min(len(text), i + len(kw) + 30)
    #             ctx = text[s:e].replace("\n", " ")
    #             hits.append({"kw": kw, "pos": i, "ctx": ctx})
    #             start = i + len(kw)
    #     return hits

    rss_url = "https://feeds.bbci.co.uk/burmese/rss.xml"
    session = requests.Session()

    try:
        res = session.get(rss_url, timeout=10)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    soup = BeautifulSoup(res.content, "xml")
    articles = []

    for item in soup.find_all("item"):
        pub_date_tag = item.find("pubDate")
        if not pub_date_tag:
            continue

        # RSSã¯UTC â†’ MMTã¸å¤‰æ›ã—ã€å¯¾è±¡æ—¥ã ã‘é€šã™
        try:
            pub_date = parse_date(pub_date_tag.text)
            pub_date_mmt = pub_date.astimezone(MMT).date()
        except Exception as e:
            print(f"âŒ pubDate parse error: {e}")
            continue

        if pub_date_mmt != target_date_mmt:
            continue

        title = (
            (item.find("title") or {}).get_text(strip=True)
            if item.find("title")
            else ""
        )
        link = (
            (item.find("link") or {}).get_text(strip=True) if item.find("link") else ""
        )
        if not link:
            continue
        
        # === ã‚¿ã‚¤ãƒˆãƒ«é™¤å¤–ï¼ˆRSS ã‹ã‚‰å–å¾—ã—ãŸã‚¿ã‚¤ãƒˆãƒ«ã§å…ˆã«åˆ¤å®šã—ã¦æ—©æœŸã‚¹ã‚­ãƒƒãƒ—ï¼‰ ===
        rss_title_nfc = unicodedata.normalize("NFC", title or "")
        if any(kw in rss_title_nfc for kw in EXCLUDE_TITLE_KEYWORDS):
            print(f"SKIP: excluded title keyword (BBC) â†’ {link} | TITLE: {rss_title_nfc}")
            continue

        try:
            article_res = session.get(link, timeout=10)
            article_res.raise_for_status()
            article_soup = BeautifulSoup(article_res.content, "html.parser")

            # ===== ã“ã“ã§é™¤å¤–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã¾ã¨ã‚ã¦å‰Šé™¤ =====
            # è¨˜äº‹ç½²åã‚„ãƒ¡ã‚¿æƒ…å ±
            for node in article_soup.select(
                'section[role="region"][aria-labelledby="article-byline"]'
            ):
                node.decompose()
            # ã€ŒãŠã™ã™ã‚ï¼æœ€ã‚‚èª­ã¾ã‚ŒãŸã€ãƒ–ãƒ­ãƒƒã‚¯
            for node in article_soup.select(
                'section[data-e2e="recommendations-heading"][role="region"]'
            ):
                node.decompose()
            # ã¤ã„ã§ã«ãƒ˜ãƒƒãƒ€ãƒ¼/ãƒŠãƒ“/ãƒ•ãƒƒã‚¿ãƒ¼ç­‰ã®ãƒã‚¤ã‚ºã‚‚è½ã¨ã—ã¦ãŠãï¼ˆä»»æ„ï¼‰
            for node in article_soup.select(
                'header[role="banner"], nav[role="navigation"], footer[role="contentinfo"], aside'
            ):
                node.decompose()
            # ============================================

            # æœ¬æ–‡ã¯ main å†…ã® <p> ã«é™å®š
            main = article_soup.select_one('main[role="main"]') or article_soup
            paragraphs = [p.get_text(strip=True) for p in main.find_all("p")]
            # ç©ºè¡Œã‚„ãƒã‚¤ã‚ºã‚’å‰Šã‚‹
            paragraphs = [t for t in paragraphs if t]
            body_text = "\n".join(paragraphs)

            # ãƒŸãƒ£ãƒ³ãƒãƒ¼æ–‡å­—ã®åˆæˆå·®ç•°ã‚’é¿ã‘ã‚‹ãŸã‚ NFC æ­£è¦åŒ–
            title_nfc = unicodedata.normalize("NFC", title)
            title_nfc = _remove_noise_phrases(title_nfc)
            body_text_nfc = unicodedata.normalize("NFC", body_text)
            body_text_nfc = _remove_noise_phrases(body_text_nfc)

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
            if not any_keyword_hit(title_nfc, body_text_nfc):
                log_no_keyword_hit(
                    "BBC Burmese", link, title_nfc, body_text_nfc, "bbc:article"
                )
                continue

            # MEMO: ãƒ­ã‚°ç”¨ã€=== ãƒ‡ãƒãƒƒã‚°: åˆ¤å®šå‰ã«ã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡ã®è¦ç´„ã‚’å‡ºã™ ===
            # print("----- DEBUG CANDIDATE -----")
            # print("URL:", link)
            # print("TITLE:", repr(title_nfc))
            # print("BODY_HEAD:", repr(body_text_nfc[:500]))
            # print("BODY_LEN:", len(body_text_nfc))

            # # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆãƒ’ãƒƒãƒˆè©³ç´°ã‚‚å–ã‚‹ï¼‰
            # title_hits = _find_hits(title_nfc, NEWS_KEYWORDS)
            # body_hits  = _find_hits(body_text_nfc, NEWS_KEYWORDS)
            # total_hits = title_hits + body_hits

            # if not total_hits:
            #     print("SKIP: no keyword hits.")
            #     continue

            # # === ãƒ‡ãƒãƒƒã‚°: ã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã“ã§å½“ãŸã£ãŸã‹ ===
            # print("HITS:", len(total_hits))
            # if title_hits:
            #     print(" - in TITLE:")
            #     for h in title_hits[:10]:
            #         print(f"   kw={repr(h['kw'])} ctx=â€¦{h['ctx']}â€¦")
            # if body_hits:
            #     print(" - in BODY:")
            #     for h in body_hits[:10]:  # é•·ããªã‚‹ã®ã§æœ€å¤§10ä»¶
            #         print(f"   kw={repr(h['kw'])} ctx=â€¦{h['ctx']}â€¦")

            print(f"âœ… æŠ½å‡ºè¨˜äº‹: {title_nfc} ({link})")
            articles.append(
                {
                    "title": title_nfc,
                    "url": link,
                    "date": pub_date_mmt.isoformat(),
                    "source": "BBC Burmese",
                    "body": body_text_nfc,
                }
            )

        except Exception as e:
            print(f"âŒ è¨˜äº‹å–å¾—/è§£æã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return articles


# khit_thit_mediaã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸å·¡å›ã§å–å¾—
def get_khit_thit_media_articles_from_category(date_obj, max_pages=3):
    # è¿½åŠ ã‚«ãƒ†ã‚´ãƒªã‚’å«ã‚€å·¡å›å¯¾è±¡
    CATEGORY_URLS = [
        "https://yktnews.com/category/news/",
        "https://yktnews.com/category/politics/",
        "https://yktnews.com/category/editor-choice/",
        "https://yktnews.com/category/interview/",
        "https://yktnews.com/category/china-watch/",
    ]

    HASHTAG_TOKEN_RE = re.compile(
        r"(?:(?<=\s)|^)\#[^\s#]+"
    )  # ç©ºç™½orè¡Œé ­ã‹ã‚‰å§‹ã¾ã‚‹ #ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»ï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰

    def _remove_hashtag_links(soup):
        """
        <a>ã‚„<strong><a>â€¦</a></strong>ã®ã‚ˆã†ãªå…¥ã‚Œå­ã‚’å«ã‚ã€
        ãƒ†ã‚­ã‚¹ãƒˆãŒ '#' ã§å§‹ã¾ã‚‹ã‚¢ãƒ³ã‚«ãƒ¼ã‚’æœ¬æ–‡ã‹ã‚‰é™¤å»ã™ã‚‹ã€‚
        """
        # aè¦ç´ ã®ä¸­ã§å¯è¦–ãƒ†ã‚­ã‚¹ãƒˆãŒ '#' ã§å§‹ã¾ã‚‹ã‚‚ã®ã‚’ã¾ã‚‹ã”ã¨å‰Šé™¤
        for a in soup.select("a"):
            txt = a.get_text(strip=True)
            if txt.startswith("#"):
                a.decompose()

    collected_urls = set()
    for base_url in CATEGORY_URLS:
        for page in range(1, max_pages + 1):
            url = f"{base_url}page/{page}/" if page > 1 else base_url
            print(f"Fetching {url}")
            try:
                res = fetch_with_retry(url)
            except Exception as e:
                print(f"[khitthit] stop pagination (missing/unreachable): {url} -> {e}")
                break

            soup = BeautifulSoup(res.content, "html.parser")
            entry_links = soup.select("p.entry-title.td-module-title a[href]")
            if not entry_links:
                print(f"[khitthit] stop pagination (no entries): {url}")
                break

            for a in entry_links:
                href = a.get("href")
                if not href:
                    continue
                if href in collected_urls:  # â† æ—¢å‡ºURLã¯æ˜ç¤ºã‚¹ã‚­ãƒƒãƒ—
                    continue
                collected_urls.add(href)

    filtered_articles = []
    for url in collected_urls:
        try:
            res_article = fetch_with_retry(url)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            # æ—¥ä»˜å–å¾—
            meta_tag = soup_article.find("meta", property="article:published_time")
            if not meta_tag or not meta_tag.has_attr("content"):
                continue
            date_str = meta_tag["content"]
            article_datetime_utc = datetime.fromisoformat(date_str)
            article_datetime_mmt = article_datetime_utc.astimezone(MMT)
            article_date = article_datetime_mmt.date()
            if article_date != date_obj:
                continue  # å¯¾è±¡æ—¥ã§ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_tag = soup_article.find("h1")
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)

            # æœ¬æ–‡å–å¾—  â† ã“ã®ç›´å‰ã« â€œãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°é™¤å»â€ ã‚’å·®ã—è¾¼ã‚€
            _remove_hashtag_links(soup_article)  # â‘  HTMLæ®µéšã§ #ã‚¢ãƒ³ã‚«ãƒ¼ã‚’é™¤å»
            paragraphs = extract_paragraphs_with_wait(soup_article)
            # â‘¡ ãƒ†ã‚­ã‚¹ãƒˆåŒ–å¾Œã‚‚ä¿é™ºã§ #ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»
            body_text = "\n".join(
                HASHTAG_TOKEN_RE.sub("", p.get_text(strip=True)).strip()
                for p in paragraphs
                if p.get_text(strip=True)  # ç©ºãƒ‘ãƒ©ã¯ãã‚‚ãã‚‚æ¨ã¦ã‚‹
            )
            body_text = unicodedata.normalize("NFC", body_text)
            if not body_text.strip():
                continue  # æœ¬æ–‡ãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—

            if not any_keyword_hit(title, body_text):
                log_no_keyword_hit(
                    "Khit Thit Media", url, title, body_text, "khitthit:category"
                )
                continue  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç„¡ã—ã¯é™¤å¤–

            filtered_articles.append(
                {
                    "url": url,
                    "title": title,
                    "date": date_obj.isoformat(),
                    "source": "Khit Thit Media",  # deduplicate_by_urlã®ãƒ­ã‚°ã§ä½¿ã‚ã‚Œã‚‹
                    "body": body_text,
                }
            )
        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    before = len(filtered_articles)
    filtered_articles = deduplicate_by_url(filtered_articles)
    print(f"[khitthit] dedup: {before} -> {len(filtered_articles)}")  # æœ€å°ãƒ­ã‚°

    return filtered_articles


# irrawaddy
def get_irrawaddy_articles_for(date_obj, debug=True):
    """
    æŒ‡å®šã® Irrawaddy ã‚«ãƒ†ã‚´ãƒªURLç¾¤ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ï¼‰ã‚’1å›ãšã¤å·¡å›ã—ã€
    MMTã®æŒ‡å®šæ—¥(æ—¢å®š: ä»Šæ—¥)ã«ãƒ’ãƒƒãƒˆã™ã‚‹è¨˜äº‹ã®ã¿è¿”ã™ã€‚
    ã•ã‚‰ã«ãƒ›ãƒ¼ãƒ  https://www.irrawaddy.com/ ã®
    data-id="kuDRpuo" ã‚«ãƒ©ãƒ å†…ã‹ã‚‰ã‚‚åŒæ§˜ã«å€™è£œåé›†ã™ã‚‹ã€‚

    - /category/news/asia, /category/news/world ã¯é™¤å¤–ï¼ˆå…ˆé ­ä¸€è‡´ãƒ»å¤§å°ç„¡è¦–ï¼‰
    - ä¸€è¦§ã§ã¯ã€Œæ™‚è¨ˆã‚¢ã‚¤ã‚³ãƒ³ä»˜ãã®æ—¥ä»˜ãƒªãƒ³ã‚¯ã€ã‹ã‚‰å½“æ—¥å€™è£œã‚’æŠ½å‡º
    - è¨˜äº‹å´ã§ã¯ <meta property="article:published_time"> ã‚’ MMT ã«å¤‰æ›ã—ã¦å†ç¢ºèª
    - æœ¬æ–‡ã¯ <div class="content-inner "> é…ä¸‹ã® <p> ã‹ã‚‰æŠ½å‡ºï¼ˆç‰¹å®šãƒ–ãƒ­ãƒƒã‚¯é…ä¸‹ã¯é™¤å¤–ï¼‰
    è¿”ã‚Šå€¤: [{url, title, date}]
    ä¾å­˜: MMT, get_today_date_mmt, fetch_with_retry, any_keyword_hit
    """

    session = requests.Session()

    # â–¼ è¿½åŠ ï¼šæ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨æŒ™å‹•ãƒ•ãƒ©ã‚°
    seen_persist = _load_seen_set()
    USE_RSS_PRIMARY = _bool_env("IRRAWADDY_RSS_PRIMARY", True)
    USE_RESILIENT_FIRST = _bool_env("IRRAWADDY_RESILIENT_FIRST", True)
    # ã‚«ãƒ†ã‚´ãƒª/ãƒ›ãƒ¼ãƒ ã®ä¸€è¦§å–å¾—ã«ã‚‚ Resilient/Playwright ã‚’é©ç”¨ï¼ˆæ—¢å®š ONï¼‰
    USE_RESILIENT_LISTING = _bool_env("IRRAWADDY_RESILIENT_LISTING", True)

    # ==== å·¡å›å¯¾è±¡ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã€é‡è¤‡ã‚ã‚Šã§ã‚‚OKï¼šå†…éƒ¨ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰ ====
    CATEGORY_PATHS_RAW = [
        "/category/news/",
        "/category/politics",
        "/category/news/war-against-the-junta",
        "/category/news/conflicts-in-numbers",
        "/category/news/junta-crony",
        "/category/news/ethnic-issues",
        "/category/business",
        "/category/business/economy",
        "/category/Features",
        "/category/Opinion",
        "/category/Opinion/editorial",
        "/category/Opinion/commentary",
        "/category/Opinion/guest-column",
        "/category/Opinion/analysis",
        "/category/in-person",
        "/category/in-person/interview",
        "/category/in-person/profile",
        "/category/Specials",
        "/category/specials/women",
        "/category/from-the-archive",
        "/category/Specials/myanmar-china-watch",
        # "/category/Video" # é™¤å¤–ä¾é ¼æœ‰
        # "/category/culture/books" #é™¤å¤–ä¾é ¼æœ‰
        # "/category/Cartoons" # é™¤å¤–ä¾é ¼æœ‰
        # "/category/election-2020", # 2021å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Opinion/letters", # 2014å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Dateline", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/specials/places-in-history", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/specials/on-this-day", # 2023å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Specials/myanmar-covid-19", # 2022å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Travel", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle/Food", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle/fashion-design", # 2019å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/photo", # 2016å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/photo-essay", # 2021å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
    ]
    BASE = "https://www.irrawaddy.com"
    EXCLUDE_PREFIXES = [
        "/category/news/asia",  # é™¤å¤–ä¾é ¼æœ‰
        "/category/news/world",  # é™¤å¤–ä¾é ¼æœ‰
        "/video",  # "/category/Video"ã¯é™¤å¤–å¯¾è±¡ã ãŒã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ã‚ã‚‹
        "/cartoons",  # "/category/Cartoons"ã¯é™¤å¤–å¯¾è±¡ã ãŒã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ã‚ã‚‹
        "/tag",      # ã‚¿ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¯é™¤å¤–
        "/author",   # è‘—è€…ãƒšãƒ¼ã‚¸ã‚‚é™¤å¤–
        "/search",   # æ¤œç´¢çµæœã‚‚é™¤å¤–
    ]  # å…ˆé ­ä¸€è‡´ãƒ»å¤§å°ç„¡è¦–

    def _is_excluded_url(href: str) -> bool:
        try:
            p = urlparse(href or "").path.lower()
        except Exception:
            p = (href or "").lower()
        return any(p.startswith(x) for x in EXCLUDE_PREFIXES)

    # ==== æ­£è¦åŒ–ãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ãƒ»é™¤å¤– ====
    def _norm(p: str) -> str:
        return re.sub(r"/{2,}", "/", p.strip())

    paths, seen = [], set()
    for p in CATEGORY_PATHS_RAW:
        q = _norm(p)
        if any(q.lower().startswith(x) for x in EXCLUDE_PREFIXES):
            continue
        if q not in seen:
            seen.add(q)
            paths.append(q)

    # 2) ç°¡æ˜“ãƒ­ã‚¬ãƒ¼ï¼ˆæ¶ˆã™æ™‚ã¯ã“ã®1è¡Œã¨ dbg(...) ã‚’æ¶ˆã™ã ã‘ï¼‰
    dbg = (lambda *a, **k: print(*a, **k)) if debug else (lambda *a, **k: None)

    results = []
    seen_urls = set()
    candidate_urls = []
    fallback_titles: Dict[str, str] = {}
    # åé›†å…ƒã®ç”±æ¥ï¼ˆcat/home/feedï¼‰ã¨ã€ãƒ•ã‚£ãƒ¼ãƒ‰ç”±æ¥ã®è£œåŠ©æƒ…å ±ï¼ˆtitle/dateï¼‰ã‚’ä¿æŒ
    origins: Dict[str, str] = {}
    feed_hints: Dict[str, Dict[str, str]] = {}
    # â–¼ è¿½åŠ ï¼šRSS ã‚’ä¸€æ¬¡çµŒè·¯ã«ã—ã¦å½“æ—¥å€™è£œã‚’å…ˆã«æŠ•å…¥ï¼ˆé‡è¤‡ã¯ seen_urls ã§å›é¿ï¼‰
    if USE_RSS_PRIMARY:
        try:
            rss_urls = _collect_irrawaddy_candidates_via_rss(date_obj)
            for u in rss_urls:
                if u and u not in seen_urls:
                    candidate_urls.append(u)
                    seen_urls.add(u)
                    origins[u] = "feed"
        except Exception as e:
            print(f"[irrawaddy] RSS primary failed: {e}")


    # ==== 1) å„ã‚«ãƒ†ã‚´ãƒªURLã‚’1å›ãšã¤å·¡å› â†’ å½“æ—¥å€™è£œæŠ½å‡º ====
    # ä¸€è¦§ãƒ•ã‚§ãƒƒãƒã®å…±é€šãƒ˜ãƒ«ãƒ‘ï¼ˆResilientâ†’å¤±æ•—æ™‚ã¯æ—¢å­˜ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    def _fetch_listing_html(url: str) -> bytes:
        if USE_RESILIENT_LISTING:
            try:
                html = fetch_irrawaddy_resilient(url, session=session)
                try:
                    dbg(f"[listing] resilient ok: {len(html)} bytes @ {url}")
                except Exception:
                    pass
                return html
            except Exception as e:
                print(f"[listing] resilient failed: {e} â†’ {url}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯æ—¢å­˜ã® requests ç³» + ã‚¸ãƒƒã‚¿ãƒ¼
        try:
            r = fetch_with_retry_irrawaddy(url, session=session)
            try:
                _sleep_jitter(8.0, 12.0)
            except Exception:
                pass
            html = getattr(r, "content", None) or (getattr(r, "text", "") or "").encode("utf-8", "ignore")
            try:
                dbg(f"[listing] fallback ok: {len(html)} bytes @ {url}")
            except Exception:
                pass
            return html
        except Exception as e:
            print(f"[listing] requests failed: {e} â†’ {url}")
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: r.jina.ai çµŒç”±ã§é™çš„æŠ½å‡º
            try:
                alt = f"https://r.jina.ai/http://{url.lstrip('/')}"
                rj = requests.get(
                    alt,
                    headers={
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
                        "Accept": "text/plain, */*;q=0.1",
                    },
                    timeout=25,
                )
                if rj.status_code == 200 and (rj.text or "").strip():
                    t = rj.text
                    try:
                        dbg(f"[listing] jina ok: {len(t)} bytes(text) @ {url}")
                    except Exception:
                        pass
                    return t.encode("utf-8", "ignore")
            except Exception as ee:
                print(f"[listing] jina failed: {ee} â†’ {url}")
            # ã“ã“ã¾ã§å…¨æ»…ãªã‚‰å†raise
            raise

    for rel_path in paths:
        url = f"{BASE}{rel_path}"
        try:
            html = _fetch_listing_html(url)
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            continue

        soup = BeautifulSoup(html, "html.parser")
        try:
            tit = (soup.title.get_text(strip=True) if soup.title else "").strip()
            low = (soup.get_text(" ", strip=True)[:2000] or "").lower()
            suspicious = any(x in low for x in ("cloudflare", "captcha", "verify you are human", "attention required"))
            dbg(f"[cat] page ready: url={url} bytes={len(html)} title={tit!r} suspicious={suspicious}")
        except Exception:
            pass
        wrapper = soup.select_one("div.jeg_content")  # ãƒ†ãƒ¼ãƒã«ã‚ˆã£ã¦ã¯ç„¡ã„ã“ã¨ã‚‚ã‚ã‚‹

        # âœ… union æ–¹å¼ï¼šwrapper å†…â†’è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒšãƒ¼ã‚¸å…¨ä½“ã®é †ã§æ¢ç´¢
        scopes = ([wrapper] if wrapper else []) + [soup]

        for scope in scopes:
            # ãƒ’ãƒ¼ãƒ­ãƒ¼æ ï¼‹é€šå¸¸ãƒªã‚¹ãƒˆï¼‹æ±ç”¨ãƒ¡ã‚¿ã‚’ä¸€ç™ºã§æ‹¾ã†ï¼ˆæ™‚è¨ˆã‚¢ã‚¤ã‚³ãƒ³æœ‰ç„¡ã«ä¾å­˜ã—ãªã„ï¼‰
            links = scope.select(
                ".jnews_category_hero_container .jeg_meta_date a[href], "
                "div.jeg_postblock_content .jeg_meta_date a[href], "
                ".jeg_post_meta .jeg_meta_date a[href]"
            )
            try:
                dbg(f"[cat] meta-links count={len(links)} @ {url}")
                for a in links[:3]:
                    dbg("   â†’", re.sub(r"\s+", " ", a.get_text(" ", strip=True))[:120], "|", a.get("href"))
            except Exception:
                pass

            # ï¼ˆä»»æ„ï¼‰ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
            # dbg(f"[cat] union-links={len(links)} @ {url}")
            for a in links[:2]:
                _txt = re.sub(r"\s+", " ", a.get_text(" ", strip=True))
                # dbg("   â†’", _txt, "|", a.get("href"))

            found = 0
            for a in links:
                href = a.get("href") or ""
                raw = a.get_text(" ", strip=True)
                try:
                    shown_date = _parse_category_date_text(raw)
                except Exception:
                    try:
                        dbg("[cat] date-parse-fail:", re.sub(r"\s+", " ", raw)[:120])
                    except Exception:
                        pass
                    continue

                # â–¼ ã“ã“ã§ /video ãªã©ã‚’é™¤å¤–
                if _is_excluded_url(href):
                    continue

                if shown_date == date_obj and href and href not in seen_urls:
                    candidate_urls.append(href)
                    seen_urls.add(href)
                    origins[href] = origins.get(href, "cat")
                    found += 1

            # wrapper å†…ã§â€œå½“æ—¥â€ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ soup ã¾ã§åºƒã’ãšçµ‚äº†ã€‚
            # wrapper ãŒç„¡ã„å ´åˆï¼ˆscopes ãŒ [soup] ã ã‘ã®æ™‚ï¼‰ã‚‚1å‘¨ã§æŠœã‘ã‚‹ã€‚
            if found > 0:
                # dbg(f"[cat] STOP (added {found} candidates) @ {url}")
                break

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1: ã‚¿ã‚¤ãƒˆãƒ«è¦‹å‡ºã—ãƒªãƒ³ã‚¯ã‹ã‚‰å€™è£œã‚’è£œå®Œï¼ˆå½“æ—¥åˆ¤å®šã¯è¨˜äº‹å´ã§å®Ÿæ–½ï¼‰
        if found == 0:
            try:
                tlinks = soup.select("h2.jeg_post_title a[href], h3.jeg_post_title a[href]")
            except Exception:
                tlinks = []
            added_titles = 0
            for a in tlinks:
                href = a.get("href") or ""
                if not href:
                    continue
                # çµ¶å¯¾åŒ–ã¨ãƒ•ã‚£ãƒ«ã‚¿
                try:
                    from urllib.parse import urljoin as _ujoin, urlparse as _uparse
                    absu = href if _uparse(href).netloc else _ujoin(BASE + "/", href)
                    host = (_uparse(absu).netloc or "").lower()
                    path = (_uparse(absu).path or "").lower()
                except Exception:
                    absu = href
                    host = ""
                    path = href.lower()
                if "irrawaddy.com" not in host:
                    continue
                if _is_excluded_url(absu):
                    continue
                looks_article = ("/news/" in path) or bool(re.search(r"/20\d{2}/\d{2}/\d{2}/", path))
                if not looks_article:
                    continue
                if absu in seen_urls:
                    continue
                candidate_urls.append(absu)
                seen_urls.add(absu)
                origins[absu] = origins.get(absu, "cat")
                added_titles += 1
                if added_titles >= 15:
                    break
            try:
                dbg(f"[cat] title-links added={added_titles} @ {url}")
            except Exception:
                pass

        # è£œåŠ©: ä¸Šã®ã‚»ãƒ¬ã‚¯ã‚¿ã§å½“æ—¥ãŒæ‹¾ãˆãªã‹ã£ãŸå ´åˆã€ãƒšãƒ¼ã‚¸å†…ã®ã‚¢ãƒ³ã‚«ãƒ¼ã‹ã‚‰æ±ç”¨å€™è£œã‚’è¿½åŠ 
        if found == 0:
            try:
                anchors = soup.select("a[href]")
            except Exception:
                anchors = []
            added = 0
            for a in anchors:
                href = a.get("href") or ""
                if not href:
                    continue
                # çµ¶å¯¾URLã«æ­£è¦åŒ–
                try:
                    from urllib.parse import urljoin as _ujoin, urlparse as _uparse
                    absu = href if _uparse(href).netloc else _ujoin(BASE + "/", href)
                    host = (_uparse(absu).netloc or "").lower()
                    path = (_uparse(absu).path or "").lower()
                except Exception:
                    absu = href
                    host = ""
                    path = href.lower()
                if "irrawaddy.com" not in host:
                    continue
                if _is_excluded_url(absu):
                    continue
                # è¨˜äº‹ã‚‰ã—ã•ï¼ˆ/news/ ã‹ã€YYYY/MM/DD ã‚’å«ã‚€ï¼‰
                looks_article = ("/news/" in path) or bool(re.search(r"/20\d{2}/\d{2}/\d{2}/", path))
                if not looks_article:
                    continue
                if absu in seen_urls:
                    continue
                candidate_urls.append(absu)
                seen_urls.add(absu)
                origins[absu] = origins.get(absu, "cat")
                added += 1
                if added >= 15:  # å®‰å…¨ã®ãŸã‚ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šæœ€å¤§15ä»¶
                    break
            try:
                dbg(f"[cat] fallback anchors added={added} @ {url}")
            except Exception:
                pass

    # ==== 1.5) ãƒ›ãƒ¼ãƒ ï¼ˆkuDRpuoã‚«ãƒ©ãƒ ï¼‰å·¡å› â†’ å½“æ—¥å€™è£œæŠ½å‡ºï¼ˆæ–°è¦ï¼‰ ====
    try:
        home_url = f"{BASE}/"
        html_home = _fetch_listing_html(home_url)
        soup_home = BeautifulSoup(html_home, "html.parser")

        # data-id ã§ã‚¹ã‚³ãƒ¼ãƒ—ç‰¹å®šï¼ˆclass ã§ã‚‚æ‹¾ãˆã‚‹ã‚ˆã†ã«å†—é•·åŒ–ï¼‰
        home_scope = soup_home.select_one(
            'div.elementor-element-kuDRpuo[data-id="kuDRpuo"], '
            "div.elementor-element-kuDRpuo, "
            '[data-id="kuDRpuo"]'
        )

        if home_scope:
            links = home_scope.select(".jeg_meta_date a[href]")
            found_home = 0
            try:
                dbg(f"[home] meta-links count={len(links)} @ {home_url}")
                for a in links[:3]:
                    dbg("   â†’", re.sub(r"\s+", " ", a.get_text(" ", strip=True))[:120], "|", a.get("href"))
            except Exception:
                pass
            for a in links:
                href = a.get("href") or ""
                raw = a.get_text(" ", strip=True)
                try:
                    shown_date = _parse_category_date_text(raw)
                except Exception:
                    try:
                        dbg("[home] date-parse-fail:", re.sub(r"\s+", " ", raw)[:120])
                    except Exception:
                        pass
                    continue

                # â–¼ ã“ã“ã§ã‚‚é™¤å¤–
                if _is_excluded_url(href):
                    continue

                if shown_date == date_obj and href and href not in seen_urls:
                    candidate_urls.append(href)
                    seen_urls.add(href)
                    origins[href] = origins.get(href, "home")
                    found_home += 1

            # è£œåŠ©: å½“æ—¥ãŒæ‹¾ãˆãªã„å ´åˆã¯ãƒ›ãƒ¼ãƒ åˆ—ã‹ã‚‰æ±ç”¨å€™è£œã‚’æ•°ä»¶æ‹¾ã†
            if found_home == 0:
                # ã‚¿ã‚¤ãƒˆãƒ«è¦‹å‡ºã—ãƒªãƒ³ã‚¯å„ªå…ˆ
                try:
                    tlinks = home_scope.select("h2.jeg_post_title a[href], h3.jeg_post_title a[href]")
                except Exception:
                    tlinks = []
                added_titles = 0
                for a in tlinks:
                    href = a.get("href") or ""
                    if not href:
                        continue
                    try:
                        from urllib.parse import urljoin as _ujoin, urlparse as _uparse
                        absu = href if _uparse(href).netloc else _ujoin(BASE + "/", href)
                        host = (_uparse(absu).netloc or "").lower()
                        path = (_uparse(absu).path or "").lower()
                    except Exception:
                        absu = href
                        host = ""
                        path = href.lower()
                    if "irrawaddy.com" not in host:
                        continue
                    if _is_excluded_url(absu):
                        continue
                    looks_article = ("/news/" in path) or bool(re.search(r"/20\d{2}/\d{2}/\d{2}/", path))
                    if not looks_article:
                        continue
                    if absu in seen_urls:
                        continue
                    candidate_urls.append(absu)
                    seen_urls.add(absu)
                    origins[absu] = origins.get(absu, "home")
                    added_titles += 1
                    if added_titles >= 10:
                        break
                try:
                    dbg(f"[home] title-links added={added_titles} @ {home_url}")
                except Exception:
                    pass

            if found_home == 0 and 'added_titles' in locals() and added_titles == 0:
                try:
                    anchors = home_scope.select("a[href]")
                except Exception:
                    anchors = []
                added = 0
                for a in anchors:
                    href = a.get("href") or ""
                    if not href:
                        continue
                    # çµ¶å¯¾URLã«æ­£è¦åŒ–
                    try:
                        from urllib.parse import urljoin as _ujoin, urlparse as _uparse
                        absu = href if _uparse(href).netloc else _ujoin(BASE + "/", href)
                        host = (_uparse(absu).netloc or "").lower()
                        path = (_uparse(absu).path or "").lower()
                    except Exception:
                        absu = href
                        host = ""
                        path = href.lower()
                    if "irrawaddy.com" not in host:
                        continue
                    if _is_excluded_url(absu):
                        continue
                    looks_article = ("/news/" in path) or bool(re.search(r"/20\d{2}/\d{2}/\d{2}/", path))
                    if not looks_article:
                        continue
                    if absu in seen_urls:
                        continue
                    candidate_urls.append(absu)
                    seen_urls.add(absu)
                    origins[absu] = origins.get(absu, "home")
                    added += 1
                    if added >= 10:
                        break
                try:
                    dbg(f"[home] fallback anchors added={added} @ {home_url}")
                except Exception:
                    pass
    except Exception as e:
        print(f"Error scanning homepage column kuDRpuo: {e}")

    # ãƒ­ã‚°ã€å€™è£œURLåé›†ãŒçµ‚ã‚ã£ãŸç›´å¾Œï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‹ãƒ›ãƒ¼ãƒ çµ±åˆã®ã‚ã¨ï¼‰
    dbg(f"[irrawaddy] candidates={len(candidate_urls)} (unique)")

    # ==== 1.9) ãƒ•ã‚£ãƒ¼ãƒ‰/å¤–éƒ¨RSSãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆGitHub Actionsç­‰ã§403ãŒç¶šãå ´åˆï¼‰ ====
    def _mmt_date(dt: datetime) -> date:
        try:
            return dt.astimezone(MMT).date()
        except Exception:
            # naive ã®å ´åˆã¯UTCâ†’MMTæ›ç®—ã¨ã¿ãªã™
            return (dt.replace(tzinfo=timezone.utc)).astimezone(MMT).date()

    def _fetch_text(url: str, timeout: int = 20) -> str:
        try:
            r = requests.get(
                url,
                headers={
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/128.0.0.0 Safari/537.36"
                    )
                },
                timeout=timeout,
            )
            if r.status_code == 200 and (r.text or "").strip():
                return r.text
        except Exception:
            pass
        return ""

    def _fetch_text_via_jina(url: str, timeout: int = 25) -> str:
        try:
            alt = f"https://r.jina.ai/http://{url.lstrip('/')}"
            r = requests.get(
                alt,
                headers={
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/128.0.0.0 Safari/537.36"
                    )
                },
                timeout=timeout,
            )
            if r.status_code == 200 and (r.text or "").strip():
                return r.text
        except Exception:
            pass
        return ""

    def _rss_items_from_google_news() -> List[Dict[str, str]]:
        # Google News RSSï¼ˆIrrawaddyé™å®šï¼‰
        gnews = (
            "https://news.google.com/rss/search?"
            "q=site:www.irrawaddy.com+when:2d&hl=en-US&gl=US&ceid=US:en"
        )
        # RSS/æ©Ÿæ¢°APIã¯é€£çµ¡å…ˆå…¥ã‚ŠUA
        contact = os.getenv("CONTACT_EMAIL") or os.getenv("CONTACT_URL")
        ua_contact = (
            f"MyanmarNewsDigestBot/1.0 (+{contact})" if contact else "MyanmarNewsDigestBot/1.0"
        )
        try:
            r0 = requests.get(
                gnews,
                headers={
                    "User-Agent": ua_contact,
                    "Accept": "application/rss+xml, application/xml;q=0.9, */*;q=0.1",
                },
                timeout=15,
            )
            xml = r0.text if r0.status_code == 200 else ""
        except Exception:
            xml = ""
        if not xml:
            return []
        try:
            root = ET.fromstring(xml)
        except Exception:
            return []
        items = []
        href_re = re.compile(r'href=["\']([^"\']+)["\']', re.I)
        for it in root.findall(".//item"):
            title = (it.findtext("title") or "").strip()
            link = (it.findtext("link") or "").strip()
            pub = (it.findtext("pubDate") or "").strip()
            desc = (it.findtext("description") or "").strip()
            # description å†…ã«è¤‡æ•° href ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‚‹ â†’ è¨˜äº‹ã£ã½ã•ã§é¸ã¶
            direct = None
            try:
                cands = href_re.findall(desc) or []
                cands = [c for c in cands if "www.irrawaddy.com" in c]
                def _score(h: str) -> int:
                    try:
                        from urllib.parse import urlparse as _urlparse
                        path = (_urlparse(h).path or "").lower()
                    except Exception:
                        path = h.lower()
                    s = 0
                    if "/news/" in path:
                        s += 5
                    if re.search(r"/20\d{2}/\d{2}/\d{2}/", path):
                        s += 3
                    if path.endswith(".html"):
                        s += 2
                    if any(x in path for x in ("/tag/", "/category/", "/author/", "/search/")):
                        s -= 5
                    return s
                if cands:
                    direct = max(cands, key=_score)
            except Exception:
                direct = None
            items.append({
                "title": title,
                "link": direct or link,
                "pubDate": pub,
            })
        return items

    def _parse_rfc822_date(s: str) -> Optional[datetime]:
        s = (s or "").strip()
        if not s:
            return None
        try:
            return parse_date(s)
        except Exception:
            return None

    def _fallback_candidates_via_feeds() -> List[Dict[str, str]]:
        # 1) WordPress JSONï¼ˆå¤šãã®å ´åˆWAFå¯¾è±¡ï¼‰
        wp_url = "https://www.irrawaddy.com/wp-json/wp/v2/posts?per_page=50&_fields=link,date,title"
        # API/JSONã¯é€£çµ¡å…ˆå…¥ã‚ŠUA
        contact = os.getenv("CONTACT_EMAIL") or os.getenv("CONTACT_URL")
        ua_contact = (
            f"MyanmarNewsDigestBot/1.0 (+{contact})" if contact else "MyanmarNewsDigestBot/1.0"
        )
        try:
            r_wp = requests.get(
                wp_url,
                headers={
                    "User-Agent": ua_contact,
                    "Accept": "application/json, */*;q=0.1",
                },
                timeout=20,
            )
            wp_json = r_wp.text if r_wp.status_code == 200 else ""
        except Exception:
            wp_json = ""
        if not wp_json:
            wp_json = _fetch_text_via_jina(wp_url)
        cands: List[Dict[str, str]] = []
        if wp_json:
            try:
                arr = json.loads(wp_json)
                for o in arr:
                    link = (o.get("link") or "").strip()
                    ds = o.get("date") or ""
                    # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆWPã¯ title.rendered ã®ã“ã¨ãŒå¤šã„ï¼‰
                    t = o.get("title")
                    if isinstance(t, dict):
                        tt = (t.get("rendered") or "").strip()
                    else:
                        tt = (t or "").strip()
                    dt = _parse_rfc822_date(ds) or (
                        parse_date(ds) if ds else None
                    )
                    if link and dt and _mmt_date(dt) == date_obj and not _is_excluded_url(link):
                        cands.append({"url": link, "title": tt, "date": date_obj.isoformat()})
            except Exception:
                pass

        # 2) ã‚µã‚¤ãƒˆRSSï¼ˆ403ã®å¯èƒ½æ€§ã‚ã‚Š â†’ å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if not cands:
            feed_url = "https://www.irrawaddy.com/feed"
            # RSSã‚‚é€£çµ¡å…ˆå…¥ã‚ŠUA
            try:
                r_feed = requests.get(
                    feed_url,
                    headers={
                        "User-Agent": ua_contact,
                        "Accept": "application/rss+xml, application/xml;q=0.9, */*;q=0.1",
                    },
                    timeout=20,
                )
                feed_xml = r_feed.text if r_feed.status_code == 200 else ""
            except Exception:
                feed_xml = ""
            if not feed_xml:
                feed_xml = _fetch_text_via_jina(feed_url)
            if feed_xml:
                try:
                    root = ET.fromstring(feed_xml)
                    for it in root.findall(".//item"):
                        title = (it.findtext("title") or "").strip()
                        link = (it.findtext("link") or "").strip()
                        pub = (it.findtext("pubDate") or "").strip()
                        dt = _parse_rfc822_date(pub)
                        if link and dt and _mmt_date(dt) == date_obj and not _is_excluded_url(link):
                            cands.append({
                                "url": link,
                                "title": title,
                                "date": date_obj.isoformat(),
                            })
                except Exception:
                    pass

        # 3) Google News RSSï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not cands:
            # Google News RSS ã‚‚é€šå¸¸å–å¾—â†’ãƒ€ãƒ¡ãªã‚‰ Jina çµŒç”±ã§è©¦ã™
            items = _rss_items_from_google_news()
            if not items:
                gnews = (
                    "https://news.google.com/rss/search?q=site:www.irrawaddy.com+when:2d&hl=en-US&gl=US&ceid=US:en"
                )
                xml = _fetch_text_via_jina(gnews)
                if xml:
                    try:
                        root = ET.fromstring(xml)
                        href_re = re.compile(r'href=["\']([^"\']+)["\']', re.I)
                        items = []
                        for it in root.findall(".//item"):
                            title = (it.findtext("title") or "").strip()
                            link = (it.findtext("link") or "").strip()
                            pub = (it.findtext("pubDate") or "").strip()
                            desc = (it.findtext("description") or "").strip()
                            direct = None
                            m = href_re.search(desc)
                            if m and "www.irrawaddy.com" in m.group(1):
                                direct = m.group(1)
                            items.append({
                                "title": title,
                                "link": direct or link,
                                "pubDate": pub,
                            })
                    except Exception:
                        items = []

            for it in items:
                link = resolve_gnews_url(it.get("link") or "")
                title = it.get("title") or ""
                pub = it.get("pubDate") or ""
                dt = _parse_rfc822_date(pub)
                if not link or _is_excluded_url(link):
                    continue
                if dt and _mmt_date(dt) == date_obj:
                    cands.append({
                        "url": link,
                        "title": title,
                        "date": date_obj.isoformat(),
                    })

        # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
        seen = set()
        uniq = []
        for o in cands:
            u = o["url"].strip()
            if u and u not in seen:
                seen.add(u)
                uniq.append(o)
        return uniq

    if len(candidate_urls) == 0:
        dbg("[irrawaddy] fallback to RSS/Google News due to 0 candidates")
        feed_cands = _fallback_candidates_via_feeds()
        dbg(f"[irrawaddy] feed candidates={len(feed_cands)}")
        # æ—¢å­˜ãƒ‘ã‚¹ã¨åŒã˜å½¢å¼ã«åˆã‚ã›ã‚‹
        for o in feed_cands:
            u = o.get("url") or ""
            if u and u not in seen_urls:
                candidate_urls.append(u)
                seen_urls.add(u)
                origins[u] = "feed"
                t_fb = o.get("title") or ""
                d_fb = o.get("date") or ""
                if t_fb:
                    fallback_titles[u] = t_fb
                if t_fb or d_fb:
                    feed_hints[u] = {"title": t_fb, "date": d_fb}

    # ==== 2) å€™è£œè¨˜äº‹ã§å³å¯†ç¢ºèªï¼ˆmetaæ—¥ä»˜/æœ¬æ–‡/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰ ====
    for url in candidate_urls:
        # â–¼ å‡¦ç†ä»¶æ•°ä¸Šé™ï¼ˆå®‰å…¨ã‚¬ãƒ¼ãƒ‰ï¼‰
        max_per_run = int(os.getenv("IRRAWADDY_MAX_PAGES_PER_RUN", "20"))
        processed = locals().get("_irrawaddy_processed", 0)
        if processed >= max_per_run:
            print(f"[irrawaddy] reached per-run limit: {max_per_run}, stop.")
            break

        # å‰å›ã¾ã§ã«å–ã‚Šè¾¼ã¿æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        # äº‹å‰ã« Google News ã®ä¸­é–“URLã‚’ç›´ãƒªãƒ³ã‚¯åŒ–ã—ã€ã‚¿ã‚°ç€åœ°ã‚‚è¨˜äº‹ã¸å¯„ã›ã‚‹
        try:
            if "news.google.com" in (url or ""):
                old = url
                new = resolve_gnews_url(url)
                if new and new != url:
                    if old in fallback_titles and new not in fallback_titles:
                        fallback_titles[new] = fallback_titles.get(old, "")
                    url = new
            if is_irrawaddy_tag_url(url):
                tnew = pick_article_from_irrawaddy_tag(url, date_obj)
                if tnew and tnew != url:
                    if url in fallback_titles and tnew not in fallback_titles:
                        fallback_titles[tnew] = fallback_titles.get(url, "")
                    url = tnew
        except Exception:
            pass

        # ç›´ãƒªãƒ³ã‚¯åŒ–å¾Œã®URLã§æ—¢èª­åˆ¤å®š
        normu = _norm_id(url)
        if normu in seen_persist:
            continue

        # Resilient + Playwright ã‚’å„ªå…ˆï¼ˆå¤±æ•—æ™‚ã¯æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if USE_RESILIENT_FIRST:
            try:
                html = fetch_irrawaddy_resilient(url, session=session)
                soup_article = BeautifulSoup(html, "html.parser")
                try:
                    host = urlparse(url).netloc.lower()
                except Exception:
                    host = ""
                meta_date = _article_date_from_meta_mmt(soup_article) if "irrawaddy.com" in host else None
                if (meta_date is not None) and (meta_date != date_obj):
                    hint = feed_hints.get(url)
                    if hint and (hint.get("date") == date_obj.isoformat()):
                        title_fb = (hint.get("title") or "").strip() or _title_from_slug(url)
                        results.append({
                            "source": "Irrawaddy",
                            "title": unicodedata.normalize("NFC", title_fb),
                            "url": url,
                            "date": date_obj.isoformat(),
                            "body": "",
                        })
                        seen_persist.add(normu)
                        _irrawaddy_processed = processed + 1
                        continue
                    if origins.get(url) in ("cat", "home"):
                        title_fb = (feed_hints.get(url, {}).get("title") or "").strip() or _title_from_slug(url)
                        results.append({
                            "source": "Irrawaddy",
                            "title": unicodedata.normalize("NFC", title_fb),
                            "url": url,
                            "date": date_obj.isoformat(),
                            "body": "",
                        })
                        seen_persist.add(normu)
                        _irrawaddy_processed = processed + 1
                        continue

                title = _extract_title(soup_article) or ""
                body = extract_body_irrawaddy(soup_article) or extract_body_generic_from_soup(soup_article) or ""
                title_nfc = unicodedata.normalize("NFC", title)
                body_nfc = unicodedata.normalize("NFC", body)
                results.append({
                    "url": url,
                    "title": title_nfc,
                    "date": date_obj.isoformat(),
                    "body": body_nfc,
                    "source": "Irrawaddy",
                })
                seen_persist.add(normu)
                _irrawaddy_processed = processed + 1
                continue
            except Exception as e:
                print(f"[irrawaddy] resilient-first failed: {e} â†’ {url}")


        # Google News ç”±æ¥ã® tag ãƒšãƒ¼ã‚¸ã«ç€åœ°ã—ãŸå ´åˆã¯è¨˜äº‹URLã¸ç½®æ›ã‚’è©¦ã¿ã‚‹
        if is_irrawaddy_tag_url(url):
            newu = pick_article_from_irrawaddy_tag(url, date_obj)
            if newu and newu != url:
                # ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚ã‚­ãƒ¼å¼•ãç¶™ã
                if url in fallback_titles and newu not in fallback_titles:
                    fallback_titles[newu] = fallback_titles.get(url, "")
                url = newu
        # www.irrawaddy.com å›ºå®šï¼ˆä»–ã®ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        try:
            host_now = urlparse(url).netloc.lower()
        except Exception:
            host_now = ""
        if host_now != "www.irrawaddy.com":
            continue
        if _is_excluded_url(url):  # ãƒ™ãƒ«ãƒˆï¼†ã‚µã‚¹ãƒšãƒ³ãƒ€ãƒ¼
            continue
        try:
            title = ""
            body = ""
            # Google News çµŒç”±URLã¯å…ˆã«ç›´ãƒªãƒ³ã‚¯è§£æ±ºã‚’è©¦ã¿ã‚‹
            if "news.google.com" in (url or ""):
                old = url
                new = resolve_gnews_url(url)
                if new and new != url:
                    # feed ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚ã‚­ãƒ¼ã‚’å¼•ãç¶™ã
                    if old in fallback_titles and new not in fallback_titles:
                        fallback_titles[new] = fallback_titles.get(old, "")
                    url = new
                # è§£æ±ºå…ˆãŒã‚¿ã‚°ãƒšãƒ¼ã‚¸ãªã‚‰ã€ã“ã“ã§è¨˜äº‹URLã¸ç½®æ›ã‚’è©¦ã¿ã‚‹
                if is_irrawaddy_tag_url(url):
                    tnew = pick_article_from_irrawaddy_tag(url, date_obj)
                    if tnew and tnew != url:
                        if url in fallback_titles and tnew not in fallback_titles:
                            fallback_titles[tnew] = fallback_titles.get(url, "")
                        url = tnew
            # â‘  ç›´æ¥å–å¾—ï¼ˆ1å›ã ã‘ï¼‰
            try:
                html_once = _fetch_text(url, timeout=20)
                if html_once:
                    soup_article = BeautifulSoup(html_once, "html.parser")
                    # Irrawaddy ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¨ãã ã‘ã€å¯èƒ½ãªã‚‰ meta æ—¥ä»˜ã‚’ç…§åˆ
                    try:
                        host = urlparse(url).netloc.lower()
                    except Exception:
                        host = ""
                    meta_date = None
                    if "irrawaddy.com" in host:
                        meta_date = _article_date_from_meta_mmt(soup_article)
                    # ã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡æŠ½å‡ºã¯ç¶šã‘ã‚‹
                    title = _extract_title(soup_article) or ""
                    body = extract_body_irrawaddy(soup_article) or ""
                    # ãƒ¡ã‚¿æ—¥ä»˜ä¸ä¸€è‡´æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¡ç”¨ï¼ˆcollect_irrawaddy_all_for_date ã¨åŒç­‰ï¼‰
                    if (meta_date is not None) and (meta_date != date_obj):
                        hint = feed_hints.get(url)
                        if hint and (hint.get("date") == date_obj.isoformat()):
                            title_fb = (hint.get("title") or "").strip()
                            if title_fb:
                                results.append(
                                    {
                                        "source": "Irrawaddy",
                                        "title": unicodedata.normalize("NFC", title_fb),
                                        "url": url,
                                        "date": date_obj.isoformat(),
                                        "body": "",
                                    }
                                )
                                continue
                        if origins.get(url) in ("cat", "home"):
                            # ä¸€è¦§ã®å½“æ—¥åˆ¤å®šã‚’ä¿¡é ¼ã—ã¦æ¡ç”¨ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã¯ãƒ’ãƒ³ãƒˆ/ã‚¹ãƒ©ãƒƒã‚°ã§è£œå®Œï¼‰
                            title_fb = (feed_hints.get(url, {}).get("title") or "").strip()
                            if not title_fb:
                                title_fb = _title_from_slug(url)
                            results.append(
                                {
                                    "source": "Irrawaddy",
                                    "title": unicodedata.normalize("NFC", title_fb),
                                    "url": url,
                                    "date": date_obj.isoformat(),
                                    "body": "",
                                }
                            )
                            continue
                        # ãã‚Œä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
                        continue
            except Exception:
                pass

            # â‘¡ ç›´æ¥å–å¾—ã§ããªã„å ´åˆã€r.jina.ai çµŒç”±ã®æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not body:
                def _jina_fetch(u: str) -> str:
                    # r.jina.ai ã¯ Readability æŠ½å‡ºã—ãŸãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
                    alt = f"https://r.jina.ai/http://{u.lstrip('/') }"
                    t = _fetch_text(alt, timeout=25)
                    if not t and "/news/" in u:
                        # AMP ã‚’è©¦ã™
                        amp = u if u.endswith("/amp") else urljoin(u.rstrip("/") + "/", "amp")
                        alt2 = f"https://r.jina.ai/http://{amp.lstrip('/') }"
                        t = _fetch_text(alt2, timeout=25)
                    return t

                body_txt = _jina_fetch(url)
                if body_txt:
                    body = body_txt
                    if not title:
                        # ãƒ•ã‚£ãƒ¼ãƒ‰ã§æ‹¾ã£ãŸã‚¿ã‚¤ãƒˆãƒ«ã‚’æœ€çµ‚æ‰‹æ®µã¨ã—ã¦æµç”¨
                        title = fallback_titles.get(url, "")

            # â‘¢ ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºã€ã¾ãŸã¯ news.google.com ç”±æ¥ã®æ±ç”¨ã‚¿ã‚¤ãƒˆãƒ«ãªã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¡ç”¨
            if (not title) or (title.strip().lower() == "google news"):
                # ãƒ•ã‚£ãƒ¼ãƒ‰ã§æ‹¾ã£ãŸã‚¿ã‚¤ãƒˆãƒ«ã‚’æœ€å„ªå…ˆ
                if not title:
                    t_fb = fallback_titles.get(url, "")
                else:
                    t_fb = fallback_titles.get(url, "") or ""
                if t_fb:
                    title = t_fb
            # â‘¢' ãã‚Œã§ã‚‚ç©ºãªã‚‰ã€oEmbed ã¾ãŸã¯ã‚¹ãƒ©ãƒƒã‚°ã‹ã‚‰è£œå®Œ
            if not title:
                if "irrawaddy.com" in (url or ""):
                    t2 = _oembed_title_irrawaddy(url)
                    if t2:
                        title = t2
                if not title:
                    title = _title_from_slug(url)

            # Irrawaddy ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è½ã¨ã•ãªã„ï¼ˆå¾Œæ®µã® bypass_keyword ã¨æ•´åˆï¼‰
            title_nfc = unicodedata.normalize("NFC", title)
            body_nfc = unicodedata.normalize("NFC", body)

            results.append(
                {
                    "url": url,
                    "title": title_nfc,
                    "date": date_obj.isoformat(),
                    "body": body_nfc,
                    "source": "Irrawaddy",
                }
            )
        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    # ==== 3) æœ€çµ‚é‡è¤‡æ’é™¤ï¼ˆURLã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ãƒ»å…ˆå‹ã¡ï¼‰ ====
    before_dedup = len(results)
    results = deduplicate_by_url(results)

    # ãƒ­ã‚°ã€é‡è¤‡å‰Šé™¤ä»¶æ•°
    dbg(f"[irrawaddy] dedup: {before_dedup} -> {len(results)}")

    # ãƒ­ã‚°ã€æœ€çµ‚çš„ãªresultã®ä¸­èº«
    dbg(f"[irrawaddy] kept={len(results)}")

    def _one(s: str, n: int = 60) -> str:
        s = re.sub(r"\s+", " ", (s or "")).strip()
        return s[:n]

    for r in results[:3]:
        dbg(f"  - {_one(r.get('title'))} | {r.get('url')}")
    if len(results) > 3:
        dbg(f"  ... (+{len(results)-3} more)")

    _save_seen_set(seen_persist)

    return results


# DVB
def get_dvb_articles_for(date_obj: date, debug: bool = True) -> List[Dict]:
    """
    - /category/... ã®ä¸€è¦§ï¼ˆ1ãƒšãƒ¼ã‚¸ç›®ï¼‹?page=2ï¼‰ã‹ã‚‰ã€æŒ‡å®šæ—¥ã¨ä¸€è‡´ã™ã‚‹ã‚«ãƒ¼ãƒ‰ã ã‘å€™è£œåŒ–ã€‚
    - è¨˜äº‹ãƒšãƒ¼ã‚¸ã§ã¯ <title> / .full_content p ã‚’æŠ½å‡ºã€‚
    - ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ã‚’NFCæ­£è¦åŒ–ã—ã¦ any_keyword_hit ã§ãƒ•ã‚£ãƒ«ã‚¿ã€‚
    - è¿”ã‚Šå€¤: [{url, title, date, body, source}]
    â€» DVBå°‚ç”¨ fetch_with_retry_dvb ã‚’ä½¿ç”¨ã€‚
    ä»¥ä¸‹3ã‚«ãƒ†ã‚´ãƒªä»¥å¤–ã®è¨˜äº‹ã¯ã€ã™ã¹ã¦/category/8/newsã«å«ã¾ã‚Œã¦ã„ã‚‹ã€‚
    - /category/1799/international-news
    - /category/1793/sports-news
    - /category/6/features
    å½“è©²3ã‚«ãƒ†ã‚´ãƒªã¯é™¤å¤–ã—ãŸã„ã‚°ãƒ«ãƒ¼ãƒ—ã«ãªã‚‹ã®ã§/category/8/newsã®ã¿ã‚’å–å¾—å¯¾è±¡ã¨ã™ã‚‹ã€‚
    """
    BASE = "https://burmese.dvb.no"
    CATEGORY_PATHS = [
        "/category/8/news",
        # "/category/17/news_politics-new",
        # "/category/16/news_economics-new",
        # "/category/15/news_health-news-news",
        # "/category/18/news_social-news",
        # "/category/1787/news_education-news",
        # "/category/10/news_environment-weather",
        # "/category/1789/news_labour-news",
        # "/category/1788/news_farmers-news",
        # "/category/1797/news_criminals-news",
        # "/category/9/news_media-news",
        # "/category/6/features",
        # "/category/13/interview",
        # "/category/1799/international-news",
        # "/category/1793/sports-news",
    ]

    def _norm_path(p: str) -> str:
        return re.sub(r"/{2,}", "/", (p or "").strip())

    def _parse_dvb_date(text: str) -> Optional[date]:
        if not text:
            return None
        s = re.sub(r"\s+", " ", text.strip())
        try:
            return datetime.strptime(s, "%B %d, %Y").date()
        except ValueError:
            return None

    def _extract_title_dvb(soup: BeautifulSoup) -> str:
        t = (soup.title.string or "").strip() if soup.title else ""
        if t:
            return t
        h = soup.select_one(".text-2xl, h1, .post-title")
        return (h.get_text(" ", strip=True) if h else "").strip()

    def _extract_body_dvb(soup: BeautifulSoup) -> str:
        host = soup.select_one(".full_content")
        if not host:
            return ""
        parts = []
        for p in host.select("p"):
            txt = p.get_text(" ", strip=True)
            txt = re.sub(r"\s+", " ", txt)
            if txt:
                parts.append(txt)
        return "\n".join(parts).strip()

    log = (lambda *a, **k: print(*a, **k)) if debug else (lambda *a, **k: None)
    results: List[Dict] = []
    candidate_urls: List[str] = []
    seen_urls = set()

    # å…±æœ‰ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆcookies/æŒ‡ç´‹ã‚’ä¸€è¦§â†’è¨˜äº‹ã§å¼•ãç¶™ãï¼‰
    try:
        sess = requests.Session()
    except Exception:
        sess = None

    # ---- 1) ã‚«ãƒ†ã‚´ãƒªä¸€è¦§å·¡å›ï¼ˆå„ã‚«ãƒ†ã‚´ãƒªã«ã¤ã page=1,2ï¼‰
    for rel in CATEGORY_PATHS:
        rel = _norm_path(rel)
        for page_no in (1, 2):
            url = f"{BASE}{rel}" if page_no == 1 else f"{BASE}{rel}?page=2"
            try:
                res = fetch_with_retry_dvb(url, retries=4, wait_seconds=2, session=sess)
            except Exception as e:
                log(f"[warn] fetch fail {url}: {e}")
                continue

            if getattr(res, "status_code", 200) != 200:
                log(f"[skip] non-200 ({res.status_code}) {url}")
                continue

            soup = BeautifulSoup(
                getattr(res, "content", None) or res.text, "html.parser"
            )

            # ä¸€è¦§ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆç‰¹å¾´ã§ç‰¹å®šã€‚ç„¡ã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ãƒšãƒ¼ã‚¸å…¨ä½“ï¼‰
            blocks = soup.select(
                "div.md\\:grid.grid-cols-3.gap-4.mt-5, div.grid.grid-cols-3.gap-4.mt-5"
            ) or [soup]

            found = 0
            for scope in blocks:
                anchors = scope.select('a[href^="/post/"]')
                for a in anchors:
                    href = a.get("href") or ""
                    # ç¬¬ä¸€å€™è£œï¼šã‚«ãƒ¼ãƒ‰å†…ã® date ãƒ–ãƒ­ãƒƒã‚¯
                    date_div = a.select_one(
                        "div.flex.gap-1.text-xs.mt-2.text-gray-500 div"
                    )
                    date_text = (
                        date_div.get_text(" ", strip=True) if date_div else ""
                    ).strip()
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šè‹±èªæœˆåãƒ‘ã‚¿ãƒ¼ãƒ³
                    if not date_text:
                        full = a.get_text(" ", strip=True)
                        m = re.search(
                            r"(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s*\d{4}",
                            full,
                        )
                        date_text = m.group(0) if m else ""
                    d = _parse_dvb_date(date_text)
                    if d and d == date_obj:
                        uabs = href if href.startswith("http") else f"{BASE}{href}"
                        if uabs not in seen_urls:
                            candidate_urls.append(uabs)
                            seen_urls.add(uabs)
                            found += 1
            log(f"[list] {url} -> candidates+{found}")

    log(f"[dvb] candidates total = {len(candidate_urls)} (unique)")

    # ---- 2) å€™è£œè¨˜äº‹ãƒšãƒ¼ã‚¸ã§æŠ½å‡ºï¼ˆany_keyword_hit ã§çµã‚Šè¾¼ã¿ï¼‰
    for url in candidate_urls:
        try:
            res = fetch_with_retry_dvb(url, retries=4, wait_seconds=2, session=sess)
            if getattr(res, "status_code", 200) != 200:
                log(f"[skip] non-200 article {res.status_code} {url}")
                continue
            soup = BeautifulSoup(
                getattr(res, "content", None) or res.text, "html.parser"
            )

            title = _extract_title_dvb(soup)
            body = _extract_body_dvb(soup)
            if not title or not body:
                log(f"[skip] empty title/body {url}")
                continue

            title_nfc = unicodedata.normalize("NFC", title)
            body_nfc = unicodedata.normalize("NFC", body)
            if not any_keyword_hit(title_nfc, body_nfc):
                log_no_keyword_hit("DVB", url, title_nfc, body_nfc, "dvb:article")
                continue

            results.append(
                {
                    "url": url,
                    "title": title_nfc,
                    "date": date_obj.isoformat(),
                    "body": body_nfc,
                    "source": "dvb",
                }
            )
        except Exception as e:
            log(f"[warn] article fail {url}: {e}")
            continue

    # ---- 3) é‡è¤‡æ’é™¤
    before = len(results)
    results = deduplicate_by_url(results)
    log(f"[dvb] dedup: {before} -> {len(results)}")

    # ---- 4) ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºï¼ˆå…ˆé ­æ•°ä»¶ï¼‰
    def _one(s: str, n: int = 60) -> str:
        return re.sub(r"\s+", " ", (s or "").strip())[:n]

    for r in results[:3]:
        log(f"  - {_one(r.get('title'))} | {r.get('url')}")
    if len(results) > 3:
        log(f"  ... (+{len(results)-3} more)")

    _save_seen_set(seen_persist)

    return results

# Myanmar Now (mm) 
def get_myanmar_now_articles_mm(date_obj, max_pages=3):
    """
    Myanmar Now (mm) ã®å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰å¯¾è±¡æ—¥ã®è¨˜äº‹ã‚’å–å¾—ã—ã¦è¿”ã™ã€‚
    - ã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã‚’æœ€å¤§ max_pages ãƒšãƒ¼ã‚¸å·¡å›
    - ä¸€è¦§ã§ã¯ <span class="date meta-item tie-icon">Month D, YYYY</span> ã‚’è¦‹ã¦ä»Šæ—¥ã ã‘æŠ½å‡º
    - å€‹åˆ¥è¨˜äº‹ã§ã¯ <meta property="article:published_time" content="..."> ã‚’UTCâ†’MMTå¤‰æ›ã—ã¦æœ€çµ‚ç¢ºèª
    - ã‚¿ã‚¤ãƒˆãƒ«æœ«å°¾ã® " - Myanmar Now" ã‚’é™¤å»
    - æœ¬æ–‡ã¯ div.entry-content.entry.clearfix å†…ã® <p> ã ã‘ï¼ˆç”»åƒç­‰ã¯å«ã‚ãªã„ï¼‰
    è¿”ã‚Šå€¤: list[dict] {url, title, date(ISO str, MMT), body, source="Myanmar Now (mm)"}
    """

    BASE_CATEGORIES = [
        "https://myanmar-now.org/mm/news/category/news/",                 # ãƒ‹ãƒ¥ãƒ¼ã‚¹
        "https://myanmar-now.org/mm/news/category/news/3/",               # æ”¿æ²»
        "https://myanmar-now.org/mm/news/category/news/17/",              # çµŒæ¸ˆ
        "https://myanmar-now.org/mm/news/category/news/social-issue/",    # ç¤¾ä¼š
        "https://myanmar-now.org/mm/news/category/news/19/",              # æ•™è‚²
        "https://myanmar-now.org/mm/news/category/news/international-news/",  # å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹
        "https://myanmar-now.org/mm/news/category/multimedia/16/",        # å¥åº·
        "https://myanmar-now.org/mm/news/category/in-depth/",             # ç‰¹é›†è¨˜äº‹
        "https://myanmar-now.org/mm/news/category/in-depth/analysis/",    # åˆ†æ
        "https://myanmar-now.org/mm/news/category/in-depth/investigation/", # èª¿æŸ»å ±é“
        "https://myanmar-now.org/mm/news/category/in-depth/profile/",     # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«
        "https://myanmar-now.org/mm/news/category/in-depth/society/",     # ç¤¾ä¼šåˆ†é‡
        "https://myanmar-now.org/mm/news/category/opinion/",              # è«–èª¬
        "https://myanmar-now.org/mm/news/category/opinion/commentary/",   # è«–è©•
        "https://myanmar-now.org/mm/news/category/opinion/29/",           # ç·¨é›†é•·ã®è«–èª¬
        "https://myanmar-now.org/mm/news/category/opinion/interview/",    # ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
        # "https://myanmar-now.org/mm/news/category/opinion/essay/",        # ã‚¨ãƒƒã‚»ã‚¤
        # "https://myanmar-now.org/mm/news/category/opinion/26/",           # é¢¨åˆº
        # "https://myanmar-now.org/mm/news/category/multimedia/video/",     # å‹•ç”»ãƒ‹ãƒ¥ãƒ¼ã‚¹
        # "https://myanmar-now.org/mm/news/category/multimedia/13/",        # ãƒ•ã‚©ãƒˆã‚¨ãƒƒã‚»ã‚¤
    ]

    # "September 8, 2025" ã®ã‚ˆã†ãªè‹±èªè¡¨è¨˜
    today_label = f"{date_obj.strftime('%B')} {date_obj.day}, {date_obj.year}"

    def _strip_source_suffix(title: str) -> str:
        if not title:
            return title
        return re.sub(r"\s*-\s*Myanmar Now\s*$", "", title).strip()

    def _collect_article_urls_from_category(cat_url: str) -> set[str]:
        urls = set()
        for page in range(1, max_pages + 1):
            url = f"{cat_url}page/{page}/" if page > 1 else cat_url
            try:
                res = fetch_with_retry(url)
            except Exception:
                break
            soup = BeautifulSoup(res.content, "html.parser")

            for span in soup.select("span.date.meta-item.tie-icon"):
                if (span.get_text(strip=True) or "") != today_label:
                    continue
                a = span.find_parent("a", href=True)
                if not a:
                    parent_a = span
                    while parent_a and parent_a.name != "a":
                        parent_a = parent_a.parent
                    if parent_a and parent_a.name == "a" and parent_a.get("href"):
                        a = parent_a
                if a and a.get("href"):
                    href = a["href"]
                    if "/mm/news/" in href:
                        urls.add(href)
        return urls

    collected = set()
    for base in BASE_CATEGORIES:
        collected |= _collect_article_urls_from_category(base)

    results = []
    for url in collected:
        try:
            # --- helpers ---
            def _fetch_text(u: str, timeout: int = 20) -> str:
                try:
                    r = requests.get(u, headers={"User-Agent": "Mozilla/5.0"}, timeout=timeout)
                    if r.status_code == 200 and (r.text or "").strip():
                        return r.text
                except Exception:
                    pass
                return ""

            def _fetch_text_via_jina(u: str, timeout: int = 25) -> str:
                try:
                    alt = f"https://r.jina.ai/http://{u.lstrip('/')}"
                    r = requests.get(alt, headers={"User-Agent": "Mozilla/5.0"}, timeout=timeout)
                    if r.status_code == 200 and (r.text or "").strip():
                        return r.text
                except Exception:
                    pass
                return ""

            def _oembed_title(u: str) -> str:
                try:
                    api = (
                        "https://myanmar-now.org/wp-json/oembed/1.0/embed?url="
                        + requests.utils.requote_uri(u)
                    )
                    r = requests.get(api, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
                    if r.status_code == 200 and (r.text or "").strip():
                        data = r.json()
                        return unicodedata.normalize("NFC", (data.get("title") or "").strip())
                except Exception:
                    pass
                return ""

            def _title_from_slug(u: str) -> str:
                try:
                    from urllib.parse import urlparse, unquote
                    seg = urlparse(u).path.rstrip("/").split("/")[-1]
                    seg = unquote(seg).replace("-", " ")
                    return unicodedata.normalize("NFC", seg)
                except Exception:
                    return ""

            # --- fetch once ---
            soup = None
            dt_mmt = None
            try:
                res = fetch_with_retry(url)
                soup = BeautifulSoup(res.content, "html.parser")
                meta = soup.find("meta", attrs={"property": "article:published_time"})
                if meta and meta.get("content"):
                    try:
                        dt_utc = datetime.fromisoformat(meta["content"])
                    except Exception:
                        dt_utc = parse_date(meta["content"]).astimezone(timezone.utc)
                    dt_mmt = dt_utc.astimezone(MMT)
            except Exception:
                soup = None
            meta_ok = dt_mmt is not None and dt_mmt.date() == date_obj

            # --- title ---
            title = ""
            if soup is not None:
                title_raw = (soup.title.get_text(strip=True) if soup.title else "").strip()
                title = _strip_source_suffix(unicodedata.normalize("NFC", title_raw))
                if not title:
                    h1 = soup.find("h1")
                    if h1:
                        title = _strip_source_suffix(
                            unicodedata.normalize("NFC", h1.get_text(strip=True))
                        )
            if not title:
                title = _oembed_title(url) or _title_from_slug(url)
                title = _strip_source_suffix(title)
            if not title:
                continue

            # --- body ---
            body = ""
            if soup is not None:
                content_root = soup.select_one("div.entry-content.entry.clearfix") or soup
                parts = []
                for p in content_root.find_all("p"):
                    txt = p.get_text(strip=True)
                    if txt:
                        parts.append(txt)
                body = unicodedata.normalize("NFC", "\n".join(parts).strip())
                if not body:
                    paragraphs = extract_paragraphs_with_wait(soup)
                    body = unicodedata.normalize(
                        "NFC",
                        "\n".join(
                            p.get_text(strip=True)
                            for p in paragraphs
                            if getattr(p, "get_text", None)
                        ),
                    ).strip()
            if not body:
                body = _fetch_text_via_jina(url) or _fetch_text(url)
            if not body:
                continue

            # 4) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
            if not any_keyword_hit(title, body):
                log_no_keyword_hit("Myanmar Now (mm)", url, title, body, "mnw:article")
                continue

            results.append({
                "url": url,
                "title": title,
                "date": (dt_mmt.isoformat() if meta_ok else date_obj.isoformat()),
                "body": body,
                "source": "Myanmar Now (mm)",
            })
        except Exception as e:
            print(f"[warn] Myanmar Now article fetch failed: {url} ({e})")
            continue

    before = len(results)
    results = deduplicate_by_url(results)
    print(f"[myanmar-now-mm] dedup: {before} -> {len(results)}")
    _save_seen_set(seen_persist)

    return results

# åŒã˜URLã®é‡è¤‡å‰Šé™¤
def deduplicate_by_url(articles):
    seen_urls = set()
    unique_articles = []
    for art in articles:
        if art["url"] in seen_urls:
            print(
                f"ğŸ›‘ URL Duplicate Removed: {art['source']} | {art['title']} | {art['url']}"
            )
            continue
        seen_urls.add(art["url"])
        unique_articles.append(art)
    return unique_articles


# ç¿»è¨³å¯¾è±¡ã‚­ãƒ¥ãƒ¼
translation_queue = []


def process_and_enqueue_articles(
    articles,
    source_name,
    seen_urls=None,
    bypass_keyword=False,
    trust_existing_body=False,
):
    if seen_urls is None:
        seen_urls = set()

    queued_items = []
    for art in articles:
        if art["url"] in seen_urls:
            continue
        seen_urls.add(art["url"])

        try:
            # â‘  ã¾ãšã¯è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«æœ¬æ–‡ãŒæ¥ã¦ã„ãŸã‚‰ãã‚Œã‚’ä½¿ã†
            body_text = (art.get("body") or "").strip() if trust_existing_body else ""

            # â‘¡ ç„¡ã‘ã‚Œã°ãƒ•ã‚§ãƒƒãƒï¼ˆå†…éƒ¨ã§å†ãƒ•ã‚§ãƒƒãƒä»˜ããƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ï¼‰
            if not body_text:
                if source_name == "Irrawaddy" or "irrawaddy.com" in art["url"]:
                    body_text = get_body_with_refetch(
                        art["url"],
                        fetcher=lambda u: fetch_once_irrawaddy(
                            u, session=requests.Session()
                        ),
                        extractor=extract_body_irrawaddy,  # æ—¢å­˜ã®æŠ½å‡ºå™¨ã‚’ä½¿ç”¨
                        retries=3,
                        wait_seconds=2,
                        quiet=False,
                    )
                else:
                    body_text = get_body_with_refetch(
                        art["url"],
                        fetcher=fetch_once_requests,
                        extractor=extract_body_generic_from_soup,
                        retries=2,
                        wait_seconds=1,
                        quiet=True,
                    )

            # â‘¢ æ­£è¦åŒ–
            title_nfc = unicodedata.normalize("NFC", art["title"])
            body_nfc = unicodedata.normalize("NFC", body_text)
            
            # ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»/å…¨ä½“/éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ã®ãƒ’ãƒƒãƒˆåˆ¤å®šï¼ˆå„1å›ã®ã¿ï¼‰
            is_ayeyar = is_ayeyarwady_hit(title_nfc, body_nfc)
            # éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ã®ãƒ’ãƒƒãƒˆï¼ˆNEWS_KEYWORDS ã®ã¿ï¼‰
            hit_non_aye = any_keyword_hit(title_nfc, body_nfc)
            # å…¨ä½“ãƒ’ãƒƒãƒˆ = éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ or ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£
            hit_full = hit_non_aye or is_ayeyar

            # â‘£ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆIrrawaddyãªã©å¿…è¦ã«å¿œã˜ã¦ãƒã‚¤ãƒ‘ã‚¹ï¼‰
            if not bypass_keyword:
                if not hit_full:
                    log_no_keyword_hit(
                        source_name,
                        art["url"],
                        title_nfc,
                        body_nfc,
                        "enqueue:after-fetch",
                    )
                    continue

            # â‘¤ ã‚­ãƒ¥ãƒ¼æŠ•å…¥
            queued_items.append(
                {
                    "source": source_name,
                    "url": art["url"],
                    "title": art["title"],  # ç¿»è¨³å‰ã‚¿ã‚¤ãƒˆãƒ«
                    "body": body_text,  # ç¿»è¨³å‰æœ¬æ–‡
                    "is_ayeyar": is_ayeyar,  # ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ãƒ’ãƒƒãƒˆåˆ¤å®š
                    "hit_full": hit_full,  # å…¨ä½“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
                    "hit_non_ayeyar": hit_non_aye,  # éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£åˆ¤å®š
                }
            )

        except Exception as e:
            print(f"Error processing {art['url']}: {e}")
            continue

    translation_queue.extend(queued_items)


# MEMO: ãƒ­ã‚°ç”¨ã€ãƒ‡ãƒãƒƒã‚¯ç”¨é–¢æ•°
# def process_translation_batches(batch_size=10, wait_seconds=60):
#     summarized_results = []

#     # ãƒ†ã‚¹ãƒˆç”¨ã« translation_queue ã®ä¸­èº«ã‚’ãã®ã¾ã¾ summarized_results ã«è©°ã‚ã‚‹
#     for item in translation_queue:
#         summarized_results.append({
#             "source": item["source"],
#             "url": item["url"],
#             "title": item["title"],      # ç¿»è¨³å‰ã‚¿ã‚¤ãƒˆãƒ«
#             "summary": item["body"][:2000]  # è¦ç´„ã®ä»£ã‚ã‚Šã«æœ¬æ–‡å†’é ­
#         })

#     # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆsummarized_results ã®ä¸­èº«ã‚’çœç•¥ã›ãšç¢ºèªï¼‰
#     print("===== DEBUG: summarized_results =====")
#     pprint.pprint(summarized_results, width=120, compact=False)
#     print("===== END DEBUG =====")

#     # ã“ã“ã§å‡¦ç†çµ‚äº†
#     return summarized_results


# é‡è¤‡è¨˜äº‹å‰Šé™¤å‡¦ç†ã‚»ãƒƒãƒˆ
def _strip_tags(text: str) -> str:
    # è¦ç´„ã«å«ã‚ãŸ <br> ãªã©ã‚’ç´ ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼ˆæœ€ä½é™ï¼‰
    text = text.replace("<br>", "\n")
    return re.sub(r"<[^>]+>", "", text)


def _safe_json_loads_maybe_extract(text: str):
    """
    ç”ŸæˆAIãŒå‰å¾Œã«ä½™è¨ˆãªæ–‡ã‚’ä»˜ã‘ãŸå ´åˆã§ã‚‚JSONéƒ¨åˆ†ã ã‘æŠ½å‡ºã—ã¦èª­ã‚€ä¿é™ºã€‚
    """
    try:
        return json.loads(text)
    except Exception:
        # æœ€å¾Œã® { ... } ã‚’ç´ æœ´ã«æŠ½å‡º
        m = re.search(r"\{.*\}", text, flags=re.DOTALL)
        if m:
            return json.loads(m.group(0))
        raise


# é‡è¤‡åˆ¤å®šã®ãƒ­ã‚°å‡ºåŠ›
def log_dedupe_report(
    data: dict,
    id_map: dict,
    id_to_meta: dict,
    article_ids_in_order: list[str],
    *,
    printer=print,
    header="ğŸ§© DEDUPE REPORT",
):
    """
    LLMå¿œç­”ãƒ‡ãƒ¼ã‚¿(data)ã¨ã€IDâ†’è¨˜äº‹ãƒ¡ã‚¿æƒ…å ±ã®ãƒãƒƒãƒ—ã‚’å—ã‘å–ã‚Šã€
    é‡è¤‡åˆ¤å®šãƒ¬ãƒãƒ¼ãƒˆã‚’æ•´å½¢ã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚

    - data: {"kept":[...], "removed":[...], "clusters":[...]}
    - id_map: {id -> å…ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ}
    - id_to_meta: {id -> {"title": str, "source": str}}
    - article_ids_in_order: å…¥åŠ›é †åºã®IDãƒªã‚¹ãƒˆï¼ˆå…ƒé…åˆ—ã®é †ã‚’ä¿ã¤ãŸã‚ã«ä½¿ç”¨ï¼‰
    - printer: å‡ºåŠ›é–¢æ•°ï¼ˆprint ã‚„ logger.info ãªã©ï¼‰
    """
    kept_list = data.get("kept") or []
    removed_list = data.get("removed") or []
    clusters = data.get("clusters") or []

    kept_ids = [x.get("id") for x in kept_list if x.get("id") in id_map]
    kept_set = set(kept_ids)

    printer(f"\n===== {header} =====")

    # 1) Kept æ¦‚è¦
    printer(f"Kept: {len(kept_ids)} item(s)")
    for k in kept_list:
        kid = k.get("id")
        meta = id_to_meta.get(kid, {})
        why = (k.get("why") or "").strip()
        if kid in id_map:
            why_part = (
                f"  | why: {why}" if why else ""
            )  # â† ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å¼ã«å…¥ã‚Œãªã„
            printer(
                f"  âœ“ [{kid}] {meta.get('title','(no title)')}  | src={meta.get('source','')}"
                f"{why_part}"
            )
        else:
            printer(f"  âœ“ [{kid}] (unknown id)")

    # 2) Removed è©³ç´°ï¼ˆã©ã‚Œã®é‡è¤‡ã¨ã—ã¦è½ã¡ãŸã‹ï¼‰
    printer(f"\nRemoved (LLM-reported): {len(removed_list)} item(s)")
    for r in removed_list:
        rid = r.get("id")
        dup = r.get("duplicate_of")
        why = (r.get("why") or "").strip()
        rmeta = id_to_meta.get(rid, {"title": "(unknown)", "source": ""})
        kmeta = id_to_meta.get(dup, {"title": "(unknown)", "source": ""})
        unknown_flags = []
        if rid not in id_map:
            unknown_flags.append("RID_NOT_IN_INPUT")
        if dup and dup not in id_map:
            unknown_flags.append("KEPT_NOT_IN_INPUT")
        uf = f"  [{', '.join(unknown_flags)}]" if unknown_flags else ""
        reason_line = f"\n      reason: {why}" if why else ""  # â† å…ˆã«ä½œã‚‹
        printer(
            f"  - [{rid}] {rmeta['title']}  | src={rmeta['source']}\n"
            f"      â†’ duplicate of [{dup}] {kmeta['title']}  | src={kmeta['source']}{uf}"
            f"{reason_line}"
        )

    # 3) å®Ÿå·®åˆ†ï¼ˆå…¥åŠ› - keptï¼‰
    derived_removed_ids = [aid for aid in article_ids_in_order if aid not in kept_set]
    printer(f"\nRemoved (derived by kept-set): {len(derived_removed_ids)} item(s)")
    for rid in derived_removed_ids:
        rmeta = id_to_meta.get(rid, {"title": "(unknown)", "source": ""})
        rrec = next((x for x in removed_list if x.get("id") == rid), None)
        if rrec:
            dup = rrec.get("duplicate_of")
            why = (rrec.get("why") or "").strip()
            kmeta = id_to_meta.get(dup, {"title": "(unknown)", "source": ""})
            reason_line = f"\n      reason: {why}" if why else ""  # â† å…ˆã«ä½œã‚‹
            printer(
                f"  - [{rid}] {rmeta['title']}  | src={rmeta['source']}\n"
                f"      â†’ duplicate of [{dup}] {kmeta['title']}  | src={kmeta['source']}"
                f"{reason_line}"
            )
        else:
            printer(
                f"  - [{rid}] {rmeta['title']}  | src={rmeta['source']} (â€» LLMã®removedã«æœªè¨˜è¼‰)"
            )

    # 4) å‚ç…§æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    unknown_kept = [
        kid for kid in [x.get("id") for x in kept_list] if kid not in id_map
    ]
    unknown_removed = [r.get("id") for r in removed_list if r.get("id") not in id_map]
    if unknown_kept:
        printer(f"\nâš ï¸ Keptã«æœªçŸ¥ã®IDãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {unknown_kept}")
    if unknown_removed:
        printer(f"âš ï¸ Removedã«æœªçŸ¥ã®IDãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {unknown_removed}")

    # 5) ã‚¯ãƒ©ã‚¹ã‚¿æ¦‚è¦ï¼ˆä»»æ„ï¼‰
    if clusters:
        printer("\nCluster summary:")
        cluster_kept_map = {
            k.get("cluster_id"): k.get("id") for k in kept_list if k.get("cluster_id")
        }
        for c in clusters:
            cid = c.get("cluster_id")
            members = c.get("member_ids") or []
            event_key = c.get("event_key") or ""
            kept_id_for_cluster = cluster_kept_map.get(cid)
            printer(
                f"  â€¢ cluster={cid}  members={len(members)}  kept={kept_id_for_cluster}  event='{event_key}'"
            )

    printer("===== END DEDUPE REPORT =====\n")


def dedupe_articles_with_llm(
    client,
    summarized_results,
    debug=True,
    *,
    logger=None,
    ultra_max_chars=300,
    summary_fallback_chars=600,
):
    """
    summarized_results (list[dict]) ã‚’å—ã‘å–ã‚Šã€é‡è¤‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã”ã¨ã«1æœ¬ã ã‘æ®‹ã—ãŸé…åˆ—ã‚’è¿”ã™ã€‚
    Irrawaddyï¼ˆsource == "Irrawaddy" ã¾ãŸã¯ URL ã« "irrawaddy.com" ã‚’å«ã‚€ï¼‰ã¯
    LLM ã§ã®é‡è¤‡åˆ¤å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å¸¸ã« keep ã™ã‚‹ã€‚
    ä¾å­˜: call_gemini_with_retries, _safe_json_loads_maybe_extract, _strip_tags, log_dedupe_report
    """

    if not summarized_results:
        return summarized_results

    # å‡ºåŠ›é–¢æ•°
    if debug:
        printer = logger.info if logger else print
    else:

        def _noop(*args, **kwargs):
            return None

        printer = _noop

    # ===== LLMå…¥åŠ›ç”¨ï¼ˆIrrawaddy ã‚’é™¤å¤–ï¼‰ã‚’æ§‹ç¯‰ =====
    irrawaddy_ids = set()
    articles_for_llm = []
    id_map_llm = {}
    id_to_meta_llm = {}
    ids_in_order_llm = []
    all_ids_in_order = []  # è¿”å´æ™‚ã®é †åºç¶­æŒç”¨

    for idx, it in enumerate(summarized_results):
        _id_raw = it.get("url") or f"idx-{idx}"
        _id = _norm_id(_id_raw)  # â˜… å…¥åŠ›å´ï¼ˆè‡ªåˆ†å´ï¼‰ã®IDã‚’æ­£è¦åŒ–
        all_ids_in_order.append(_id)

        # Irrawaddy åˆ¤å®šï¼ˆã”æŒ‡å®šã©ãŠã‚Šï¼‰
        is_irrawaddy = (it.get("source") == "Irrawaddy") or (
            "irrawaddy.com" in (it.get("url") or "")
        )
        if is_irrawaddy:
            irrawaddy_ids.add(_id)
            continue  # LLM ã«ã¯é€ã‚‰ãªã„

        # é Irrawaddy â†’ LLM å…¥åŠ›ã¸
        body_ultra = (it.get("ultra") or "").strip()
        body_fallback = _strip_tags(it.get("summary", ""))[:summary_fallback_chars]
        body = body_ultra[:ultra_max_chars] if body_ultra else body_fallback

        ids_in_order_llm.append(_id)
        id_map_llm[_id] = it
        id_to_meta_llm[_id] = {"title": it.get("title"), "source": it.get("source")}
        articles_for_llm.append(
            {
                "id": _id,
                "source": it.get("source"),
                "title": it.get("title"),
                "body": body,
            }
        )

    # ã™ã¹ã¦ Irrawaddy ã ã£ãŸå ´åˆã¯ãã®ã¾ã¾è¿”ã™
    if not articles_for_llm:
        if debug and irrawaddy_ids:
            printer(
                f"â­ï¸ å…¨ {len(irrawaddy_ids)} ä»¶ãŒ Irrawaddyã€‚LLM é‡è¤‡åˆ¤å®šã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
        return summarized_results

    # ===== ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆLLM ã«é€ã‚‹åˆ†ã®ã¿ï¼‰ =====
    if debug:
        if irrawaddy_ids:
            printer(f"â­ï¸ Irrawaddy {len(irrawaddy_ids)} ä»¶ã¯å¸¸ã« keepï¼ˆLLM ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚")
        printer("===== DEBUG 2: articles SENT TO LLM =====")
        printer(_pprint.pformat(articles_for_llm, width=120, compact=False))
        printer("===== END DEBUG 2 =====\n")

    # ===== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆé Irrawaddy ã®ã¿ï¼‰ =====
    prompt = (
        "ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡è¤‡åˆ¤å®šãƒ•ã‚£ãƒ«ã‚¿ã§ã™ã€‚\n"
        "ä»¥å¾Œã®åˆ¤å®šã¯å„è¨˜äº‹ã®ã€Œtitleã€ã¨ã€Œbodyï¼ˆã“ã‚Œã¯è¶…è¦ç´„ã¾ãŸã¯çŸ­ç¸®è¦ç´„ï¼‰ã€ã®ã¿ã‚’ä½¿ç”¨ã—ã€å…ƒæœ¬æ–‡ã«ã¯æˆ»ã£ã¦å†å‚ç…§ã—ã¾ã›ã‚“ã€‚\n"
        "ç›®çš„ï¼šåŒä¸€ä¸»æ—¨ï¼ˆãƒˆãƒ”ãƒƒã‚¯ + è§’åº¦ + ç™ºä¿¡ä¸»ä½“ï¼‰ã‚’å ±ã˜ã‚‹è¨˜äº‹ã‚’æŸã­ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰1æœ¬ã ã‘æ®‹ã—ã¾ã™ã€‚å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ã€‚\n\n"
        "ã€å®šç¾©ã€‘\n"
        "ãƒ»ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ï¼šwho / what / where / when ã®ã†ã¡å°‘ãªãã¨ã‚‚3è¦ç´ ãŒä¸€è‡´ï¼ˆè¨€ã„æ›ãˆãƒ»è¨€èªå·®ã¯åŒä¸€æ‰±ã„ã€‚æ—¥ä»˜ã¯Â±14æ—¥ã‚’åŒä¸€æ‰±ã„å¯ï¼‰ã€‚\n"
        "ãƒ»è¨˜äº‹ã®ç¨®é¡ï¼ˆtypeï¼‰ï¼šä»¥ä¸‹ã®æ­£è¦åŒ–ã‚«ãƒ†ã‚´ãƒªã®ã„ãšã‚Œã‹1ã¤ã«å†…éƒ¨ã§åˆ†é¡ã—ã¦ç”¨ã„ã‚‹ï¼ˆå‡ºåŠ›ã«ã¯å«ã‚ãªã„ï¼‰ã€‚\n"
        "  é€Ÿå ±/å˜å ±, æ”¿ç­–ç™ºè¡¨è¦ç‚¹, å…¬å¼ç™ºè¡¨/å£°æ˜, ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼, è§£èª¬/èƒŒæ™¯, ç‰©å£°æ˜, çµ„ç¹”å£°æ˜, å…¬ç¤º,\n"
        "  ãƒ‡ãƒ¼ã‚¿/çµ±è¨ˆ, ã¾ã¨ã‚/ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ, ãƒ©ã‚¤ãƒ–/æ™‚ç³»åˆ—æ›´æ–°,\n"
        "  å†™çœŸ/æ˜ åƒç‰¹é›†, ç¤¾èª¬/è«–èª¬/å¯„ç¨¿, ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«\n"
        "  è¿‘ã„åŒç¾©èªã¯å†…éƒ¨ã§æ­£è¦åŒ–ï¼šã€press release/announcementâ†’å…¬å¼ç™ºè¡¨/å£°æ˜ã€ã€explainer/analysisâ†’è§£èª¬/èƒŒæ™¯ã€\n"
        "  ã€roundup/digestâ†’ã¾ã¨ã‚/ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã€ã€live updatesâ†’ãƒ©ã‚¤ãƒ–/æ™‚ç³»åˆ—æ›´æ–°ã€\n"
        "  åˆ¤åˆ¥ä¸èƒ½ãªå ´åˆã¯ type=ä¸æ˜ ã¨ã—ã€ç¨®é¡ä¸€è‡´ã«ã¯æ•°ãˆãªã„ã€‚\n"
        "ãƒ»ç™ºä¿¡ä¸»ä½“ï¼ˆprovenanceï¼‰ï¼šä»¥ä¸‹ã®ã„ãšã‚Œã‹1ã¤ã‚’å†…éƒ¨ã§æ¨å®šã—ã¦ç”¨ã„ã‚‹ã€‚\n"
        "  â‘  æœ¬äººæŒ‡ç¤º/é¦–é•·ã®ç›´è¨€ï¼ˆä¾‹ï¼šãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³ãŒã€ŒæŒ‡ç¤º/å‘½ä»¤/è¡¨æ˜ã€ï¼‰\n"
        "  â‘¡ å…¬å¼æ©Ÿé–¢ã®ç™ºè¡¨ï¼ˆå®˜å ±/ä¼šè¦‹/æ–‡æ›¸/åºƒå ±ï¼‰\n"
        "  â‘¢ åŒ¿åã®è»ç­‹/é–¢ä¿‚è€…/æ¶ˆæ¯ç­‹/å†…éƒ¨ç­‹ï¼ˆã€Œè»ç­‹ã«ã‚ˆã‚Œã°ã€ã€Œé–¢ä¿‚è€…ã«ã‚ˆã‚‹ã¨ã€ç­‰ï¼‰\n"
        "  â‘£ ç¾åœ°é‹ç”¨ãƒ»æ²»å®‰éƒ¨éšŠ/å§”å“¡ä¼šã®å®Ÿå‹™é€šé”\n\n"
        "ã€åˆ¤å®šæ–¹é‡ã€‘\n"
        "1) åŒä¸€ä¸»æ—¨ = ã€ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ã€ã‹ã¤ã€ç¨®é¡ä¸€è‡´ï¼ˆtypeãŒä¸€è‡´ã€ã‹ã¤ä¸æ˜ä»¥å¤–ï¼‰ã€ã‹ã¤ã€ç™ºä¿¡ä¸»ä½“ï¼ˆprovenanceï¼‰ä¸€è‡´ã€ã®å…¨ã¦ã‚’æº€ãŸã™å ´åˆã«é™ã‚‹ã€‚\n"
        "   â€» ã¾ã¨ã‚/ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ/è¤‡æ•°æ¡ˆä»¶åˆ—æŒ™ã®è¦ç´„ã¨ã€å˜ä¸€æ¡ˆä»¶ã®é€Ÿå ±ãƒ»è§£èª¬ã¯é‡è¤‡ã«ã—ãªã„ï¼ˆåˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼‰ã€‚\n"
        "   â€» åŒä¸€ãƒ†ãƒ¼ãƒï¼ˆä¾‹ï¼šé¸æŒ™é‹å‹•è¦åˆ¶ï¼‰ã§ã‚‚ã€å†…å®¹è¦åˆ¶ã€ã¨ã€é‹ç”¨ãƒ»æ‰‹ç¶šï¼ˆè¨±èªå¯/å ´æ‰€/æ™‚é–“/è­¦å‚™/ç®¡ç†ï¼‰ã€ã¯åˆ¥è§’åº¦ã¨ã—ã¦å¿…ãšåˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã™ã‚‹ã€‚\n"
        "   ä¾‹ï¼šã€ˆè»ã¸ã®æ‰¹åˆ¤çš„é¸æŒ™é‹å‹•ã‚’ç¦ã˜ã‚‹ï¼ˆå†…å®¹è¦åˆ¶ï¼‰ã€‰ã¨ã€ˆè»ã®ç®¡ç†ä¸‹ãƒ»äº‹å‰è¨±å¯ã§ã®ã¿é¸æŒ™æ´»å‹•å¯ï¼ˆé‹ç”¨ãƒ»æ‰‹ç¶šï¼‰ã€‰ã¯åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã€‚\n"
        "   ä¾‹ï¼šã€ˆMAHæœ¬äººãŒâ€œæ‰¹åˆ¤ç¦æ­¢â€ã‚’æŒ‡ç¤ºï¼ˆæœ¬äººæŒ‡ç¤ºï¼‰ã€‰ã¨ã€ˆãƒãƒ”ãƒ‰ãƒ¼è»ç­‹ãŒâ€œè¨±å¯åˆ¶ãƒ»ç®¡ç†ä¸‹â€ã¨ä¼èï¼ˆè»ç­‹ï¼‰ã€‰ã¯ã€è§’åº¦ã‚‚ç™ºä¿¡ä¸»ä½“ã‚‚ç•°ãªã‚‹ãŸã‚åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã€‚\n"
        "2) ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–ï¼šè¨˜äº‹ã¯æœ€ã‚‚ä¸€è‡´åº¦ãŒé«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã®ã¿æ‰€å±ã€‚ä¸ç¢ºå®Ÿãªã‚‰åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã™ã‚‹ã€‚\n"
        "3) æ®‹ã™åŸºæº–ï¼ša)å›ºæœ‰æƒ…å ±é‡ï¼ˆåœ°å/äººæ•°/é‡‘é¡/çµ„ç¹”å/æ–°è¦äº‹å®Ÿï¼‰ b)å…·ä½“æ€§/æ˜ç­ã• c)ã‚¿ã‚¤ãƒˆãƒ«æƒ…å ±é‡ã€‚\n"
        "   åŒç‚¹ãªã‚‰ æœ¬æ–‡é•·ï¼ˆbodyã®æ–‡å­—æ•°ï¼‰â†’ sourceæ˜‡é † â†’ idæ˜‡é † ã®é †ã§æ±ºå®šã€‚\n"
        "4) å…¥åŠ›å¤–ã®äº‹å®Ÿã¯åŠ ãˆãªã„ã€‚çµ±åˆè¨˜äº‹ã¯ä½œã‚‰ãªã„ã€‚\n\n"
        "ã€å‡ºåŠ›ã®åˆ¶ç´„ã€‘\n"
        "ãƒ»JSONã®ã¿ã‚’è¿”ã™ã€‚ä½™è¨ˆãªãƒ†ã‚­ã‚¹ãƒˆã‚„ã‚­ãƒ¼ã¯ç¦æ­¢ã€‚\n"
        "ãƒ»kept/removed/clusters ã® id ã¯å¿…ãšå…¥åŠ› articles ã® id ã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚\n"
        "ãƒ»clusters[].member_ids ã¯å…¥åŠ› id ã‚’é‡è¤‡ãªãã™ã¹ã¦å«ã‚€ã“ã¨ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã¨ keptä»¶æ•°ã¯åŒæ•°ã€‚\n"
        "ãƒ»removed[].duplicate_of ã¯åŒä¸€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã® kept id ã‚’æŒ‡ã™ã“ã¨ã€‚\n"
        "ãƒ»why ã¯16ã€œ24å­—ç¨‹åº¦ã€event_key ã¯25å­—ä»¥å†…ã«åã‚ã‚‹ã“ã¨ã€‚\n\n"
        "å…¥åŠ›:\n"
        f'{{\\n  "articles": {json.dumps(articles_for_llm, ensure_ascii=False)}\\n}}\\n\\n'
        "å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSONã®ã¿ï¼‰:\n"
        "{\n"
        '  "kept": [ {"id":"<æ®‹ã™è¨˜äº‹ID>", "cluster_id":"<ID>", "why":"16-24å­—"} ],\n'
        '  "removed": [ {"id":"<é™¤å¤–è¨˜äº‹ID>", "duplicate_of":"<æ®‹ã—ãŸè¨˜äº‹ID>", "why":"16-24å­—"} ],\n'
        '  "clusters": [ {"cluster_id":"<ID>", "member_ids":["<id1>","<id2>","..."], "event_key":"25å­—ä»¥å†…"} ]\n'
        "}\n"
    )

    try:
        resp = call_gemini_with_retries(
            client,
            prompt,
            model="gemini-2.5-flash",
            max_retries=GEMINI_MAX_RETRIES,
            base_delay=GEMINI_BASE_DELAY,
            max_delay=GEMINI_MAX_DELAY,
            usage_tag="dedupe",
        )
        data = _safe_json_loads_maybe_extract(resp.text)

        # â˜… LLMå¿œç­”å†…ã®IDã‚’ã™ã¹ã¦æ­£è¦åŒ–ã—ã¦ãŠã
        for k in ("kept", "removed"):
            arr = data.get(k) or []
            for rec in arr:
                if "id" in rec:
                    rec["id"] = _norm_id(rec["id"])
                if "duplicate_of" in rec and rec["duplicate_of"]:
                    rec["duplicate_of"] = _norm_id(rec["duplicate_of"])

        for c in data.get("clusters", []) or []:
            if "cluster_id" in c:
                c["cluster_id"] = _norm_id(c["cluster_id"])
            if "member_ids" in c and isinstance(c["member_ids"], list):
                c["member_ids"] = [_norm_id(x) for x in c["member_ids"]]

        kept_ids_others = [
            x.get("id") for x in data.get("kept", []) if x.get("id") in id_map_llm
        ]

        # ãƒ¬ãƒãƒ¼ãƒˆï¼ˆLLM ã«é€ã£ãŸåˆ†ã®ã¿ï¼‰
        if debug:
            log_dedupe_report(
                data=data,
                id_map=id_map_llm,
                id_to_meta=id_to_meta_llm,
                article_ids_in_order=ids_in_order_llm,
                printer=printer,
                header="ğŸ§© DEDUPE REPORT (non-Irrawaddy only)",
            )

        # kept ãŒå‡ºãŸã¨ãã®ã¿ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ã—ã€Irrawaddy ã‚’åˆæµ
        if kept_ids_others:
            kept_union = set(kept_ids_others) | irrawaddy_ids
            filtered = [
                obj
                for obj, _id in zip(summarized_results, all_ids_in_order)
                if _id in kept_union
            ]
            return filtered

        # ã†ã¾ãåˆ¤å®šã§ããªã‹ã£ãŸã‚‰åŸæœ¬ã‚’è¿”ã™ï¼ˆIrrawaddy ã‚‚å½“ç„¶æ®‹ã‚‹ï¼‰
        return summarized_results

    except Exception as e:
        print(f"ğŸ›‘ Dedupe failed, returning original list: {e}")
        return summarized_results


# ===== è¦ç´„ãƒ»ç¿»è¨³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ãƒ¼ãƒ„ =====
STEP12_FILTERS = (
    "Step 1: ä¾‹å¤–ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰\n"
    "Q1. è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯æœ¬æ–‡ãŒ `Myawaddy`, `á€™á€¼á€á€á€®`, `Muse`, `á€™á€°á€†á€šá€º`, `å›½å¢ƒè²¿æ˜“`, `å›½å¢ƒäº¤æ˜“`ã«é–¢ã™ã‚‹å†…å®¹ã§ã™ã‹ï¼Ÿ\n"
    "â†’ Yes ã®å ´åˆã€ã“ã®å¾Œã®åˆ¤å®šã¯è¡Œã‚ãš Step 3 ã«é€²ã‚“ã§ãã ã•ã„ã€‚\n"
    "â†’ No ã®å ´åˆã¯ Step 2 ã¸é€²ã‚“ã§ãã ã•ã„ã€‚\n\n"
    "Step 2: é™¤å¤–æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆåœ°åŸŸä¾‹å¤–ã‚ã‚Šï¼ä¸»é¡Œï¼è¨˜äº‹ã®è§’åº¦ã§åˆ¤å®šï¼‰\n"
    "Q2. æœ¬è¨˜äº‹ã®ä¸»é¡ŒãŒã€ç‰¹å®šã®åœ°åŸŸï¼ˆéƒ¡åŒºãƒ»ã‚¿ã‚¦ãƒ³ã‚·ãƒƒãƒ—ãƒ»å¸‚ãƒ»æ‘ï¼‰ã«ãŠã‘ã‚‹\n"
    "    å±€åœ°çš„æ²»å®‰ã‚¤ãƒ™ãƒ³ãƒˆã®ã€Œç™ºç”Ÿãã®ã‚‚ã®ã€ã‚’é€Ÿå ±ãƒ»è¨˜éŒ²ãƒ»å ±é“ã™ã‚‹è¨˜äº‹ã§ã™ã‹ï¼Ÿ\n"
    "    ï¼ˆæˆ¦é—˜ãƒ»äº¤æˆ¦ã€æ”»æ’ƒã€”è¥²æ’ƒ/çˆ†ç ´/ç‹™æ’ƒ/IED/ãƒ‰ãƒ­ãƒ¼ãƒ³ã€•ã€è¡çªã€çˆ†æ’ƒ/ç ²æ’ƒ/ç©ºçˆ†ã€\n"
    "      å¼·ç›—/ç•¥å¥ªã€æŠ—è­°æ´»å‹•ã€æŠ•é™ã€è§£æ”¾ã€æ®ºäººäº‹ä»¶ã®ç™ºç”Ÿå ±é“ãƒ»è¢«å®³é›†è¨ˆï¼‰\n"
    "\n"
    "é™¤å¤–ã—ãªã„æ˜ç¢ºãªä¾‹ï¼ˆ= No ã¨ã™ã‚‹ï¼‰ï¼š\n"
    "- äººç‰©/çµ„ç¹”ã®ç™ºè¨€ãƒ»åè«–ãƒ»å£°æ˜ãƒ»ä¼šè¦‹ãƒ»ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ãƒ»å‘Šç™ºãƒ»å¦å®šãŒä¸»é¡Œã®ã‚‚ã®\n"
    "  ï¼ˆäº‹ä»¶ã®å…·ä½“ä¾‹ã‚„åœ°åãƒ»äººæ•°ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ä¸»é¡ŒãŒä¸Šè¨˜ã§ç¤ºã—ãŸã€Œç™ºè¨€ã€ãªã‚‰ Noï¼‰\n"
    "- äº‹ä»¶ã®ç™ºç”Ÿãã®ã‚‚ã®ã§ã¯ãªãã€äº‹ä»¶ãŒå¼•ç”¨ã¨ã—ã¦ä½¿ã‚ã‚Œã¦ã„ã‚‹ã ã‘ã®è¨˜äº‹\n"
    "\n"
    "ä¸»é¡Œåˆ¤å®šã®æ‰‹ãŒã‹ã‚Šï¼š\n"
    "- ã‚¿ã‚¤ãƒˆãƒ«ãƒ»å†’é ­å„ªå…ˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«å…ˆé ­60å­—ï¼‹æœ¬æ–‡å†’é ­300å­—ã‚’é‡ã¿ä»˜ã‘ï¼‰\n"
    "- ç™ºè¨€ä¸»é¡Œã‚’ç¤ºã™åˆå›³èªï¼ˆæ—¥æœ¬èª/è‹±èª/ãƒ“ãƒ«ãƒèªï¼‰ï¼š\n"
    "  ã€Œå£°æ˜ã€ã€Œç™ºè¡¨ã€ã€Œåè«–ã€ã€Œå¦å®šã€ã€Œä¼šè¦‹ã€ã€Œè«‡è©±ã€ã€Œã¨è¿°ã¹ãŸã€ã€Œã¨èªã£ãŸã€ã€Œã¨ä¸»å¼µã€ã€\n"
    '  "statement","press conference","spokesperson","said","denied","accused",\n'
    "  ã€Œá€•á€¼á€±á€¬á€†á€­á€¯ã€ã€Œá€‘á€¯á€á€ºá€•á€¼á€”á€ºã€ã€Œá€á€¯á€¶á€·á€•á€¼á€”á€ºã€ã€Œá€†á€­á€¯á€á€Šá€ºã€ã€Œá€•á€¼á€±á€¬á€€á€¼á€¬á€¸ã€ã€Œá€•á€¼á€±á€¬á€›á€±á€¸á€†á€­á€¯á€á€½á€„á€·á€ºá€›á€¾á€­á€á€°ã€ã€Œá€á€á€„á€ºá€¸á€…á€¬á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€•á€½á€²ã€\n"
    "â†’ ä¸Šè¨˜ã«è©²å½“ã™ã‚Œã° No ã¨ã—ã¦ Step 3 ã¸é€²ã‚€ã€‚\n"
    "\n"
    "â†’ Yes ã®å ´åˆã§ã‚‚ã€è¨˜äº‹ã®ä¸»ãŸã‚‹ç™ºç”Ÿå ´æ‰€ãŒæ¬¡ã®åœ°åŸŸã«è©²å½“ã™ã‚‹ãªã‚‰é™¤å¤–ã›ãš Step 3 ã¸é€²ã‚“ã§ãã ã•ã„ï¼š\n"
    "   ãƒ»ãƒ¤ãƒ³ã‚´ãƒ³ç®¡åŒº / Yangon Region / á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸\n"
    "   ãƒ»ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç®¡åŒº / Ayeyarwady Region / á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸\n"
    "â†’ ä¸Šè¨˜ä»¥å¤–ã®åœ°åŸŸã§ã‚ã‚Œã°å‡¦ç†ã‚’çµ‚äº†ã—ã€Step 3 ã«ã¯é€²ã¾ãªã„ã§ãã ã•ã„ã€‚å›ç­”ã¯ exit ã®1èªã®ã¿ï¼ˆè¨˜å·ãƒ»è£…é£¾ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãªã—ã€å°æ–‡å­—ï¼‰ã§è¿”ã—ã¦ãã ã•ã„ã€‚\n"
    "â†’ No ã®å ´åˆã¯ Step 3 ã¸é€²ã‚“ã§ãã ã•ã„ã€‚\n"
)

STEP3_TASK = (
    "Step 3: ç¿»è¨³ã¨è¦ç´„å‡¦ç†\n"
    "ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã€æœ¬æ–‡ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\n"
    "ã€ç¿»è¨³æ™‚ã®ç”¨èªçµ±ä¸€ãƒ«ãƒ¼ãƒ«ï¼ˆå¿…ãšå¾“ã†ã“ã¨ï¼‰ã€‘\n"
    "ã“ã®ãƒ«ãƒ¼ãƒ«ã¯è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®ç¿»è¨³ã«å¿…ãšé©ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
    "ã‚¯ãƒ¼ãƒ‡ã‚¿ãƒ¼æŒ‡å°è€…â‡’ç·å¸ä»¤å®˜\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆæŒ‡å°è€…ãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³â‡’ãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆã®ãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³â‡’ãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆè»äº‹æŒ‡å°è€…â‡’ç·å¸ä»¤å®˜\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆè»äº‹æ”¿æ¨©â‡’è»äº‹æ”¿æ¨©\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆè»äº‹è©•è­°ä¼šâ‡’è»äº‹æ”¿æ¨©\n"
    "ãƒ†ãƒ­ãƒªã‚¹ãƒˆè»â‡’å›½è»\n"
    "è»äº‹è©•è­°ä¼šâ‡’è»äº‹æ”¿æ¨©\n"
    "è»äº‹å§”å“¡ä¼šâ‡’è»äº‹æ”¿æ¨©\n"
    "å¾´ç”¨â‡’å¾´å…µ\n"
    "è»äº‹è©•è­°ä¼šè»â‡’å›½è»\n\n"
    "ã€ç¿»è¨³æ™‚ã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«ã€‘\n"
    "ã“ã®ãƒ«ãƒ¼ãƒ«ã‚‚è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®ç¿»è¨³ã«å¿…ãšé©ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
    "ã€Œá€–á€™á€ºá€¸á€†á€®á€¸ã€ã®è¨³èªã¯æ–‡è„ˆã«ã‚ˆã£ã¦ä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ã€‚\n"
    "- çŠ¯ç½ªå®¹ç–‘ã‚„æ³•å¾‹é•åã«å¯¾ã™ã‚‹æ–‡è„ˆã®å ´åˆã¯ã€Œé€®æ•ã€ã¨ã™ã‚‹ã€‚\n"
    "- çŠ¯ç½ªå®¹ç–‘ã‚„æ³•å¾‹é•åã«åŸºã¥ã‹ãªã„æ–‡è„ˆã®å ´åˆã¯ã€Œæ‹˜æŸã€ã¨ã™ã‚‹ã€‚\n\n"
    "ã€é€šè²¨æ›ç®—ãƒ«ãƒ¼ãƒ«ã€‘\n"
    "ã“ã®ãƒ«ãƒ¼ãƒ«ã‚‚è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã®ç¿»è¨³ã«å¿…ãšé©ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
    "ãƒŸãƒ£ãƒ³ãƒãƒ¼é€šè²¨ã€Œãƒãƒ£ãƒƒãƒˆï¼ˆKyatã€á€€á€»á€•á€ºï¼‰ã€ãŒå‡ºã¦ããŸå ´åˆã¯ã€æ—¥æœ¬å††ã«æ›ç®—ã—ã¦ä½µè¨˜ã—ã¦ãã ã•ã„ã€‚\n"
    "- æ›ç®—ãƒ¬ãƒ¼ãƒˆã¯ 1ãƒãƒ£ãƒƒãƒˆ = 0.033å†† ã‚’å¿…ãšä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚\n"
    "- è¨˜äº‹ä¸­ã«ãƒãƒ£ãƒƒãƒˆãŒå‡ºãŸå ´åˆã¯å¿…ãšã€Œâ—¯ãƒãƒ£ãƒƒãƒˆï¼ˆç´„â—¯å††ï¼‰ã€ã®å½¢å¼ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
    "- æ—¥æœ¬å††ã®è¡¨è¨˜ã¯å°æ•°ç‚¹ä»¥ä¸‹ã¯å››æ¨äº”å…¥ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 16,500å††ï¼‰ã€‚\n"
    "- ä»–ã®ãƒ¬ãƒ¼ãƒˆã¯ä½¿ç”¨ç¦æ­¢ã€‚\n\n"
    "ã‚¿ã‚¤ãƒˆãƒ«ï¼š\n"
    "- è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
    "ã‚¿ã‚¤ãƒˆãƒ«ã®å‡ºåŠ›æ¡ä»¶ï¼š\n"
    "- å‡ºåŠ›ã¯å¿…ãš1è¡Œã§ã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘<åŠè§’ã‚¹ãƒšãƒ¼ã‚¹1ã¤><è¨³ã—ãŸã‚¿ã‚¤ãƒˆãƒ«>ã€ã®å½¢å¼ã«ã™ã‚‹ã€‚\n"
    "- ã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘ã€ã®ç›´å¾Œã«æ”¹è¡Œã—ãªã„ã€‚\n"
    "- ã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘<åŠè§’ã‚¹ãƒšãƒ¼ã‚¹1ã¤><è¨³ã—ãŸã‚¿ã‚¤ãƒˆãƒ«>ã€ä»¥å¤–ã®æ–‡è¨€ã¯å›ç­”ã«å«ã‚ãªã„ã€‚\n\n"
    "æœ¬æ–‡è¦ç´„ï¼š\n"
    "- ä»¥ä¸‹ã®è¨˜äº‹æœ¬æ–‡ã«ã¤ã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ã¾ã¨ã‚ã€500å­—ä»¥å†…ã§å…·ä½“çš„ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
    "- è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
    "- å€‹åˆ¥è¨˜äº‹ã®æœ¬æ–‡ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€ãƒ¡ãƒ‡ã‚£ã‚¢èª¬æ˜ã‚„ãƒšãƒ¼ã‚¸å…¨ä½“ã®è§£èª¬ã¯ä¸è¦ã§ã™ã€‚\n"
    "- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã¯è¦ç´„ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€ãã‚Œä»¥å¤–ã®æ–‡è¨€ã¯ä¸è¦ã§ã™ã€‚\n\n"
    "æœ¬æ–‡è¦ç´„ã®å‡ºåŠ›æ¡ä»¶ï¼š\n"
    "- 1è¡Œç›®ã¯`ã€è¦ç´„ã€‘`ã¨ã ã‘ã—ã¦ãã ã•ã„ã€‚\n"
    "- 2è¡Œç›®ä»¥é™ãŒå…¨ã¦ç©ºè¡Œã«ãªã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
    "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã‚’é©åˆ‡ã«ä½¿ã£ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚\n"
    "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã«ã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ï¼ˆ#, *, - ãªã©ï¼‰ã‚’ä½¿ã‚ãšã€å˜ç´”ãªãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
    "- è¦‹å‡ºã—ã¯ `[ ]` ã§å›²ã‚“ã§ãã ã•ã„ã€‚\n"
    "- ç©ºè¡Œã¯ä½œã‚‰ãªã„ã§ãã ã•ã„ã€‚\n"
    "- ç‰¹æ®Šè¨˜å·ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ï¼ˆå…¨ä½“ã‚’HTMLã¨ã—ã¦é€ä¿¡ã™ã‚‹ã‚ã‘ã§ã¯ãªã„ãŸã‚ï¼‰ã€‚\n"
    "- ç®‡æ¡æ›¸ãã¯`ãƒ»`ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚\n"
    "- æœ¬æ–‡è¦ç´„ã®åˆè¨ˆã¯æœ€å¤§500æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„ã€‚\n\n"
    "æœ¬æ–‡è¶…è¦ç´„ï¼š\n"
    "- ä»¥ä¸‹ã®è¨˜äº‹æœ¬æ–‡ã«ã¤ã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã¾ã¨ã‚ã€200å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
    "- å€‹åˆ¥è¨˜äº‹ã®æœ¬æ–‡ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€ãƒ¡ãƒ‡ã‚£ã‚¢èª¬æ˜ã‚„ãƒšãƒ¼ã‚¸å…¨ä½“ã®è§£èª¬ã¯ä¸è¦ã§ã™ã€‚\n"
    "- ä¾‹ï¼šã€èª°ãŒã€ã€ä½•ã‚’ã€ã€ã©ã“ã§ã€ã€ã„ã¤ã€ã€è¦æ¨¡ï¼ˆäººæ•°/é‡‘é¡ç­‰ï¼‰ã€ã‚’å«ã‚ã‚‹ã€‚\n\n"
    "æœ¬æ–‡è¶…è¦ç´„ã®å‡ºåŠ›æ¡ä»¶ï¼š\n"
    "- 1è¡Œç›®ã¯`ã€è¶…è¦ç´„ã€‘`ã¨ã ã‘ã—ã¦ãã ã•ã„ã€‚\n"
    "- 2è¡Œç›®ä»¥ãŒé™å…¨ã¦ç©ºè¡Œã«ãªã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
    "- æœ¬æ–‡è¶…è¦ç´„ã®åˆè¨ˆã¯æœ€å¤§200æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„ã€‚\n\n"
)

SKIP_NOTE_IRRAWADDY = "ã€é‡è¦ã€‘æœ¬è¨˜äº‹ã¯ Irrawaddy ã®è¨˜äº‹ã§ã™ã€‚Step 1 ã¨ Step 2 ã¯å®Ÿæ–½ã›ãšã€ç›´ã¡ã« Step 3 ã®ã¿ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚\n\n"


def build_prompt(item: dict, *, skip_filters: bool, body_max: int) -> str:
    header = "æ¬¡ã®æ‰‹é †ã§è¨˜äº‹ã‚’åˆ¤å®šãƒ»å‡¦ç†ã—ã¦ãã ã•ã„ã€‚\n\n"
    pre = SKIP_NOTE_IRRAWADDY if skip_filters else STEP12_FILTERS + "\n\n"
    input_block = (
        "å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼š\n"
        "###\n[è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«]\n###\n"
        f"{item['title']}\n\n"
        "[è¨˜äº‹æœ¬æ–‡]\n###\n"
        f"{item['body'][:body_max]}\n"
        "###\n"
    )
    return header + pre + STEP3_TASK + "\n" + input_block


# è¶…è¦ç´„ã‚’å…ˆã«æŠœãå‡¦ç†
def _normalize_heading_text(s: str) -> str:
    """è¦‹å‡ºã—æ¤œå‡ºã®ãŸã‚ã®è»½é‡æ­£è¦åŒ–ï¼ˆæ‹¬å¼§ã®ç•°ä½“å­—ã‚„ä¸å¯è¦–æ–‡å­—ã‚’å¸åï¼‰"""
    trans = {
        ord("ï¼»"): "ã€",
        ord("ã€”"): "ã€",
        ord("ã€–"): "ã€",  # å·¦
        ord("ï¼½"): "ã€‘",
        ord("ã€•"): "ã€‘",
        ord("ã€—"): "ã€‘",  # å³
    }
    s = s.translate(trans)
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹â†’åŠè§’ã€NBSP/ZWSP/FEFF/ZWJ/ZWNJ ã‚’é™¤å»
    s = s.replace("\u3000", " ").replace("\xa0", " ")
    s = re.sub(r"[\u200b\u200c\u200d\ufeff]", "", s)
    # ã€Œè¦ã€ã€Œç´„ã€ã®é–“ã®å¤‰å‰‡ã‚¹ãƒšãƒ¼ã‚¹ã‚‚å¸å
    s = re.sub(r"(è¦)\s+(ç´„)", r"\1\2", s)
    return unicodedata.normalize("NFC", s)


def _cut_ultra_block(lines):
    """
    ã€Œè¶…è¦ç´„ã€ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆè¦‹å‡ºã—è¡Œã€œæ¬¡è¦‹å‡ºã—ç›´å‰ã¾ã§ï¼‰ã‚’åˆ‡ã‚Šå‡ºã—ã¦å‰Šé™¤ã€‚
    æ‹¬å¼§ã®ç•°ä½“å­—ï¼ˆï¼»ï¼½/ã€–ã€—/ã€”ã€•ï¼‰ã‚„ä¸å¯è¦–æ–‡å­—ã€å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã«è€æ€§ã‚ã‚Šã€‚
    """
    # æ­£è¦åŒ–ã—ãŸå½±ã‚’ä½œã‚‹ï¼ˆæ¤œå‡ºã¯ã“ã£ã¡ã€å‰Šé™¤ã¯å…ƒlinesã§ï¼‰
    norm = [_normalize_heading_text(ln) for ln in lines]

    HEAD_RE = re.compile(
        r"^ã€[\s\u3000\u200b\ufeff]*è¶…[\s\u3000\u200b\ufeff]*è¦[\s\u3000\u200b\ufeff]*ç´„[\s\u3000\u200b\ufeff]*ã€‘"
    )
    NEXT_HDR_RE = re.compile(r"^ã€.*?ã€‘")  # ä»–ã®è¦‹å‡ºã—ï¼ˆè¦ç´„/ã‚¿ã‚¤ãƒˆãƒ«ç­‰ï¼‰

    for i, ln_norm in enumerate(norm):
        if not HEAD_RE.match(ln_norm):
            continue

        # è¦‹å‡ºã—è¡Œã®â€œåŒä¸€è¡Œæœ¬æ–‡â€ï¼ˆæ­£è¦åŒ–å¾Œã§OKï¼‰
        inline = HEAD_RE.sub("", ln_norm).strip()
        start = i + 1

        # æ¬¡ã®è¦‹å‡ºã—ç›´å‰ã¾ã§
        end = start
        while end < len(norm) and not NEXT_HDR_RE.match(norm[end]):
            end += 1

        parts = []
        if inline:
            parts.append(inline)
        parts.extend(lines[start:end])  # æœ¬æ–‡ã¯å…ƒã®è¡Œã‚’ä½¿ã†

        new_lines = lines[:i] + lines[end:]
        return " ".join(parts).strip(), new_lines

    return "", lines


# æœ¬å‡¦ç†é–¢æ•°
def process_translation_batches(batch_size=TRANSLATION_BATCH_SIZE, wait_seconds=60):
    # MEMO: TESTç”¨ã€Geminiã‚’å‘¼ã°ãšã€URLãƒªã‚¹ãƒˆã ã‘è¿”ã™
    # summarized_results = []
    # for item in translation_queue:
    #     summarized_results.append({
    #         "source": item["source"],
    #         "url": item["url"],
    #         "title": item['title'],
    #         "summary": item['body'][:BODY_MAX_CHARS]
    #     })

    summarized_results = []
    for i in range(0, len(translation_queue), batch_size):
        batch = translation_queue[i : i + batch_size]
        print(f"âš™ï¸ Processing batch {i // batch_size + 1}...")

        for item in batch:
            try:
                # ãƒ‡ãƒãƒƒã‚°: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                print("----- DEBUG: Prompt Input -----")
                print(f"TITLE: {item['title']}")
                print(f"BODY[:{BODY_MAX_CHARS}]: {item['body'][:BODY_MAX_CHARS]}")

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œã€Irrawaddy ã¯ Step1/2 ã‚’ã‚¹ã‚­ãƒƒãƒ—
                is_irrawaddy = (item.get("source") == "Irrawaddy") or (
                    "irrawaddy.com" in (item.get("url") or "")
                )
                prompt = build_prompt(
                    item, skip_filters=is_irrawaddy, body_max=BODY_MAX_CHARS
                )

                resp = call_gemini_with_retries(
                    client_summary, prompt, model="gemini-2.5-flash"
                )
                output_text = resp.text.strip()

                print("----- DEBUG: Model Output -----")
                print(output_text)

                # --- exit ã‚’åºƒã‚ã«åˆ¤å®šï¼ˆãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆã‚„å¥èª­ç‚¹æ··å…¥å¯¾ç­–ï¼‰---
                EXIT_ONLY_RE = re.compile(
                    r"^\s*(?:`{0,3})?\s*exit\s*(?:`{0,3})?\.?\s*$", re.IGNORECASE
                )
                if EXIT_ONLY_RE.match(output_text):
                    continue

                # --- è¡Œæ•´å½¢ï¼ˆNFC + ç©ºè¡Œé™¤å»ï¼‰---
                lines = [
                    unicodedata.normalize("NFC", ln).strip()
                    for ln in output_text.splitlines()
                    if ln.strip()
                ]

                # --- è¶…è¦ç´„ã‚’å…ˆã«æŠœãï¼ˆæœ¬æ–‡ã‹ã‚‰ã‚‚æ¶ˆã™ï¼‰---
                ultra_text, lines = _cut_ultra_block(lines)

                # --- ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºï¼ˆè¦ä»¶ã«åˆã‚ã›ã¦å³æ ¼åŒ–ï¼‰---
                # ãƒ«ãƒ¼ãƒ«:
                #  A) ã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘è¨³é¡Œã€= åŒä¸€è¡Œ
                #  B) 1è¡Œç›®ãŒã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘ã€ã®ã¿ â†’ æ¬¡ã®è¡Œã‚’è¨³é¡Œã¨ã—ã¦æ¡ç”¨
                #  C) ä¸Šè¨˜ä»¥å¤–ã®ãƒ©ãƒ™ãƒ«æºã‚Œï¼ˆã‚¿ã‚¤ãƒˆãƒ«:, Title: ãªã©ï¼‰ã¯ç„¡è¦–ï¼ˆæ•‘æ¸ˆã—ãªã„ï¼‰
                title_text = ""
                title_idx = next(
                    (
                        i
                        for i, ln in enumerate(lines)
                        if re.match(r"^ã€\s*ã‚¿ã‚¤ãƒˆãƒ«\s*ã€‘", ln)
                    ),
                    None,
                )
                if title_idx is not None:
                    # ãƒãƒ¼ã‚«ãƒ¼è¡Œã‚’è§£æ
                    m = re.match(r"^ã€\s*ã‚¿ã‚¤ãƒˆãƒ«\s*ã€‘\s*(.*)$", lines[title_idx])
                    inline = (m.group(1) or "").strip()
                    # ãƒãƒ¼ã‚«ãƒ¼è¡Œã¯æ¶ˆã™
                    lines.pop(title_idx)

                    if inline:
                        # A) åŒä¸€è¡Œï¼ˆã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘â—¯â—¯ï¼‰
                        # å…ˆé ­ã«ã‚³ãƒ­ãƒ³ãŒç´›ã‚Œã‚‹äº‹æ•…ã ã‘è»½ãé™¤å»ï¼ˆãƒ©ãƒ™ãƒ«æ•‘æ¸ˆã§ã¯ãªã„ï¼‰
                        title_text = inline.lstrip(":ï¼š").strip()
                    else:
                        # B) æ¬¡ã®è¡Œã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦æ¡ç”¨ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰
                        if title_idx < len(lines):
                            title_text = lines[title_idx].strip()
                            lines.pop(title_idx)

                # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç©ºã‚’è¨±ã•ãªã„ï¼‰
                translated_title = (
                    title_text or item.get("title") or "ï¼ˆç¿»è¨³å¤±æ•—ï¼‰"
                ).strip()

                # --- è¦ç´„ãƒ©ãƒ™ãƒ«ã‚’å…ˆé ­ã«å¼·åˆ¶ ---
                if not lines or not re.match(r"^ã€\s*è¦ç´„\s*ã€‘\s*$", lines[0]):
                    lines.insert(0, "ã€è¦ç´„ã€‘")

                summary_text = "\n".join(lines).strip()
                summary_html = summary_text.replace("\n", "<br>")

                norm_url = _norm_id(item.get("url") or "")

                summarized_results.append(
                    {
                        "source": item["source"],
                        "url": norm_url,  # â˜… æ­£è¦åŒ–æ¸ˆã¿
                        "title": translated_title,
                        "summary": summary_html,
                        "ultra": ultra_text,
                        "is_ayeyar": item.get("is_ayeyar", False),  # ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ãƒ’ãƒƒãƒˆåˆ¤å®š
                        "hit_full": item.get("hit_full", False),  # å…¨ä½“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
                        "hit_non_ayeyar": item.get("hit_non_ayeyar", False),  # éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£åˆ¤å®š
                    }
                )

            except Exception as e:
                print(
                    "ğŸ›‘ Error during translation:", e.__class__.__name__, "|", repr(e)
                )
                continue

            # ãƒãƒƒãƒå†…ã§å¾®ã‚¹ãƒªãƒ¼ãƒ—ã—ã¦ãƒãƒ¼ã‚¹ãƒˆã‚’æŠ‘ãˆã‚‹
            time.sleep(0.6)

        if i + batch_size < len(translation_queue):
            print(f"ğŸ•’ Waiting {wait_seconds} seconds before next batch...")
            time.sleep(wait_seconds)

    # é‡è¤‡åˆ¤å®šâ†’ç‰‡æ–¹æ®‹ã—ï¼ˆæœ€çµ‚ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã®å½¢å¼ã¯å¤‰ãˆãªã„ï¼‰
    deduped = dedupe_articles_with_llm(client_dedupe, summarized_results, debug=True)

    # å¿µã®ãŸã‚ï¼šè¿”å´ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å›ºå®šï¼ˆä½™è¨ˆãªã‚­ãƒ¼ãŒæ··ã–ã£ã¦ã„ãŸã‚‰è½ã¨ã™ï¼‰
    normalized = [
        {
            "source": x.get("source"),
            "url": x.get("url"),
            "title": x.get("title"),
            "summary": x.get("summary"),
            "is_ayeyar": x.get("is_ayeyar", False),  # ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç³»ãƒ’ãƒƒãƒˆåˆ¤å®š
            "hit_full": x.get("hit_full", False),  # å…¨ä½“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
            "hit_non_ayeyar": x.get("hit_non_ayeyar", False),  # éã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£åˆ¤å®š
        }
        for x in deduped
    ]
    return normalized


def send_email_digest(summaries, *, recipients_env=None, subject_suffix=""):
    def _build_gmail_service():
        cid = os.getenv("GMAIL_CLIENT_ID")
        csec = os.getenv("GMAIL_CLIENT_SECRET")
        rtok = os.getenv("GMAIL_REFRESH_TOKEN")
        if not (cid and csec and rtok):
            raise RuntimeError(
                "Gmail API credentials (CLIENT_ID/SECRET/REFRESH_TOKEN) are missing."
            )

        # ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚‹å ´åˆã€scopes ã‚’æ¸¡ã•ãªã„
        creds = Credentials(
            token=None,
            refresh_token=rtok,
            token_uri="https://oauth2.googleapis.com/token",
            client_id=cid,
            client_secret=csec,
        )
        return build("gmail", "v1", credentials=creds, cache_discovery=False)

    sender_email = os.getenv("EMAIL_SENDER")
    env_name = recipients_env
    recipient_emails = [x.strip() for x in os.getenv(env_name, "").split(",") if x.strip()]

    digest_date = get_today_date_mmt()
    date_str = digest_date.strftime("%Yå¹´%-mæœˆ%-dæ—¥") + "åˆ†"

    # ãƒ¡ãƒ‡ã‚£ã‚¢ã”ã¨ã«ã¾ã¨ã‚ã‚‹
    media_grouped = defaultdict(list)
    for item in summaries:
        media_grouped[item["source"]].append(item)

    subject = "ãƒŸãƒ£ãƒ³ãƒãƒ¼é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€" + date_str + "ã€‘"
    # ãƒ†ã‚¹ãƒˆç”¨è¨˜è¿°
    if subject_suffix:
        subject += " " + subject_suffix 

    # âœ… ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³éƒ¨åˆ†ã‚’å…ˆã«æ§‹ç¯‰
    headlines = []
    for item in summaries:
        headlines.append(f"âœ“ {item['title']}")  # â† åŠè§’ã‚¹ãƒšãƒ¼ã‚¹è¿½åŠ 

    headline_html = (
        "<div style='margin-bottom:20px'>"
        f"------- ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ ({len(summaries)}æœ¬) -------<br>"
        + "<br>".join(headlines)  # â† å„ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ”¹è¡Œã§è¡¨ç¤º
        + "</div><hr>"
    )

    # âœ… ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡å…¨ä½“ã®HTML
    html_content = """
    <html>
    <body style="font-family: Arial, sans-serif; background-color: #ffffff; color: #333333;">
    """

    # å…ˆé ­ã«ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³æŒ¿å…¥
    html_content += headline_html

    # è¨˜äº‹ã”ã¨ã®æœ¬æ–‡
    for media, articles in media_grouped.items():
        for item in articles:
            title_jp = item["title"]
            url = item["url"]
            summary_html = item["summary"]

            heading_html = (
                "<h2 style='margin-bottom:5px'>"
                f"{title_jp}ã€€"
                "<span style='font-size:0.83rem;font-weight:600'>"
                f"{media} "
                "</span>"
                "</h2>"
            )

            html_content += (
                "<div style='margin-bottom:20px'>"
                f"{heading_html}"
                "<div style='background-color:#f9f9f9;padding:10px;border-radius:8px'>"
                f"{summary_html}"
                "</div>"
                f"<p><a href='{url}' style='color:#1a0dab' target='_blank'>æœ¬æ–‡ã‚’èª­ã‚€</a></p>"
                "</div><hr style='border-top: 1px solid #cccccc;'>"
            )

    html_content += "</body></html>"
    html_content = clean_html_content(html_content)

    from_display_name = "Myanmar News Digest"

    subject = re.sub(r"[\r\n]+", " ", subject).strip()
    msg = EmailMessage(policy=SMTP)
    msg["Subject"] = subject
    msg["From"] = formataddr((str(Header(from_display_name, "utf-8")), sender_email))
    msg["To"] = ", ".join(recipient_emails)
    msg.set_content("HTMLãƒ¡ãƒ¼ãƒ«ã‚’é–‹ã‘ã‚‹ç’°å¢ƒã§ã”ç¢ºèªãã ã•ã„ã€‚", charset="utf-8")
    msg.add_alternative(html_content, subtype="html", charset="utf-8")

    try:
        service = _build_gmail_service()
        raw = base64.urlsafe_b64encode(msg.as_bytes()).decode("utf-8")
        body = {"raw": raw}
        sent = service.users().messages().send(userId="me", body=body).execute()
        print("âœ… Gmail API é€ä¿¡å®Œäº† messageId:", sent.get("id"))
    except HttpError as e:
        print(f"âŒ Gmail API ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    
    # ä»Šæ—¥ã®æ—¥ä»˜ã‚’ãƒŸãƒ£ãƒ³ãƒãƒ¼æ™‚é–“ã§å–å¾—
    date_mmt = get_today_date_mmt()
    seen_urls = set()

    # === Mizzima (Burmese) ===
    # print("=== Mizzima (Burmese) ===")
    # articles_mizzima = get_mizzima_articles_from_category(
    #     date_mmt,
    #     "https://bur.mizzima.com",
    #     "Mizzima (Burmese)",
    #     "/category/%e1%80%9e%e1%80%90%e1%80%84%e1%80%ba%e1%80%b8/%e1%80%99%e1%80%bc%e1%80%94%e1%80%ba%e1%80%99%e1%80%ac%e1%80%9e%e1%80%90%e1%80%84%e1%80%ba%e1%80%b8",
    #     max_pages=3,
    # )
    # process_and_enqueue_articles(
    #     articles_mizzima, 
    #     "Mizzima (Burmese)", 
    #     seen_urls, 
    #     trust_existing_body=True
    # )

    # print("=== BBC Burmese ===")
    # articles_bbc = get_bbc_burmese_articles_for(date_mmt)
    # process_and_enqueue_articles(
    #     articles_bbc, 
    #     "BBC Burmese", 
    #     seen_urls, 
    #     trust_existing_body=True
    # )

    print("=== Irrawaddy ===")
    articles_irrawaddy = get_irrawaddy_articles_for(date_mmt, debug=True)
    
    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆä»¶æ•°ï¼‹å…ˆé ­æ•°ä»¶ã‚’è¡¨ç¤ºï¼‰
    try:
        print(f"[irrawaddy] collected: {len(articles_irrawaddy)} items for {date_mmt}")
        for a in articles_irrawaddy[:10]:
            title = (a.get("title") or "").replace("\n", " ").strip()
            url = a.get("url", "")
            d = a.get("date", "")
            print(f"  - {d} | {title[:80]} | {url}")
        if len(articles_irrawaddy) > 10:
            print(f"  ... (+{len(articles_irrawaddy)-10} more)")
    except Exception:
        pass
    
    sys.exit(1)  # for debug
    
    # MEMO: ãƒ­ã‚°ç”¨ã€ãƒ‡ãƒãƒƒã‚¯ã§ãƒ­ã‚°ç¢ºèª
    # print("RESULTS:", json.dumps(articles_irrawaddy, ensure_ascii=False, indent=2))
    process_and_enqueue_articles(
        articles_irrawaddy,
        "Irrawaddy",
        seen_urls,
        bypass_keyword=True,  # â† Irrawaddyã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è½ã¨ã•ãªã„
        trust_existing_body=True,  # â† ã•ã£ãå…¥ã‚ŒãŸ body ã‚’ãã®ã¾ã¾ä½¿ã†ï¼ˆå†ãƒ•ã‚§ãƒƒãƒã—ãªã„ï¼‰
    )

    print("=== Khit Thit Media ===")
    articles_khit = get_khit_thit_media_articles_from_category(date_mmt, max_pages=3)
    process_and_enqueue_articles(
        articles_khit, 
        "Khit Thit Media", 
        seen_urls
    )

    print("=== DVB ===")
    articles_dvb = get_dvb_articles_for(date_mmt, debug=True)
    process_and_enqueue_articles(
        articles_dvb, 
        "DVB", 
        seen_urls, 
        trust_existing_body=True
    )
    
    print("=== Myanmar Now ===")
    articles_mn = get_myanmar_now_articles_mm(date_mmt, max_pages=3)
    process_and_enqueue_articles(
        articles_mn,
        "Myanmar Now",
        seen_urls,
        bypass_keyword=False,
        trust_existing_body=True,
    )

    # URLãƒ™ãƒ¼ã‚¹ã®é‡è¤‡æ’é™¤ã‚’å…ˆã«è¡Œã†
    print(f"âš™ï¸ Removing URL duplicates from {len(translation_queue)} articles...")
    translation_queue = deduplicate_by_url(translation_queue)

    # ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œ (5ä»¶ã”ã¨ã«1åˆ†å¾…æ©Ÿ)
    all_summaries = process_translation_batches(batch_size=TRANSLATION_BATCH_SIZE, wait_seconds=60)

    # ä»•æ§˜å¤‰æ›´: 2é€šé€ä¿¡ã«åˆ†é›¢
    # 1) ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ã®ã¿ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿é€ä¿¡ã€å†…éƒ¨å‘ã‘ï¼‰
    summaries_ayeyar_only = [s for s in all_summaries if s.get("is_ayeyar")]
    if summaries_ayeyar_only:
        send_email_digest(
            summaries_ayeyar_only,
            recipients_env="INTERNAL_EMAIL_RECIPIENTS",
            subject_suffix="/ (ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ã®ã¿)"
        )
    else:
        print("ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£è¨˜äº‹ãªã—: ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ã®ã¿ãƒ¡ãƒ¼ãƒ«ã¯é€ä¿¡ã—ã¾ã›ã‚“ã€‚")

    # 2) ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ä»¥å¤–ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ’ãƒƒãƒˆï¼ˆã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ã«è©²å½“ã—ãªã„ã‚‚ã®ã ã‘ï¼‰
    summaries_non_ayeyar = [
        s for s in all_summaries if s.get("hit_non_ayeyar") and not s.get("is_ayeyar")
    ]
    send_email_digest(
        summaries_non_ayeyar,
        recipients_env="EMAIL_RECIPIENTS",
        subject_suffix="/ (ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ä»¥å¤–)"
    )
