import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil.parser import parse as parse_date
import re

# Chat GPT
# from openai import OpenAI, OpenAIError
import smtplib
import os
import sys
from email.message import EmailMessage
from email.policy import SMTPUTF8
from email.utils import formataddr
import unicodedata
from google import genai
from collections import defaultdict
import time
import json
import pprint as _pprint

import random

try:
    import httpx
except Exception:
    httpx = None
try:
    import urllib3
except Exception:
    urllib3 = None

try:
    from google.api_core.exceptions import (
        ServiceUnavailable,
        ResourceExhausted,
        DeadlineExceeded,
    )
except Exception:
    ServiceUnavailable = ResourceExhausted = DeadlineExceeded = Exception


# Geminiæœ¬ç•ªç”¨
client_summary = genai.Client(api_key=os.getenv("GEMINI_API_SUMMARY_KEY"))
client_dedupe = genai.Client(api_key=os.getenv("GEMINI_API_DEDUPE_KEY"))


# Chat GPT
# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def _is_retriable_exc(e: Exception) -> bool:
    msg = (str(e) or "").lower()
    name = e.__class__.__name__.lower()

    # Googleç³»ã®æ˜ç¤ºçš„ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡
    if isinstance(e, (ServiceUnavailable, ResourceExhausted, DeadlineExceeded)):
        return True

    # httpx/urllib3ç³»ï¼ˆç’°å¢ƒã«ç„¡ã‘ã‚Œã°ç„¡è¦–ï¼‰
    if httpx and isinstance(
        e,
        (
            getattr(httpx, "RemoteProtocolError", Exception),
            getattr(httpx, "ReadTimeout", Exception),
            getattr(httpx, "ConnectError", Exception),
        ),
    ):
        return True
    if urllib3 and isinstance(
        e,
        (
            urllib3.exceptions.ProtocolError,
            urllib3.exceptions.ReadTimeoutError,
            urllib3.exceptions.MaxRetryError,
        ),
    ):
        return True

    # æ–‡å­—åˆ—ã§ã®åˆ¤å®šï¼ˆå®Ÿè£…å·®åˆ†å¸åï¼‰
    hints = [
        "remoteprotocolerror",
        "server disconnected",
        "unavailable",
        "503",
        "502",
        "504",
        "gateway",
        "timeout",
        "temporar",
        "overload",
    ]
    if any(h in msg or h in name for h in hints):
        return True
    return False


def call_gemini_with_retries(
    client,
    prompt,
    model="gemini-2.5-flash",
    max_retries=5,
    base_delay=2.0,
    max_delay=30.0,
):
    delay = base_delay
    for attempt in range(1, max_retries + 1):
        try:
            return client.models.generate_content(model=model, contents=prompt)
        except Exception as e:
            if not _is_retriable_exc(e) or attempt == max_retries:
                raise
            print(
                f"âš ï¸ Gemini retry {attempt}/{max_retries} after: {e.__class__.__name__} | {e}"
            )
            # ã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
            time.sleep(min(max_delay, delay) + random.random() * 0.5)
            delay *= 2


# è¦ç´„ç”¨ã«é€ã‚‹æœ¬æ–‡ã®æœ€å¤§æ–‡å­—æ•°ï¼ˆå›ºå®šï¼‰
# Irrawaddyè‹±èªè¨˜äº‹ãŒ3500æ–‡å­—ãã‚‰ã„ã‚ã‚‹
BODY_MAX_CHARS = 3500

# ãƒŸãƒ£ãƒ³ãƒãƒ¼æ¨™æº–æ™‚ (UTC+6:30)
MMT = timezone(timedelta(hours=6, minutes=30))


# ä»Šæ—¥ã®æ—¥ä»˜
# ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é€Ÿå ±æ€§é‡è¦–ã§ä»Šæ—¥åˆ†ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã®æ–¹é‡
def get_today_date_mmt():
    # æœ¬ç•ªç”¨ã€ä»Šæ—¥ã®æ—¥ä»˜
    now_mmt = datetime.now(MMT)
    return now_mmt.date()


# å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆå…¨ãƒ¡ãƒ‡ã‚£ã‚¢å…±é€šã§ä½¿ç”¨ã™ã‚‹ï¼‰
NEWS_KEYWORDS = [
    # ãƒŸãƒ£ãƒ³ãƒãƒ¼ï¼ˆå›½åãƒ»ç¾è¡Œåç§°ï¼‰
    "á€™á€¼á€”á€ºá€™á€¬",
    "á€™á€¼á€”á€ºá€™á€¬á€·",
    "Myanmar",
    "myanmar",
    # ãƒ“ãƒ«ãƒï¼ˆæ—§å›½åãƒ»é€šç§°ï¼‰
    "á€—á€™á€¬",
    "Burma",
    "burma",
    # ã‚¢ã‚¦ãƒ³ã‚µãƒ³ã‚¹ãƒ¼ãƒãƒ¼ï¼ˆAung San Suu Kyiï¼‰
    "á€¡á€±á€¬á€„á€ºá€†á€”á€ºá€¸á€…á€¯á€€á€¼á€Šá€º",
    "Aung San Suu Kyi",
    "aung san suu kyi",
    # ãƒŸãƒ³ã‚¢ã‚¦ãƒ³ãƒ•ãƒ©ã‚¤ãƒ³ï¼ˆMin Aung Hlaingï¼‰
    "á€™á€„á€ºá€¸á€¡á€±á€¬á€„á€ºá€œá€¾á€­á€¯á€„á€º",
    "Min Aung Hlaing",
    "min aung hlaing",
    # ãƒãƒ£ãƒƒãƒˆï¼ˆKyatï¼‰
    "Kyat",
    "kyat",
    # å¾´å…µåˆ¶ï¼ˆConscription / Military Draftï¼‰, å¾´å…µ, å…µå½¹
    "á€…á€…á€ºá€á€•á€ºá€á€„á€ºá€á€±á€«á€ºá€›á€±á€¸",
    "á€…á€…á€ºá€™á€¾á€¯á€‘á€™á€ºá€¸",
    "á€¡á€á€„á€ºá€¸á€á€•á€ºá€á€„á€ºá€á€±á€«á€ºá€á€¼á€„á€ºá€¸",
    "á€á€•á€ºá€á€„á€ºá€á€±á€«á€º ",
    "Conscription",
    "conscription",
    "Military Draft",
    "Military draft",
    "military draft",
    "Military Service",
    "Military service",
    "military service",
    # ãƒ­ãƒ’ãƒ³ã‚®ãƒ£
    "á€›á€­á€¯á€Ÿá€„á€ºá€‚á€»á€¬",
    "Rohingya",
    "rohingya",
    # å›½å¢ƒè²¿æ˜“ãƒ»äº¤æ˜“
    "á€”á€šá€ºá€…á€•á€ºá€€á€¯á€”á€ºá€á€½á€šá€ºá€›á€±á€¸",
    # ãƒ¤ãƒ³ã‚´ãƒ³ç®¡åŒº
    "á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸",
    "Yangon Region",
    "Yangon region",
    "yangon region",
    # ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç®¡åŒº
    "á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸",
    "Ayeyarwady Region",
    "Ayeyarwady region",
    "ayeyarwady region",
]

# Unicodeæ­£è¦åŒ–ï¼ˆNFCï¼‰ã‚’é©ç”¨
NEWS_KEYWORDS = [unicodedata.normalize("NFC", kw) for kw in NEWS_KEYWORDS]

# ãƒãƒ£ãƒƒãƒˆã¯æ•°å­—ã«ç¶šãã‚‚ã®ã®ã¿ï¼ˆé€šè²¨åˆ¤å®šï¼‰
KYAT_PATTERN = re.compile(r"(?<=[0-9á€-á‰])[\s,\.]*(?:á€á€±á€¬á€„á€ºá€¸|á€á€­á€”á€ºá€¸|á€á€”á€ºá€¸)?\s*á€€á€»á€•á€º")


def any_keyword_hit(title: str, body: str) -> bool:
    # é€šå¸¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´
    if any(kw in title or kw in body for kw in NEWS_KEYWORDS):
        return True
    # é€šè²¨ã€Œá€€á€»á€•á€ºã€ã ã‘ã¯æ­£è¦è¡¨ç¾ã§åˆ¤å®š
    if KYAT_PATTERN.search(title) or KYAT_PATTERN.search(body):
        return True
    return False


def clean_html_content(html: str) -> str:
    html = html.replace("\xa0", " ").replace("&nbsp;", " ")
    # åˆ¶å¾¡æ–‡å­—ï¼ˆã‚«ãƒ†ã‚´ãƒªCï¼‰ã‚’é™¤å¤–ã€å¯è¦–Unicodeã¯ãã®ã¾ã¾
    return "".join(c for c in html if unicodedata.category(c)[0] != "C")


# æœ¬æ–‡ãŒå–å¾—ã§ãã‚‹ã¾ã§ã€Œrequestsã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€
def fetch_with_retry(url, retries=3, wait_seconds=2):
    for attempt in range(retries):
        try:
            res = requests.get(url, timeout=10)
            if res.status_code == 200 and res.text.strip():
                return res
        except Exception as e:
            print(f"Attempt {attempt + 1} failed for {url}: {e}")
        time.sleep(wait_seconds)
    raise Exception(f"Failed to fetch {url} after {retries} attempts.")


# æœ¬æ–‡ãŒç©ºãªã‚‰ã€Œä¸€å®šç§’æ•°å¾…ã£ã¦å†å–å¾—ã€
def extract_paragraphs_with_wait(soup_article, retries=2, wait_seconds=2):
    for attempt in range(retries + 1):
        paragraphs = soup_article.select("div.entry-content p")
        if not paragraphs:
            paragraphs = soup_article.select("div.node-content p")
        if not paragraphs:
            paragraphs = soup_article.select("article p")
        if not paragraphs:
            paragraphs = soup_article.find_all("p")

        if paragraphs:
            return paragraphs

        print(f"Paragraphs not found, waiting {wait_seconds}s and retrying...")
        time.sleep(wait_seconds)
    return []


# === æ±ç”¨ã® <p> æŠ½å‡ºå™¨ï¼ˆã‚µã‚¤ãƒˆå…±é€šï¼‰ ===
def extract_body_generic_from_soup(soup):
    for sel in ["div.entry-content p", "div.node-content p", "article p"]:
        ps = soup.select(sel)
        if ps:
            break
    else:
        ps = soup.find_all("p")
    txts = [p.get_text(strip=True) for p in ps if p.get_text(strip=True)]
    return "\n".join(txts).strip()


# === requests ã‚’ä½¿ã†ã‚·ãƒ³ãƒ—ãƒ«ãª fetch_onceï¼ˆ1å›ï¼‰ ===
def fetch_once_requests(url, timeout=15):
    r = requests.get(url, timeout=timeout)
    r.raise_for_status()
    # æ–‡å­—åŒ–ã‘å›é¿ã®ãŸã‚ bytes ã‚’è¿”ã™ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ã¯ BeautifulSoup ã«ä»»ã›ã‚‹ï¼‰
    return r.content


# === å†ãƒ•ã‚§ãƒƒãƒä»˜ããƒ»æœ¬æ–‡å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===
def get_body_with_refetch(
    url, fetcher, extractor, retries=3, wait_seconds=2, quiet=False
):
    """
    fetcher(url) -> html(bytes or str)
    extractor(soup) -> body(str)
    """
    last_err = None
    for attempt in range(retries + 1):
        try:
            html = fetcher(url)
            # bytes/str ã©ã¡ã‚‰ã§ã‚‚ BeautifulSoup ã«æ¸¡ã›ã‚‹
            soup = BeautifulSoup(html, "html.parser")

            # èª¤ã£ã¦ latin-1 ç³»ã§è§£é‡ˆã•ã‚ŒãŸå ´åˆã¯ UTF-8 ã§å†è§£é‡ˆã—ã¦ä¿é™ºã‚’ã‹ã‘ã‚‹
            enc = (getattr(soup, "original_encoding", None) or "").lower()
            if enc in ("iso-8859-1", "latin-1", "windows-1252"):
                soup = BeautifulSoup(html, "html.parser", from_encoding="utf-8")

            body = extractor(soup)
            if body:
                return unicodedata.normalize("NFC", body)

            if not quiet:
                print(f"[refetch] body empty, retrying {attempt+1}/{retries} â†’ {url}")
        except Exception as e:
            last_err = e
            if not quiet:
                print(f"[refetch] EXC {attempt+1}/{retries}: {e} â†’ {url}")
        time.sleep(wait_seconds)

    if not quiet and last_err:
        print(f"[refetch] give up after {retries+1} tries â†’ {url}")
    return ""


# === Irrawaddyå°‚ç”¨ ===
# æœ¬æ–‡ãŒå–å¾—ã§ãã‚‹ã¾ã§ã€Œrequestsã§ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€
def fetch_with_retry_irrawaddy(url, retries=3, wait_seconds=2, session=None):
    """
    Irrawaddyå°‚ç”¨ãƒ•ã‚§ãƒƒãƒãƒ£ï¼šæœ€åˆã‹ã‚‰ cloudscraper ã§å–å¾—ã—ã€403/429/503 ã¯æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§å†è©¦è¡Œã€‚
    æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ requests ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã»ã¼åˆ°é”ã—ãªã„æƒ³å®šï¼‰ã€‚
    """
    import random

    try:
        import cloudscraper
    except ImportError:
        raise RuntimeError(
            "cloudscraper ãŒå¿…è¦ã§ã™ã€‚pip install cloudscraper ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    sess = session or requests.Session()

    UA = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    )
    HEADERS = {
        "User-Agent": UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": "https://www.irrawaddy.com/",
        "Connection": "keep-alive",
    }

    # cloudscraper ã‚’æœ€åˆã«ä½¿ã†ï¼ˆæ—¢å­˜ Session ã‚’ãƒ©ãƒƒãƒ—ã—ã¦ã‚¯ãƒƒã‚­ãƒ¼å…±æœ‰ï¼‰
    scraper = cloudscraper.create_scraper(
        sess=sess,
        browser={"browser": "chrome", "platform": "windows", "mobile": False},
    )

    for attempt in range(retries):
        try:
            r = scraper.get(url, headers=HEADERS, timeout=30, allow_redirects=True)
            # print(
            #     f"[fetch-cs] {attempt + 1}/{retries}: HTTP {r.status_code} len={len(getattr(r, 'text', ''))} â†’ {url}"
            # )
            if r.status_code == 200 and getattr(r, "text", "").strip():
                return r
            if r.status_code in (403, 429, 503):
                time.sleep(wait_seconds * (2**attempt) + random.uniform(0, 0.8))
                continue
            break
        except Exception as e:
            print(f"[fetch-cs] {attempt + 1}/{retries} EXC: {e} â†’ {url}")
            time.sleep(wait_seconds * (2**attempt) + random.uniform(0, 0.8))

    # éå¸¸ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã»ã¼ä¸è¦ï¼‰ã€‚æˆåŠŸã™ã‚Œã°è¿”ã™ã€‚
    try:
        r2 = sess.get(url, headers=HEADERS, timeout=20, allow_redirects=True)
        print(f"[fetch-rq] final: HTTP {r2.status_code} len={len(r2.text)} â†’ {url}")
        if r2.status_code == 200 and r2.text.strip():
            return r2
    except Exception as e:
        print(f"[fetch-rq] EXC final: {e} â†’ {url}")

    raise Exception(f"Failed to fetch {url} after {retries} attempts.")


def _norm_text(text: str) -> str:
    return unicodedata.normalize("NFC", text)


def _parse_category_date_text(text: str):
    # ä¾‹: 'August 9, 2025'
    text = re.sub(r"\s+", " ", text.strip())
    return datetime.strptime(text, "%B %d, %Y").date()


def _article_date_from_meta_mmt(soup):
    meta = soup.find("meta", attrs={"property": "article:published_time"})
    if not meta or not meta.get("content"):
        return None
    iso = meta["content"].replace("Z", "+00:00")  # æœ«å°¾Zå¯¾ç­–
    dt = datetime.fromisoformat(iso)
    return dt.astimezone(MMT).date()


def _extract_title(soup):
    t = soup.find("title")
    return _norm_text(t.get_text(strip=True)) if t else None


def _is_excluded_by_ancestor(node) -> bool:
    excluded = {
        "jnews_inline_related_post",
        "jeg_postblock_21",
        "widget",
        "widget_jnews_popular",
        "jeg_postblock_5",
        "jnews_related_post_container",
        "widget widget_jnews_popular",
        "jeg_footer_primary clearfix",
    }
    for anc in node.parents:
        classes = anc.get("class", [])
        if any(c in excluded for c in classes):
            return True
    return False


# æœ¬æ–‡æŠ½å‡º
def extract_body_irrawaddy(soup):
    # <div class="content-inner "> é…ä¸‹ã® <p>ã®ã¿ï¼ˆé™¤å¤–ãƒ–ãƒ­ãƒƒã‚¯é…ä¸‹ã¯é™¤å¤–ï¼‰
    paragraphs = []
    content_inners = soup.select("div.content-inner")
    if not content_inners:
        content_inners = [
            div
            for div in soup.find_all("div")
            if "content-inner" in (div.get("class") or [])
        ]
    for root in content_inners:
        for p in root.find_all("p"):
            if _is_excluded_by_ancestor(p):
                continue
            txt = p.get_text(strip=True)
            if txt:
                paragraphs.append(_norm_text(txt))
    return "\n".join(paragraphs).strip()


#  Irrawaddy ç”¨ fetch_onceï¼ˆæ—¢å­˜ã® fetch_with_retry_irrawaddy ã‚’1å›ãƒ©ãƒƒãƒ—ï¼‰
def fetch_once_irrawaddy(url, session=None):
    r = fetch_with_retry_irrawaddy(url, retries=1, wait_seconds=0, session=session)
    # cloudscraper ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚‚ bytes ã‚’è¿”ã™ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ã¯ BeautifulSoup ã«ä»»ã›ã‚‹ï¼‰
    return r.content


# === ã“ã“ã¾ã§ ===


# ===== ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªãƒ’ãƒƒãƒˆæ™‚ã®å…±é€šãƒ­ã‚¬ãƒ¼ï¼ˆç°¡ç´ ç‰ˆï¼‰ =====
LOG_NO_KEYWORD_MISSES = True


def log_no_keyword_hit(source: str, url: str, title: str, body: str, stage: str):
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªãƒ’ãƒƒãƒˆã®è¨˜äº‹ã‚’æ¨™æº–å‡ºåŠ›ã«å‡ºã™ï¼ˆstageãƒ»æœ¬æ–‡æŠœç²‹ã¯å‡ºåŠ›ã—ãªã„ï¼‰ã€‚
    """
    if not LOG_NO_KEYWORD_MISSES:
        return
    try:
        title = unicodedata.normalize("NFC", title or "")
    except Exception:
        pass

    print("\n----- NO KEYWORD HIT -----")
    print(f"[source] {source}")
    print(f"[url]    {url}")
    print(f"[title]  {title}")
    print("----- END NO KEYWORD HIT -----\n")


# Mizzimaã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸å·¡å›ã§å–å¾—
def get_mizzima_articles_from_category(
    date_obj, base_url, source_name, category_path, max_pages=3
):
    # ==== ãƒ­ãƒ¼ã‚«ãƒ«å®šæ•° Mizzimaé™¤å¤–å¯¾è±¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼‰====
    EXCLUDE_TITLE_KEYWORDS = [
        # æ˜¥ã®é©å‘½æ—¥èªŒ
        "á€”á€½á€±á€¦á€¸á€á€±á€¬á€ºá€œá€¾á€”á€ºá€›á€±á€¸ á€”á€±á€·á€…á€‰á€ºá€™á€¾á€á€ºá€…á€¯",
        # å†™çœŸãƒ‹ãƒ¥ãƒ¼ã‚¹
        "á€“á€¬á€á€ºá€•á€¯á€¶á€á€á€„á€ºá€¸",
    ]

    article_urls = []

    for page_num in range(1, max_pages + 1):
        if page_num == 1:
            url = f"{base_url}{category_path}"
        else:
            url = f"{base_url}{category_path}/page/{page_num}/"

        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                continue

            soup = BeautifulSoup(res.content, "html.parser")
            links = [
                a["href"]
                for a in soup.select("main.site-main article a.post-thumbnail[href]")
            ]
            article_urls.extend(links)

        except Exception as e:
            print(f"Error crawling category page {url}: {e}")
            continue

    filtered_articles = []
    for url in article_urls:
        try:
            res_article = fetch_with_retry(url)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            meta_tag = soup_article.find("meta", property="article:published_time")
            if not meta_tag or not meta_tag.has_attr("content"):
                continue

            date_str = meta_tag["content"]
            article_datetime_utc = datetime.fromisoformat(date_str)
            article_datetime_mmt = article_datetime_utc.astimezone(MMT)
            article_date = article_datetime_mmt.date()

            if article_date != date_obj:
                continue

            title_tag = soup_article.find("meta", attrs={"property": "og:title"})
            if not title_tag or not title_tag.has_attr("content"):
                continue
            title = title_tag["content"].strip()

            # === é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã‚’NFCæ­£è¦åŒ–ã—ã¦ã‹ã‚‰ï¼‰ ===
            title_nfc = unicodedata.normalize("NFC", title)
            if any(kw in title_nfc for kw in EXCLUDE_TITLE_KEYWORDS):
                print(f"SKIP: excluded keyword in title â†’ {url} | TITLE: {title_nfc}")
                continue

            content_div = soup_article.find("div", class_="entry-content")
            if not content_div:
                continue

            paragraphs = []
            for p in content_div.find_all("p"):
                if p.find_previous("h2", string=re.compile("Related Posts", re.I)):
                    break
                paragraphs.append(p)

            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            body_text = unicodedata.normalize("NFC", body_text)

            if not body_text.strip():
                continue

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šã¯æ­£è¦åŒ–æ¸ˆã¿ã‚¿ã‚¤ãƒˆãƒ«ã§è¡Œã†
            if not any_keyword_hit(title, body_text):
                log_no_keyword_hit(
                    source_name, url, title, body_text, "mizzima:category"
                )
                continue

            filtered_articles.append(
                {
                    "source": source_name,
                    "url": url,
                    "title": title,
                    "date": article_date.isoformat(),
                }
            )

        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return filtered_articles


# BCCã¯RSSã‚ã‚‹ã®ã§ãã‚Œä½¿ã†
def get_bbc_burmese_articles_for(target_date_mmt):
    # ==== ãƒ­ãƒ¼ã‚«ãƒ«å®šæ•° ====
    NOISE_PATTERNS = [
        r"BBC\s*News\s*á€™á€¼á€”á€ºá€™á€¬",  # å›ºå®šç½²åï¼ˆBurmeseè¡¨è¨˜ï¼‰
        r"BBC\s*Burmese",  # è‹±èªè¡¨è¨˜
    ]

    # ==== ãƒ­ãƒ¼ã‚«ãƒ«é–¢æ•° ====
    def _remove_noise_phrases(text: str) -> str:
        """BBCç½²åãªã©ã®ãƒã‚¤ã‚ºãƒ•ãƒ¬ãƒ¼ã‚ºã‚’é™¤å»"""
        if not text:
            return text
        for pat in NOISE_PATTERNS:
            text = re.sub(pat, "", text, flags=re.IGNORECASE)
        return text.strip()

    # MEMO: ãƒ­ã‚°ç”¨
    # ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆä¸­ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã“ã«ãƒ’ãƒƒãƒˆã—ãŸã‹ã‚’è¿”ã™ï¼ˆå‘¨è¾ºæ–‡è„ˆã¤ãï¼‰
    # def _find_hits(text: str, keywords):
    #     hits = []
    #     for kw in keywords:
    #         start = 0
    #         while True:
    #             i = text.find(kw, start)
    #             if i == -1:
    #                 break
    #             s = max(0, i - 30)
    #             e = min(len(text), i + len(kw) + 30)
    #             ctx = text[s:e].replace("\n", " ")
    #             hits.append({"kw": kw, "pos": i, "ctx": ctx})
    #             start = i + len(kw)
    #     return hits

    rss_url = "https://feeds.bbci.co.uk/burmese/rss.xml"
    session = requests.Session()

    try:
        res = session.get(rss_url, timeout=10)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

    soup = BeautifulSoup(res.content, "xml")
    articles = []

    for item in soup.find_all("item"):
        pub_date_tag = item.find("pubDate")
        if not pub_date_tag:
            continue

        # RSSã¯UTC â†’ MMTã¸å¤‰æ›ã—ã€å¯¾è±¡æ—¥ã ã‘é€šã™
        try:
            pub_date = parse_date(pub_date_tag.text)
            pub_date_mmt = pub_date.astimezone(MMT).date()
        except Exception as e:
            print(f"âŒ pubDate parse error: {e}")
            continue

        if pub_date_mmt != target_date_mmt:
            continue

        title = (
            (item.find("title") or {}).get_text(strip=True)
            if item.find("title")
            else ""
        )
        link = (
            (item.find("link") or {}).get_text(strip=True) if item.find("link") else ""
        )
        if not link:
            continue

        try:
            article_res = session.get(link, timeout=10)
            article_res.raise_for_status()
            article_soup = BeautifulSoup(article_res.content, "html.parser")

            # ===== ã“ã“ã§é™¤å¤–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã¾ã¨ã‚ã¦å‰Šé™¤ =====
            # è¨˜äº‹ç½²åã‚„ãƒ¡ã‚¿æƒ…å ±
            for node in article_soup.select(
                'section[role="region"][aria-labelledby="article-byline"]'
            ):
                node.decompose()
            # ã€ŒãŠã™ã™ã‚ï¼æœ€ã‚‚èª­ã¾ã‚ŒãŸã€ãƒ–ãƒ­ãƒƒã‚¯
            for node in article_soup.select(
                'section[data-e2e="recommendations-heading"][role="region"]'
            ):
                node.decompose()
            # ã¤ã„ã§ã«ãƒ˜ãƒƒãƒ€ãƒ¼/ãƒŠãƒ“/ãƒ•ãƒƒã‚¿ãƒ¼ç­‰ã®ãƒã‚¤ã‚ºã‚‚è½ã¨ã—ã¦ãŠãï¼ˆä»»æ„ï¼‰
            for node in article_soup.select(
                'header[role="banner"], nav[role="navigation"], footer[role="contentinfo"], aside'
            ):
                node.decompose()
            # ============================================

            # æœ¬æ–‡ã¯ main å†…ã® <p> ã«é™å®š
            main = article_soup.select_one('main[role="main"]') or article_soup
            paragraphs = [p.get_text(strip=True) for p in main.find_all("p")]
            # ç©ºè¡Œã‚„ãƒã‚¤ã‚ºã‚’å‰Šã‚‹
            paragraphs = [t for t in paragraphs if t]
            body_text = "\n".join(paragraphs)

            # ãƒŸãƒ£ãƒ³ãƒãƒ¼æ–‡å­—ã®åˆæˆå·®ç•°ã‚’é¿ã‘ã‚‹ãŸã‚ NFC æ­£è¦åŒ–
            title_nfc = unicodedata.normalize("NFC", title)
            title_nfc = _remove_noise_phrases(title_nfc)
            body_text_nfc = unicodedata.normalize("NFC", body_text)
            body_text_nfc = _remove_noise_phrases(body_text_nfc)

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
            if not any_keyword_hit(title_nfc, body_text_nfc):
                log_no_keyword_hit(
                    "BBC Burmese", link, title_nfc, body_text_nfc, "bbc:article"
                )
                continue

            # MEMO: ãƒ­ã‚°ç”¨ã€=== ãƒ‡ãƒãƒƒã‚°: åˆ¤å®šå‰ã«ã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡ã®è¦ç´„ã‚’å‡ºã™ ===
            # print("----- DEBUG CANDIDATE -----")
            # print("URL:", link)
            # print("TITLE:", repr(title_nfc))
            # print("BODY_HEAD:", repr(body_text_nfc[:500]))
            # print("BODY_LEN:", len(body_text_nfc))

            # # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆãƒ’ãƒƒãƒˆè©³ç´°ã‚‚å–ã‚‹ï¼‰
            # title_hits = _find_hits(title_nfc, NEWS_KEYWORDS)
            # body_hits  = _find_hits(body_text_nfc, NEWS_KEYWORDS)
            # total_hits = title_hits + body_hits

            # if not total_hits:
            #     print("SKIP: no keyword hits.")
            #     continue

            # # === ãƒ‡ãƒãƒƒã‚°: ã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã“ã§å½“ãŸã£ãŸã‹ ===
            # print("HITS:", len(total_hits))
            # if title_hits:
            #     print(" - in TITLE:")
            #     for h in title_hits[:10]:
            #         print(f"   kw={repr(h['kw'])} ctx=â€¦{h['ctx']}â€¦")
            # if body_hits:
            #     print(" - in BODY:")
            #     for h in body_hits[:10]:  # é•·ããªã‚‹ã®ã§æœ€å¤§10ä»¶
            #         print(f"   kw={repr(h['kw'])} ctx=â€¦{h['ctx']}â€¦")

            print(f"âœ… æŠ½å‡ºè¨˜äº‹: {title_nfc} ({link})")
            articles.append(
                {
                    "title": title_nfc,
                    "url": link,
                    "date": pub_date_mmt.isoformat(),
                }
            )

        except Exception as e:
            print(f"âŒ è¨˜äº‹å–å¾—/è§£æã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return articles


# khit_thit_mediaã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸å·¡å›ã§å–å¾—
def get_khit_thit_media_articles_from_category(date_obj, max_pages=3):
    base_url = "https://yktnews.com/category/news/"
    article_urls = []

    for page in range(1, max_pages + 1):
        url = f"{base_url}page/{page}/" if page > 1 else base_url
        print(f"Fetching {url}")
        res = fetch_with_retry(url)
        soup = BeautifulSoup(res.content, "html.parser")

        # è¨˜äº‹ãƒªãƒ³ã‚¯æŠ½å‡º
        entry_links = soup.select("p.entry-title.td-module-title a[href]")
        page_article_urls = [a["href"] for a in entry_links if a.has_attr("href")]
        article_urls.extend(page_article_urls)

    filtered_articles = []
    for url in article_urls:
        try:
            res_article = fetch_with_retry(url)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            # æ—¥ä»˜å–å¾—
            meta_tag = soup_article.find("meta", property="article:published_time")
            if not meta_tag or not meta_tag.has_attr("content"):
                continue
            date_str = meta_tag["content"]
            article_datetime_utc = datetime.fromisoformat(date_str)
            article_datetime_mmt = article_datetime_utc.astimezone(MMT)
            article_date = article_datetime_mmt.date()

            if article_date != date_obj:
                continue  # å¯¾è±¡æ—¥ã§ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—

            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_tag = soup_article.find("h1")
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)

            # æœ¬æ–‡å–å¾— (khit_thit_ediaç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³)
            paragraphs = extract_paragraphs_with_wait(soup_article)
            body_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            body_text = unicodedata.normalize("NFC", body_text)

            if not body_text.strip():
                continue  # æœ¬æ–‡ãŒç©ºãªã‚‰ã‚¹ã‚­ãƒƒãƒ—

            if not any_keyword_hit(title, body_text):
                log_no_keyword_hit(
                    "Khit Thit Media", url, title, body_text, "khitthit:category"
                )
                continue  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç„¡ã—ã¯é™¤å¤–

            filtered_articles.append(
                {"url": url, "title": title, "date": date_obj.isoformat()}
            )

        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return filtered_articles


# irrawaddy
def get_irrawaddy_articles_for(date_obj, debug=True):
    """
    æŒ‡å®šã® Irrawaddy ã‚«ãƒ†ã‚´ãƒªURLç¾¤ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ï¼‰ã‚’1å›ãšã¤å·¡å›ã—ã€
    MMTã®æŒ‡å®šæ—¥(æ—¢å®š: ä»Šæ—¥)ã‹ã¤ any_keyword_hit ã«ãƒ’ãƒƒãƒˆã™ã‚‹è¨˜äº‹ã®ã¿è¿”ã™ã€‚

    - /category/news/asia, /category/news/world ã¯é™¤å¤–ï¼ˆå…ˆé ­ä¸€è‡´ãƒ»å¤§å°ç„¡è¦–ï¼‰
    - ä¸€è¦§ã§ã¯ã€Œæ™‚è¨ˆã‚¢ã‚¤ã‚³ãƒ³ä»˜ãã®æ—¥ä»˜ãƒªãƒ³ã‚¯ã€ã‹ã‚‰å½“æ—¥å€™è£œã‚’æŠ½å‡º
    - è¨˜äº‹å´ã§ã¯ <meta property="article:published_time"> ã‚’ MMT ã«å¤‰æ›ã—ã¦å†ç¢ºèª
    - æœ¬æ–‡ã¯ <div class="content-inner "> é…ä¸‹ã® <p> ã‹ã‚‰æŠ½å‡ºï¼ˆç‰¹å®šãƒ–ãƒ­ãƒƒã‚¯é…ä¸‹ã¯é™¤å¤–ï¼‰
    è¿”ã‚Šå€¤: [{url, title, date}]
    ä¾å­˜: MMT, get_today_date_mmt, fetch_with_retry, any_keyword_hit
    """

    session = requests.Session()

    # ==== å·¡å›å¯¾è±¡ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã€é‡è¤‡ã‚ã‚Šã§ã‚‚OKï¼šå†…éƒ¨ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰ ====
    CATEGORY_PATHS_RAW = [
        "/category/news/",
        "/category/politics",
        "/category/news/war-against-the-junta",
        "/category/news/conflicts-in-numbers",
        "/category/news/junta-crony",
        "/category/news/ethnic-issues",
        "/category/business",
        "/category/business/economy",
        "/category/Features",
        "/category/Opinion",
        "/category/Opinion/editorial",
        "/category/Opinion/commentary",
        "/category/Opinion/guest-column",
        "/category/Opinion/analysis",
        "/category/in-person",
        "/category/in-person/interview",
        "/category/in-person/profile",
        "/category/Specials",
        "/category/specials/women",
        "/category/from-the-archive",
        "/category/Specials/myanmar-china-watch",
        # "/category/Video" # é™¤å¤–ä¾é ¼æœ‰
        # "/category/culture/books" #é™¤å¤–ä¾é ¼æœ‰
        # "/category/Cartoons" # é™¤å¤–ä¾é ¼æœ‰
        # "/category/election-2020", # 2021å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Opinion/letters", # 2014å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Dateline", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/specials/places-in-history", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/specials/on-this-day", # 2023å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Specials/myanmar-covid-19", # 2022å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Travel", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle/Food", # 2020å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/Lifestyle/fashion-design", # 2019å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/photo", # 2016å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
        # "/category/photo-essay", # 2021å¹´ã§æ›´æ–°æ­¢ã¾ã£ã¦ã‚‹
    ]
    BASE = "https://www.irrawaddy.com"
    EXCLUDE_PREFIXES = [
        "/category/news/asia",  # é™¤å¤–ä¾é ¼æœ‰
        "/category/news/world",  # é™¤å¤–ä¾é ¼æœ‰
    ]  # å…ˆé ­ä¸€è‡´ãƒ»å¤§å°ç„¡è¦–

    # ==== æ­£è¦åŒ–ãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ãƒ»é™¤å¤– ====
    def norm(p: str) -> str:
        return re.sub(r"/{2,}", "/", p.strip())

    paths, seen = [], set()
    for p in CATEGORY_PATHS_RAW:
        q = norm(p)
        if any(q.lower().startswith(x) for x in EXCLUDE_PREFIXES):
            continue
        if q not in seen:
            seen.add(q)
            paths.append(q)

    # 2) ç°¡æ˜“ãƒ­ã‚¬ãƒ¼ï¼ˆæ¶ˆã™æ™‚ã¯ã“ã®1è¡Œã¨ dbg(...) ã‚’æ¶ˆã™ã ã‘ï¼‰
    # dbg = (lambda *a, **k: print(*a, **k)) if debug else (lambda *a, **k: None)

    # MEMO: ãƒ­ã‚°ç”¨
    # results = []
    # seen_urls = set()
    # candidate_urls = []

    # # ==== 1) ã‚«ãƒ†ã‚´ãƒªå·¡å› ====
    # for rel_path in paths:
    #     url = f"{BASE}{rel_path}"
    #     print(f"Fetching {url}")
    #     try:
    #         res = fetch_with_retry_irrawaddy(url, session=session)
    #     except Exception as e:
    #         print(f"Error fetching {url}: {e}")
    #         continue

    #     soup = BeautifulSoup(res.content, "html.parser")
    #     wrapper = soup.select_one("div.jeg_content")  # ãƒ†ãƒ¼ãƒã«ã‚ˆã£ã¦ã¯ç„¡ã„ã“ã¨ã‚‚ã‚ã‚‹

    #     # âœ… union æ–¹å¼ï¼šwrapper å†…â†’è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒšãƒ¼ã‚¸å…¨ä½“ã®é †ã§æ¢ç´¢
    #     scopes = ([wrapper] if wrapper else []) + [soup]

    #     for scope in scopes:
    #         # ãƒ’ãƒ¼ãƒ­ãƒ¼æ ï¼‹é€šå¸¸ãƒªã‚¹ãƒˆï¼‹æ±ç”¨ãƒ¡ã‚¿ã‚’ä¸€ç™ºã§æ‹¾ã†
    #         links = scope.select(
    #             ".jnews_category_hero_container .jeg_meta_date a[href], "
    #             "div.jeg_postblock_content .jeg_meta_date a[href], "
    #             ".jeg_post_meta .jeg_meta_date a[href]"
    #         )
    #         # æ™‚è¨ˆã‚¢ã‚¤ã‚³ãƒ³ä»˜ãã ã‘ã«é™å®šï¼ˆãƒã‚¤ã‚ºå›é¿ï¼‰
    #         links = [a for a in links if a.find("i", class_="fa fa-clock-o")]

    #         # ï¼ˆä»»æ„ï¼‰ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
    #         dbg(f"[cat] union-links={len(links)} @ {url}")
    #         for a in links[:2]:
    #             _txt = re.sub(r"\s+", " ", a.get_text(" ", strip=True))
    #             dbg("   â†’", _txt, "|", a.get("href"))

    #         found = 0
    #         for a in links:
    #             href = a.get("href") or ""
    #             raw = a.get_text(" ", strip=True)
    #             try:
    #                 shown_date = _parse_category_date_text(raw)
    #             except Exception:
    #                 # å¿…è¦æœ€å°é™ã®ãƒ‡ãƒãƒƒã‚°ã ã‘
    #                 dbg("[cat] date-parse-fail:", re.sub(r"\s+", " ", raw)[:120])
    #                 continue

    #             if shown_date == date_obj and href and href not in seen_urls:
    #                 candidate_urls.append(href)
    #                 seen_urls.add(href)
    #                 found += 1

    #         # wrapper å†…ã§â€œå½“æ—¥â€ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ soup ã¾ã§åºƒã’ãšçµ‚äº†ã€‚
    #         # wrapper ãŒç„¡ã„å ´åˆï¼ˆscopes ãŒ [soup] ã ã‘ã®æ™‚ï¼‰ã‚‚1å‘¨ã§æŠœã‘ã‚‹ã€‚
    #         if found > 0:
    #             dbg(f"[cat] STOP (added {found} candidates) @ {url}")
    #             break

    # dbg(f"[cat] candidates={len(candidate_urls)}")

    # # ==== 2) è¨˜äº‹ç¢ºèª ====
    # for url in candidate_urls:
    #     try:
    #         res_article = fetch_with_retry_irrawaddy(url, session=session)
    #     except Exception as e:
    #         print(f"Error processing {url}: {e}")
    #         continue

    #     soup_article = BeautifulSoup(res_article.content, "html.parser")

    #     meta_date = _article_date_from_meta_mmt(soup_article)
    #     if meta_date is None:
    #         dbg("[art] meta-missing:", url)
    #         continue
    #     if meta_date != date_obj:
    #         dbg("[art] meta-mismatch:", meta_date, "target:", date_obj, "â†’", url)
    #         continue

    #     title = _extract_title(soup_article)
    #     if not title:
    #         dbg("[art] title-missing:", url)
    #         continue

    #     body = extract_body_irrawaddy(soup_article)
    #     if not body:
    #         dbg("[art] body-empty:", url)
    #         continue

    #     #  irrawaddyã¯ã©ã®è¨˜äº‹ã‚‚ã»ã—ã„ã¨ã®ã“ã¨ãªã®ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¯å¤–ã™ã€å¤§åŠãƒŸãƒ£ãƒ³ãƒãƒ¼è¨˜äº‹ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«ã‚“ã§ãªãã¦ã‚‚å–å¾—å¯¾è±¡ã®ã“ã¨ã‚‚ã‚ã£ãŸ
    #     # if not any_keyword_hit(title, body):
    #     #     dbg("[art] keyword-not-hit:", url)
    #     #     continue

    #     results.append(
    #         {
    #             "url": url,
    #             "title": title,
    #             "date": date_obj.isoformat(),
    #         }
    #     )

    # dbg(f"[final] kept={len(results)}")

    results = []
    seen_urls = set()
    candidate_urls = []

    # ==== 1) å„ã‚«ãƒ†ã‚´ãƒªURLã‚’1å›ãšã¤å·¡å› â†’ å½“æ—¥å€™è£œæŠ½å‡º ====
    for rel_path in paths:
        url = f"{BASE}{rel_path}"
        # print(f"Fetching {url}")
        try:
            res = fetch_with_retry_irrawaddy(url, session=session)
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            continue

        soup = BeautifulSoup(res.content, "html.parser")
        wrapper = soup.select_one("div.jeg_content")  # ãƒ†ãƒ¼ãƒã«ã‚ˆã£ã¦ã¯ç„¡ã„ã“ã¨ã‚‚ã‚ã‚‹

        # âœ… union æ–¹å¼ï¼šwrapper å†…â†’è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒšãƒ¼ã‚¸å…¨ä½“ã®é †ã§æ¢ç´¢
        scopes = ([wrapper] if wrapper else []) + [soup]

        for scope in scopes:
            # ãƒ’ãƒ¼ãƒ­ãƒ¼æ ï¼‹é€šå¸¸ãƒªã‚¹ãƒˆï¼‹æ±ç”¨ãƒ¡ã‚¿ã‚’ä¸€ç™ºã§æ‹¾ã†
            links = scope.select(
                ".jnews_category_hero_container .jeg_meta_date a[href], "
                "div.jeg_postblock_content .jeg_meta_date a[href], "
                ".jeg_post_meta .jeg_meta_date a[href]"
            )
            # æ™‚è¨ˆã‚¢ã‚¤ã‚³ãƒ³ä»˜ãã ã‘ã«é™å®šï¼ˆãƒã‚¤ã‚ºå›é¿ï¼‰
            links = [a for a in links if a.find("i", class_="fa fa-clock-o")]

            # ï¼ˆä»»æ„ï¼‰ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
            # dbg(f"[cat] union-links={len(links)} @ {url}")
            for a in links[:2]:
                _txt = re.sub(r"\s+", " ", a.get_text(" ", strip=True))
                # dbg("   â†’", _txt, "|", a.get("href"))

            found = 0
            for a in links:
                href = a.get("href") or ""
                raw = a.get_text(" ", strip=True)
                try:
                    shown_date = _parse_category_date_text(raw)
                except Exception:
                    # å¿…è¦æœ€å°é™ã®ãƒ‡ãƒãƒƒã‚°ã ã‘
                    # dbg("[cat] date-parse-fail:", re.sub(r"\s+", " ", raw)[:120])
                    continue

                if shown_date == date_obj and href and href not in seen_urls:
                    candidate_urls.append(href)
                    seen_urls.add(href)
                    found += 1

            # wrapper å†…ã§â€œå½“æ—¥â€ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ soup ã¾ã§åºƒã’ãšçµ‚äº†ã€‚
            # wrapper ãŒç„¡ã„å ´åˆï¼ˆscopes ãŒ [soup] ã ã‘ã®æ™‚ï¼‰ã‚‚1å‘¨ã§æŠœã‘ã‚‹ã€‚
            if found > 0:
                # dbg(f"[cat] STOP (added {found} candidates) @ {url}")
                break

    # ==== 2) å€™è£œè¨˜äº‹ã§å³å¯†ç¢ºèªï¼ˆmetaæ—¥ä»˜/æœ¬æ–‡/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰ ====
    for url in candidate_urls:
        try:
            res_article = fetch_with_retry_irrawaddy(url, session=session)
            soup_article = BeautifulSoup(res_article.content, "html.parser")

            if _article_date_from_meta_mmt(soup_article) != date_obj:
                continue

            title = _extract_title(soup_article)
            if not title:
                continue

            body = extract_body_irrawaddy(soup_article)
            if not body:
                continue

            # irrawaddyã¯ã©ã®è¨˜äº‹ã‚‚ã»ã—ã„ã¨ã®ã“ã¨ãªã®ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¯å¤–ã™
            # å¤§åŠãƒŸãƒ£ãƒ³ãƒãƒ¼è¨˜äº‹ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«ã‚“ã§ãªãã¦ã‚‚å–å¾—å¯¾è±¡ã®ã“ã¨ã‚‚ã‚ã£ãŸã€ç„¡é§„è¨˜äº‹ã®å–å¾—ãŒç›®ç«‹ã¤ã‚ˆã†ã§ã‚ã‚Œã°è¿½åŠ æ¤œè¨
            # if not any_keyword_hit(title, body):
            #     continue

            results.append(
                {
                    "url": url,
                    "title": title,
                    "date": date_obj.isoformat(),
                    "body": body,
                }
            )
        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return results


# åŒã˜URLã®é‡è¤‡å‰Šé™¤
def deduplicate_by_url(articles):
    seen_urls = set()
    unique_articles = []
    for art in articles:
        if art["url"] in seen_urls:
            print(
                f"ğŸ›‘ URL Duplicate Removed: {art['source']} | {art['title']} | {art['url']}"
            )
            continue
        seen_urls.add(art["url"])
        unique_articles.append(art)
    return unique_articles


# ç¿»è¨³å¯¾è±¡ã‚­ãƒ¥ãƒ¼
translation_queue = []


def process_and_enqueue_articles(
    articles,
    source_name,
    seen_urls=None,
    bypass_keyword=False,
    trust_existing_body=False,
):
    if seen_urls is None:
        seen_urls = set()

    queued_items = []
    for art in articles:
        if art["url"] in seen_urls:
            continue
        seen_urls.add(art["url"])

        try:
            # â‘  ã¾ãšã¯è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«æœ¬æ–‡ãŒæ¥ã¦ã„ãŸã‚‰ãã‚Œã‚’ä½¿ã†
            body_text = (art.get("body") or "").strip() if trust_existing_body else ""

            # â‘¡ ç„¡ã‘ã‚Œã°ãƒ•ã‚§ãƒƒãƒï¼ˆå†…éƒ¨ã§å†ãƒ•ã‚§ãƒƒãƒä»˜ããƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ï¼‰
            if not body_text:
                if source_name == "Irrawaddy" or "irrawaddy.com" in art["url"]:
                    body_text = get_body_with_refetch(
                        art["url"],
                        fetcher=lambda u: fetch_once_irrawaddy(
                            u, session=requests.Session()
                        ),
                        extractor=extract_body_irrawaddy,  # æ—¢å­˜ã®æŠ½å‡ºå™¨ã‚’ä½¿ç”¨
                        retries=3,
                        wait_seconds=2,
                        quiet=False,
                    )
                else:
                    body_text = get_body_with_refetch(
                        art["url"],
                        fetcher=fetch_once_requests,
                        extractor=extract_body_generic_from_soup,
                        retries=2,
                        wait_seconds=1,
                        quiet=True,
                    )

            # â‘¢ æ­£è¦åŒ–
            title_nfc = unicodedata.normalize("NFC", art["title"])
            body_nfc = unicodedata.normalize("NFC", body_text)

            # â‘£ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆIrrawaddyãªã©å¿…è¦ã«å¿œã˜ã¦ãƒã‚¤ãƒ‘ã‚¹ï¼‰
            if not bypass_keyword:
                if not any_keyword_hit(title_nfc, body_nfc):
                    log_no_keyword_hit(
                        source_name,
                        art["url"],
                        title_nfc,
                        body_nfc,
                        "enqueue:after-fetch",
                    )
                    continue

            # â‘¤ ã‚­ãƒ¥ãƒ¼æŠ•å…¥
            queued_items.append(
                {
                    "source": source_name,
                    "url": art["url"],
                    "title": art["title"],  # ç¿»è¨³å‰ã‚¿ã‚¤ãƒˆãƒ«
                    "body": body_text,  # ç¿»è¨³å‰æœ¬æ–‡
                }
            )

        except Exception as e:
            print(f"Error processing {art['url']}: {e}")
            continue

    translation_queue.extend(queued_items)


# MEMO: ãƒ­ã‚°ç”¨ã€ãƒ‡ãƒãƒƒã‚¯ç”¨é–¢æ•°
# def process_translation_batches(batch_size=10, wait_seconds=60):
#     summarized_results = []

#     # ãƒ†ã‚¹ãƒˆç”¨ã« translation_queue ã®ä¸­èº«ã‚’ãã®ã¾ã¾ summarized_results ã«è©°ã‚ã‚‹
#     for item in translation_queue:
#         summarized_results.append({
#             "source": item["source"],
#             "url": item["url"],
#             "title": item["title"],      # ç¿»è¨³å‰ã‚¿ã‚¤ãƒˆãƒ«
#             "summary": item["body"][:2000]  # è¦ç´„ã®ä»£ã‚ã‚Šã«æœ¬æ–‡å†’é ­
#         })

#     # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆsummarized_results ã®ä¸­èº«ã‚’çœç•¥ã›ãšç¢ºèªï¼‰
#     print("===== DEBUG: summarized_results =====")
#     pprint.pprint(summarized_results, width=120, compact=False)
#     print("===== END DEBUG =====")

#     # ã“ã“ã§å‡¦ç†çµ‚äº†
#     return summarized_results


# é‡è¤‡è¨˜äº‹å‰Šé™¤å‡¦ç†ã‚»ãƒƒãƒˆ
def _strip_tags(text: str) -> str:
    # è¦ç´„ã«å«ã‚ãŸ <br> ãªã©ã‚’ç´ ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼ˆæœ€ä½é™ï¼‰
    text = text.replace("<br>", "\n")
    return re.sub(r"<[^>]+>", "", text)


def _safe_json_loads_maybe_extract(text: str):
    """
    ç”ŸæˆAIãŒå‰å¾Œã«ä½™è¨ˆãªæ–‡ã‚’ä»˜ã‘ãŸå ´åˆã§ã‚‚JSONéƒ¨åˆ†ã ã‘æŠ½å‡ºã—ã¦èª­ã‚€ä¿é™ºã€‚
    """
    try:
        return json.loads(text)
    except Exception:
        # æœ€å¾Œã® { ... } ã‚’ç´ æœ´ã«æŠ½å‡º
        m = re.search(r"\{.*\}", text, flags=re.DOTALL)
        if m:
            return json.loads(m.group(0))
        raise


# é‡è¤‡åˆ¤å®šã®ãƒ­ã‚°å‡ºåŠ›
def log_dedupe_report(
    data: dict,
    id_map: dict,
    id_to_meta: dict,
    article_ids_in_order: list[str],
    *,
    printer=print,
    header="ğŸ§© DEDUPE REPORT",
):
    """
    LLMå¿œç­”ãƒ‡ãƒ¼ã‚¿(data)ã¨ã€IDâ†’è¨˜äº‹ãƒ¡ã‚¿æƒ…å ±ã®ãƒãƒƒãƒ—ã‚’å—ã‘å–ã‚Šã€
    é‡è¤‡åˆ¤å®šãƒ¬ãƒãƒ¼ãƒˆã‚’æ•´å½¢ã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚

    - data: {"kept":[...], "removed":[...], "clusters":[...]}
    - id_map: {id -> å…ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ}
    - id_to_meta: {id -> {"title": str, "source": str}}
    - article_ids_in_order: å…¥åŠ›é †åºã®IDãƒªã‚¹ãƒˆï¼ˆå…ƒé…åˆ—ã®é †ã‚’ä¿ã¤ãŸã‚ã«ä½¿ç”¨ï¼‰
    - printer: å‡ºåŠ›é–¢æ•°ï¼ˆprint ã‚„ logger.info ãªã©ï¼‰
    """
    kept_list = data.get("kept") or []
    removed_list = data.get("removed") or []
    clusters = data.get("clusters") or []

    kept_ids = [x.get("id") for x in kept_list if x.get("id") in id_map]
    kept_set = set(kept_ids)

    printer(f"\n===== {header} =====")

    # 1) Kept æ¦‚è¦
    printer(f"Kept: {len(kept_ids)} item(s)")
    for k in kept_list:
        kid = k.get("id")
        meta = id_to_meta.get(kid, {})
        why = (k.get("why") or "").strip()
        if kid in id_map:
            why_part = (
                f"  | why: {why}" if why else ""
            )  # â† ãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’å¼ã«å…¥ã‚Œãªã„
            printer(
                f"  âœ“ [{kid}] {meta.get('title','(no title)')}  | src={meta.get('source','')}"
                f"{why_part}"
            )
        else:
            printer(f"  âœ“ [{kid}] (unknown id)")

    # 2) Removed è©³ç´°ï¼ˆã©ã‚Œã®é‡è¤‡ã¨ã—ã¦è½ã¡ãŸã‹ï¼‰
    printer(f"\nRemoved (LLM-reported): {len(removed_list)} item(s)")
    for r in removed_list:
        rid = r.get("id")
        dup = r.get("duplicate_of")
        why = (r.get("why") or "").strip()
        rmeta = id_to_meta.get(rid, {"title": "(unknown)", "source": ""})
        kmeta = id_to_meta.get(dup, {"title": "(unknown)", "source": ""})
        unknown_flags = []
        if rid not in id_map:
            unknown_flags.append("RID_NOT_IN_INPUT")
        if dup and dup not in id_map:
            unknown_flags.append("KEPT_NOT_IN_INPUT")
        uf = f"  [{', '.join(unknown_flags)}]" if unknown_flags else ""
        reason_line = f"\n      reason: {why}" if why else ""  # â† å…ˆã«ä½œã‚‹
        printer(
            f"  - [{rid}] {rmeta['title']}  | src={rmeta['source']}\n"
            f"      â†’ duplicate of [{dup}] {kmeta['title']}  | src={kmeta['source']}{uf}"
            f"{reason_line}"
        )

    # 3) å®Ÿå·®åˆ†ï¼ˆå…¥åŠ› - keptï¼‰
    derived_removed_ids = [aid for aid in article_ids_in_order if aid not in kept_set]
    printer(f"\nRemoved (derived by kept-set): {len(derived_removed_ids)} item(s)")
    for rid in derived_removed_ids:
        rmeta = id_to_meta.get(rid, {"title": "(unknown)", "source": ""})
        rrec = next((x for x in removed_list if x.get("id") == rid), None)
        if rrec:
            dup = rrec.get("duplicate_of")
            why = (rrec.get("why") or "").strip()
            kmeta = id_to_meta.get(dup, {"title": "(unknown)", "source": ""})
            reason_line = f"\n      reason: {why}" if why else ""  # â† å…ˆã«ä½œã‚‹
            printer(
                f"  - [{rid}] {rmeta['title']}  | src={rmeta['source']}\n"
                f"      â†’ duplicate of [{dup}] {kmeta['title']}  | src={kmeta['source']}"
                f"{reason_line}"
            )
        else:
            printer(
                f"  - [{rid}] {rmeta['title']}  | src={rmeta['source']} (â€» LLMã®removedã«æœªè¨˜è¼‰)"
            )

    # 4) å‚ç…§æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    unknown_kept = [
        kid for kid in [x.get("id") for x in kept_list] if kid not in id_map
    ]
    unknown_removed = [r.get("id") for r in removed_list if r.get("id") not in id_map]
    if unknown_kept:
        printer(f"\nâš ï¸ Keptã«æœªçŸ¥ã®IDãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {unknown_kept}")
    if unknown_removed:
        printer(f"âš ï¸ Removedã«æœªçŸ¥ã®IDãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {unknown_removed}")

    # 5) ã‚¯ãƒ©ã‚¹ã‚¿æ¦‚è¦ï¼ˆä»»æ„ï¼‰
    if clusters:
        printer("\nCluster summary:")
        cluster_kept_map = {
            k.get("cluster_id"): k.get("id") for k in kept_list if k.get("cluster_id")
        }
        for c in clusters:
            cid = c.get("cluster_id")
            members = c.get("member_ids") or []
            event_key = c.get("event_key") or ""
            kept_id_for_cluster = cluster_kept_map.get(cid)
            printer(
                f"  â€¢ cluster={cid}  members={len(members)}  kept={kept_id_for_cluster}  event='{event_key}'"
            )

    printer("===== END DEDUPE REPORT =====\n")


def dedupe_articles_with_llm(
    client,
    summarized_results,
    debug=True,
    *,
    logger=None,
    ultra_max_chars=300,
    summary_fallback_chars=600,
):
    """
    summarized_results (list[dict]) ã‚’å—ã‘å–ã‚Šã€é‡è¤‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã”ã¨ã«1æœ¬ã ã‘æ®‹ã—ãŸé…åˆ—ã‚’è¿”ã™ã€‚
    Irrawaddyï¼ˆsource == "Irrawaddy" ã¾ãŸã¯ URL ã« "irrawaddy.com" ã‚’å«ã‚€ï¼‰ã¯
    LLM ã§ã®é‡è¤‡åˆ¤å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å¸¸ã« keep ã™ã‚‹ã€‚
    ä¾å­˜: call_gemini_with_retries, _safe_json_loads_maybe_extract, _strip_tags, log_dedupe_report
    """

    if not summarized_results:
        return summarized_results

    # å‡ºåŠ›é–¢æ•°
    if debug:
        printer = logger.info if logger else print
    else:

        def _noop(*args, **kwargs):
            return None

        printer = _noop

    # ===== LLMå…¥åŠ›ç”¨ï¼ˆIrrawaddy ã‚’é™¤å¤–ï¼‰ã‚’æ§‹ç¯‰ =====
    irrawaddy_ids = set()
    articles_for_llm = []
    id_map_llm = {}
    id_to_meta_llm = {}
    ids_in_order_llm = []
    all_ids_in_order = []  # è¿”å´æ™‚ã®é †åºç¶­æŒç”¨

    for idx, it in enumerate(summarized_results):
        _id = it.get("url") or f"idx-{idx}"
        all_ids_in_order.append(_id)

        # Irrawaddy åˆ¤å®šï¼ˆã”æŒ‡å®šã©ãŠã‚Šï¼‰
        is_irrawaddy = (it.get("source") == "Irrawaddy") or (
            "irrawaddy.com" in (it.get("url") or "")
        )
        if is_irrawaddy:
            irrawaddy_ids.add(_id)
            continue  # LLM ã«ã¯é€ã‚‰ãªã„

        # é Irrawaddy â†’ LLM å…¥åŠ›ã¸
        body_ultra = (it.get("ultra") or "").strip()
        body_fallback = _strip_tags(it.get("summary", ""))[:summary_fallback_chars]
        body = body_ultra[:ultra_max_chars] if body_ultra else body_fallback

        ids_in_order_llm.append(_id)
        id_map_llm[_id] = it
        id_to_meta_llm[_id] = {"title": it.get("title"), "source": it.get("source")}
        articles_for_llm.append(
            {
                "id": _id,
                "source": it.get("source"),
                "title": it.get("title"),
                "body": body,
            }
        )

    # ã™ã¹ã¦ Irrawaddy ã ã£ãŸå ´åˆã¯ãã®ã¾ã¾è¿”ã™
    if not articles_for_llm:
        if debug and irrawaddy_ids:
            printer(
                f"â­ï¸ å…¨ {len(irrawaddy_ids)} ä»¶ãŒ Irrawaddyã€‚LLM é‡è¤‡åˆ¤å®šã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
        return summarized_results

    # ===== ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆLLM ã«é€ã‚‹åˆ†ã®ã¿ï¼‰ =====
    if debug:
        if irrawaddy_ids:
            printer(f"â­ï¸ Irrawaddy {len(irrawaddy_ids)} ä»¶ã¯å¸¸ã« keepï¼ˆLLM ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚")
        printer("===== DEBUG 2: articles SENT TO LLM =====")
        printer(_pprint.pformat(articles_for_llm, width=120, compact=False))
        printer("===== END DEBUG 2 =====\n")

    # ===== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆé Irrawaddy ã®ã¿ï¼‰ =====
    prompt = (
        "ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡è¤‡åˆ¤å®šãƒ•ã‚£ãƒ«ã‚¿ã§ã™ã€‚\n"
        "ä»¥å¾Œã®åˆ¤å®šã¯å„è¨˜äº‹ã®ã€Œtitleã€ã¨ã€Œbodyï¼ˆã“ã‚Œã¯è¶…è¦ç´„ã¾ãŸã¯çŸ­ç¸®è¦ç´„ï¼‰ã€ã®ã¿ã‚’ä½¿ç”¨ã—ã€å…ƒæœ¬æ–‡ã«ã¯æˆ»ã£ã¦å†å‚ç…§ã—ã¾ã›ã‚“ã€‚\n"
        "ç›®çš„ï¼šåŒä¸€ä¸»æ—¨ï¼ˆãƒˆãƒ”ãƒƒã‚¯ + è§’åº¦ + ç™ºä¿¡ä¸»ä½“ï¼‰ã‚’å ±ã˜ã‚‹è¨˜äº‹ã‚’æŸã­ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰1æœ¬ã ã‘æ®‹ã—ã¾ã™ã€‚å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ã€‚\n\n"
        "ã€å®šç¾©ã€‘\n"
        "ãƒ»ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ï¼šwho / what / where / when ã®ã†ã¡å°‘ãªãã¨ã‚‚3è¦ç´ ãŒä¸€è‡´ï¼ˆè¨€ã„æ›ãˆãƒ»è¨€èªå·®ã¯åŒä¸€æ‰±ã„ã€‚æ—¥ä»˜ã¯Â±14æ—¥ã‚’åŒä¸€æ‰±ã„å¯ï¼‰ã€‚\n"
        "ãƒ»è¨˜äº‹ã®ç¨®é¡ï¼ˆtypeï¼‰ï¼šä»¥ä¸‹ã®æ­£è¦åŒ–ã‚«ãƒ†ã‚´ãƒªã®ã„ãšã‚Œã‹1ã¤ã«å†…éƒ¨ã§åˆ†é¡ã—ã¦ç”¨ã„ã‚‹ï¼ˆå‡ºåŠ›ã«ã¯å«ã‚ãªã„ï¼‰ã€‚\n"
        "  é€Ÿå ±/å˜å ±, æ”¿ç­–ç™ºè¡¨è¦ç‚¹, å…¬å¼ç™ºè¡¨/å£°æ˜, ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼, è§£èª¬/èƒŒæ™¯, ç‰©å£°æ˜, çµ„ç¹”å£°æ˜, å…¬ç¤º,\n"
        "  ãƒ‡ãƒ¼ã‚¿/çµ±è¨ˆ, ã¾ã¨ã‚/ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ, ãƒ©ã‚¤ãƒ–/æ™‚ç³»åˆ—æ›´æ–°,\n"
        "  å†™çœŸ/æ˜ åƒç‰¹é›†, ç¤¾èª¬/è«–èª¬/å¯„ç¨¿, ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«\n"
        "  è¿‘ã„åŒç¾©èªã¯å†…éƒ¨ã§æ­£è¦åŒ–ï¼šã€press release/announcementâ†’å…¬å¼ç™ºè¡¨/å£°æ˜ã€ã€explainer/analysisâ†’è§£èª¬/èƒŒæ™¯ã€\n"
        "  ã€roundup/digestâ†’ã¾ã¨ã‚/ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã€ã€live updatesâ†’ãƒ©ã‚¤ãƒ–/æ™‚ç³»åˆ—æ›´æ–°ã€\n"
        "  åˆ¤åˆ¥ä¸èƒ½ãªå ´åˆã¯ type=ä¸æ˜ ã¨ã—ã€ç¨®é¡ä¸€è‡´ã«ã¯æ•°ãˆãªã„ã€‚\n"
        "ãƒ»ç™ºä¿¡ä¸»ä½“ï¼ˆprovenanceï¼‰ï¼šä»¥ä¸‹ã®ã„ãšã‚Œã‹1ã¤ã‚’å†…éƒ¨ã§æ¨å®šã—ã¦ç”¨ã„ã‚‹ã€‚\n"
        "  â‘  æœ¬äººæŒ‡ç¤º/é¦–é•·ã®ç›´è¨€ï¼ˆä¾‹ï¼šãƒŸãƒ³ãƒ»ã‚¢ã‚¦ãƒ³ãƒ»ãƒ•ãƒ©ã‚¤ãƒ³ãŒã€ŒæŒ‡ç¤º/å‘½ä»¤/è¡¨æ˜ã€ï¼‰\n"
        "  â‘¡ å…¬å¼æ©Ÿé–¢ã®ç™ºè¡¨ï¼ˆå®˜å ±/ä¼šè¦‹/æ–‡æ›¸/åºƒå ±ï¼‰\n"
        "  â‘¢ åŒ¿åã®è»ç­‹/é–¢ä¿‚è€…/æ¶ˆæ¯ç­‹/å†…éƒ¨ç­‹ï¼ˆã€Œè»ç­‹ã«ã‚ˆã‚Œã°ã€ã€Œé–¢ä¿‚è€…ã«ã‚ˆã‚‹ã¨ã€ç­‰ï¼‰\n"
        "  â‘£ ç¾åœ°é‹ç”¨ãƒ»æ²»å®‰éƒ¨éšŠ/å§”å“¡ä¼šã®å®Ÿå‹™é€šé”\n\n"
        "ã€åˆ¤å®šæ–¹é‡ã€‘\n"
        "1) åŒä¸€ä¸»æ—¨ = ã€ãƒˆãƒ”ãƒƒã‚¯ä¸€è‡´ã€ã‹ã¤ã€ç¨®é¡ä¸€è‡´ï¼ˆtypeãŒä¸€è‡´ã€ã‹ã¤ä¸æ˜ä»¥å¤–ï¼‰ã€ã‹ã¤ã€ç™ºä¿¡ä¸»ä½“ï¼ˆprovenanceï¼‰ä¸€è‡´ã€ã®å…¨ã¦ã‚’æº€ãŸã™å ´åˆã«é™ã‚‹ã€‚\n"
        "   â€» ã¾ã¨ã‚/ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ/è¤‡æ•°æ¡ˆä»¶åˆ—æŒ™ã®è¦ç´„ã¨ã€å˜ä¸€æ¡ˆä»¶ã®é€Ÿå ±ãƒ»è§£èª¬ã¯é‡è¤‡ã«ã—ãªã„ï¼ˆåˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼‰ã€‚\n"
        "   â€» åŒä¸€ãƒ†ãƒ¼ãƒï¼ˆä¾‹ï¼šé¸æŒ™é‹å‹•è¦åˆ¶ï¼‰ã§ã‚‚ã€å†…å®¹è¦åˆ¶ã€ã¨ã€é‹ç”¨ãƒ»æ‰‹ç¶šï¼ˆè¨±èªå¯/å ´æ‰€/æ™‚é–“/è­¦å‚™/ç®¡ç†ï¼‰ã€ã¯åˆ¥è§’åº¦ã¨ã—ã¦å¿…ãšåˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã™ã‚‹ã€‚\n"
        "   ä¾‹ï¼šã€ˆè»ã¸ã®æ‰¹åˆ¤çš„é¸æŒ™é‹å‹•ã‚’ç¦ã˜ã‚‹ï¼ˆå†…å®¹è¦åˆ¶ï¼‰ã€‰ã¨ã€ˆè»ã®ç®¡ç†ä¸‹ãƒ»äº‹å‰è¨±å¯ã§ã®ã¿é¸æŒ™æ´»å‹•å¯ï¼ˆé‹ç”¨ãƒ»æ‰‹ç¶šï¼‰ã€‰ã¯åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã€‚\n"
        "   ä¾‹ï¼šã€ˆMAHæœ¬äººãŒâ€œæ‰¹åˆ¤ç¦æ­¢â€ã‚’æŒ‡ç¤ºï¼ˆæœ¬äººæŒ‡ç¤ºï¼‰ã€‰ã¨ã€ˆãƒãƒ”ãƒ‰ãƒ¼è»ç­‹ãŒâ€œè¨±å¯åˆ¶ãƒ»ç®¡ç†ä¸‹â€ã¨ä¼èï¼ˆè»ç­‹ï¼‰ã€‰ã¯ã€è§’åº¦ã‚‚ç™ºä¿¡ä¸»ä½“ã‚‚ç•°ãªã‚‹ãŸã‚åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã€‚\n"
        "2) ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–ï¼šè¨˜äº‹ã¯æœ€ã‚‚ä¸€è‡´åº¦ãŒé«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã®ã¿æ‰€å±ã€‚ä¸ç¢ºå®Ÿãªã‚‰åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã™ã‚‹ã€‚\n"
        "3) æ®‹ã™åŸºæº–ï¼ša)å›ºæœ‰æƒ…å ±é‡ï¼ˆåœ°å/äººæ•°/é‡‘é¡/çµ„ç¹”å/æ–°è¦äº‹å®Ÿï¼‰ b)å…·ä½“æ€§/æ˜ç­ã• c)ã‚¿ã‚¤ãƒˆãƒ«æƒ…å ±é‡ã€‚\n"
        "   åŒç‚¹ãªã‚‰ æœ¬æ–‡é•·ï¼ˆbodyã®æ–‡å­—æ•°ï¼‰â†’ sourceæ˜‡é † â†’ idæ˜‡é † ã®é †ã§æ±ºå®šã€‚\n"
        "4) å…¥åŠ›å¤–ã®äº‹å®Ÿã¯åŠ ãˆãªã„ã€‚çµ±åˆè¨˜äº‹ã¯ä½œã‚‰ãªã„ã€‚\n\n"
        "ã€å‡ºåŠ›ã®åˆ¶ç´„ã€‘\n"
        "ãƒ»JSONã®ã¿ã‚’è¿”ã™ã€‚ä½™è¨ˆãªãƒ†ã‚­ã‚¹ãƒˆã‚„ã‚­ãƒ¼ã¯ç¦æ­¢ã€‚\n"
        "ãƒ»kept/removed/clusters ã® id ã¯å¿…ãšå…¥åŠ› articles ã® id ã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚\n"
        "ãƒ»clusters[].member_ids ã¯å…¥åŠ› id ã‚’é‡è¤‡ãªãã™ã¹ã¦å«ã‚€ã“ã¨ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã¨ keptä»¶æ•°ã¯åŒæ•°ã€‚\n"
        "ãƒ»removed[].duplicate_of ã¯åŒä¸€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã® kept id ã‚’æŒ‡ã™ã“ã¨ã€‚\n"
        "ãƒ»why ã¯16ã€œ24å­—ç¨‹åº¦ã€event_key ã¯25å­—ä»¥å†…ã«åã‚ã‚‹ã“ã¨ã€‚\n\n"
        "å…¥åŠ›:\n"
        f'{{\\n  "articles": {json.dumps(articles_for_llm, ensure_ascii=False)}\\n}}\\n\\n'
        "å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSONã®ã¿ï¼‰:\n"
        "{\n"
        '  "kept": [ {"id":"<æ®‹ã™è¨˜äº‹ID>", "cluster_id":"<ID>", "why":"16-24å­—"} ],\n'
        '  "removed": [ {"id":"<é™¤å¤–è¨˜äº‹ID>", "duplicate_of":"<æ®‹ã—ãŸè¨˜äº‹ID>", "why":"16-24å­—"} ],\n'
        '  "clusters": [ {"cluster_id":"<ID>", "member_ids":["<id1>","<id2>","..."], "event_key":"25å­—ä»¥å†…"} ]\n'
        "}\n"
    )

    try:
        resp = call_gemini_with_retries(client, prompt, model="gemini-2.5-flash")
        data = _safe_json_loads_maybe_extract(resp.text)

        kept_ids_others = [
            x.get("id") for x in data.get("kept", []) if x.get("id") in id_map_llm
        ]

        # ãƒ¬ãƒãƒ¼ãƒˆï¼ˆLLM ã«é€ã£ãŸåˆ†ã®ã¿ï¼‰
        if debug:
            log_dedupe_report(
                data=data,
                id_map=id_map_llm,
                id_to_meta=id_to_meta_llm,
                article_ids_in_order=ids_in_order_llm,
                printer=printer,
                header="ğŸ§© DEDUPE REPORT (non-Irrawaddy only)",
            )

        # kept ãŒå‡ºãŸã¨ãã®ã¿ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ã—ã€Irrawaddy ã‚’åˆæµ
        if kept_ids_others:
            kept_union = set(kept_ids_others) | irrawaddy_ids
            filtered = [
                obj
                for obj, _id in zip(summarized_results, all_ids_in_order)
                if _id in kept_union
            ]
            return filtered

        # ã†ã¾ãåˆ¤å®šã§ããªã‹ã£ãŸã‚‰åŸæœ¬ã‚’è¿”ã™ï¼ˆIrrawaddy ã‚‚å½“ç„¶æ®‹ã‚‹ï¼‰
        return summarized_results

    except Exception as e:
        print(f"ğŸ›‘ Dedupe failed, returning original list: {e}")
        return summarized_results


# ===== è¦ç´„ãƒ»ç¿»è¨³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ãƒ¼ãƒ„ =====
STEP12_FILTERS = (
    "Step 1: ä¾‹å¤–ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰\n"
    "Q1. è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯æœ¬æ–‡ãŒ `Myawaddy`, `á€™á€¼á€á€á€®`, `Muse`, `á€™á€°á€†á€šá€º`, `å›½å¢ƒè²¿æ˜“`, `å›½å¢ƒäº¤æ˜“`ã«é–¢ã™ã‚‹å†…å®¹ã§ã™ã‹ï¼Ÿ\n"
    "â†’ Yes ã®å ´åˆã€ã“ã®å¾Œã®åˆ¤å®šã¯è¡Œã‚ãš Step 3 ã«é€²ã‚“ã§ãã ã•ã„ã€‚\n"
    "â†’ No ã®å ´åˆã¯ Step 2 ã¸é€²ã‚“ã§ãã ã•ã„ã€‚\n\n"
    "Step 2: é™¤å¤–æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆåœ°åŸŸä¾‹å¤–ã‚ã‚Šï¼ä¸»é¡Œåˆ¤å®šï¼‰\n"
    "Q2. æœ¬è¨˜äº‹ã®ä¸»é¡ŒãŒã€ç‰¹å®šã®åœ°åŸŸï¼ˆéƒ¡åŒºãƒ»ã‚¿ã‚¦ãƒ³ã‚·ãƒƒãƒ—ãƒ»å¸‚ãƒ»æ‘ï¼‰ã«ãŠã‘ã‚‹å±€åœ°çš„ãªã€Œæˆ¦é—˜ãƒ»ç´›äº‰ãƒ»æ”»æ’ƒãƒ»è¡çªãƒ»çˆ†æ’ƒãƒ»å¼·ç›—ãƒ»æŠ—è­°æ´»å‹•ãƒ»æŠ•é™ãƒ»è§£æ”¾ãƒ»æ®ºäººäº‹ä»¶ã€ã®ç™ºç”Ÿã‚’å ±ã˜ã‚‹è¨˜äº‹ãƒ»è¢«å®³å ±å‘Šãƒ»çµ±è¨ˆã§ã™ã‹ï¼Ÿ\n"
    "- ä¸»é¡Œã§ã¯ãªãèƒŒæ™¯èª¬æ˜ã¨ã—ã¦ã®è¨€åŠã®ã¿ã¯ã€ŒNoã€ã€‚\n"
    "â†’ Yes ã®å ´åˆã§ã‚‚ã€è¨˜äº‹ã®ä¸»ãŸã‚‹ç™ºç”Ÿå ´æ‰€ãŒæ¬¡ã®åœ°åŸŸã«è©²å½“ã™ã‚‹ãªã‚‰é™¤å¤–ã›ãš Step 3 ã¸é€²ã‚“ã§ãã ã•ã„ï¼š\n"
    "   ãƒ»ãƒ¤ãƒ³ã‚´ãƒ³ç®¡åŒº / Yangon Region / á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸\n"
    "   ãƒ»ã‚¨ãƒ¼ãƒ¤ãƒ¯ãƒ‡ã‚£ç®¡åŒº / Ayeyarwady Region / á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸\n"
    "â†’ ä¸Šè¨˜ä»¥å¤–ã®åœ°åŸŸã§ã‚ã‚Œã°å‡¦ç†ã‚’çµ‚äº†ã—ã€Step 3 ã«ã¯é€²ã¾ãªã„ã§ãã ã•ã„ã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯`exit`ã®ã¿è¿”ã—ã¦ãã ã•ã„ã€‚\n"
    "â†’ No ã®å ´åˆã¯ Step 3 ã¸é€²ã‚“ã§ãã ã•ã„ã€‚\n"
)

STEP3_TASK = (
    "Step 3: ç¿»è¨³ã¨è¦ç´„å‡¦ç†\n"
    "ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã€æœ¬æ–‡ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\n"
    "ã‚¿ã‚¤ãƒˆãƒ«ï¼š\n"
    "- è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
    "- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã¯å¿…ãšã€Œã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘ â—¯â—¯ã€ã®å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚\n"
    "- ãã‚Œä»¥å¤–ã®æ–‡è¨€ã¯ä¸è¦ã§ã™ã€‚\n\n"
    "æœ¬æ–‡è¦ç´„ï¼š\n"
    "- ä»¥ä¸‹ã®è¨˜äº‹æœ¬æ–‡ã«ã¤ã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ã¾ã¨ã‚ã€700å­—ä»¥å†…ã§å…·ä½“çš„ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
    "- è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n"
    "- å€‹åˆ¥è¨˜äº‹ã®æœ¬æ–‡ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€ãƒ¡ãƒ‡ã‚£ã‚¢èª¬æ˜ã‚„ãƒšãƒ¼ã‚¸å…¨ä½“ã®è§£èª¬ã¯ä¸è¦ã§ã™ã€‚\n"
    "- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã¯è¦ç´„ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€ãã‚Œä»¥å¤–ã®æ–‡è¨€ã¯ä¸è¦ã§ã™ã€‚\n\n"
    "æœ¬æ–‡è¦ç´„ã®å‡ºåŠ›æ¡ä»¶ï¼š\n"
    "- 1è¡Œç›®ã¯`ã€è¦ç´„ã€‘`ã¨ã ã‘ã—ã¦ãã ã•ã„ã€‚\n"
    "- 2è¡Œç›®ä»¥é™ãŒå…¨ã¦ç©ºè¡Œã«ãªã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
    "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã‚’é©åˆ‡ã«ä½¿ã£ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚\n"
    "- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã«ã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜å·ï¼ˆ#, *, - ãªã©ï¼‰ã‚’ä½¿ã‚ãšã€å˜ç´”ãªãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
    "- è¦‹å‡ºã—ã¯ `[ ]` ã§å›²ã‚“ã§ãã ã•ã„ã€‚\n"
    "- ç©ºè¡Œã¯ä½œã‚‰ãªã„ã§ãã ã•ã„ã€‚\n"
    "- ç‰¹æ®Šè¨˜å·ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ï¼ˆå…¨ä½“ã‚’HTMLã¨ã—ã¦é€ä¿¡ã™ã‚‹ã‚ã‘ã§ã¯ãªã„ãŸã‚ï¼‰ã€‚\n"
    "- ç®‡æ¡æ›¸ãã¯`ãƒ»`ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚\n"
    "- æœ¬æ–‡è¦ç´„ã®åˆè¨ˆã¯æœ€å¤§700æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„ã€‚\n\n"
    "æœ¬æ–‡è¶…è¦ç´„ï¼š\n"
    "- ä»¥ä¸‹ã®è¨˜äº‹æœ¬æ–‡ã«ã¤ã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã¾ã¨ã‚ã€200å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
    "- å€‹åˆ¥è¨˜äº‹ã®æœ¬æ–‡ã®ã¿ã‚’å¯¾è±¡ã¨ã—ã€ãƒ¡ãƒ‡ã‚£ã‚¢èª¬æ˜ã‚„ãƒšãƒ¼ã‚¸å…¨ä½“ã®è§£èª¬ã¯ä¸è¦ã§ã™ã€‚\n"
    "- ä¾‹ï¼šã€èª°ãŒã€ã€ä½•ã‚’ã€ã€ã©ã“ã§ã€ã€ã„ã¤ã€ã€è¦æ¨¡ï¼ˆäººæ•°/é‡‘é¡ç­‰ï¼‰ã€ã‚’å«ã‚ã‚‹ã€‚\n\n"
    "æœ¬æ–‡è¶…è¦ç´„ã®å‡ºåŠ›æ¡ä»¶ï¼š\n"
    "- 1è¡Œç›®ã¯`ã€è¶…è¦ç´„ã€‘`ã¨ã ã‘ã—ã¦ãã ã•ã„ã€‚\n"
    "- 2è¡Œç›®ä»¥ãŒé™å…¨ã¦ç©ºè¡Œã«ãªã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n"
    "- æœ¬æ–‡è¶…è¦ç´„ã®åˆè¨ˆã¯æœ€å¤§200æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„ã€‚\n\n"
)

SKIP_NOTE_IRRAWADDY = "ã€é‡è¦ã€‘æœ¬è¨˜äº‹ã¯ Irrawaddy ã®è¨˜äº‹ã§ã™ã€‚Step 1 ã¨ Step 2 ã¯å®Ÿæ–½ã›ãšã€ç›´ã¡ã« Step 3 ã®ã¿ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚\n\n"


def build_prompt(item: dict, *, skip_filters: bool, body_max: int) -> str:
    header = "æ¬¡ã®æ‰‹é †ã§è¨˜äº‹ã‚’åˆ¤å®šãƒ»å‡¦ç†ã—ã¦ãã ã•ã„ã€‚\n\n"
    pre = SKIP_NOTE_IRRAWADDY if skip_filters else STEP12_FILTERS + "\n\n"
    input_block = (
        "å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼š\n"
        "###\n[è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«]\n###\n"
        f"{item['title']}\n\n"
        "[è¨˜äº‹æœ¬æ–‡]\n###\n"
        f"{item['body'][:body_max]}\n"
        "###\n"
    )
    return header + pre + STEP3_TASK + "\n" + input_block


# æœ¬å‡¦ç†é–¢æ•°
def process_translation_batches(batch_size=5, wait_seconds=60):
    # MEMO: TESTç”¨ã€Geminiã‚’å‘¼ã°ãšã€URLãƒªã‚¹ãƒˆã ã‘è¿”ã™
    # summarized_results = []
    # for item in translation_queue:
    #     summarized_results.append({
    #         "source": item["source"],
    #         "url": item["url"],
    #         "title": item['title'],
    #         "summary": item['body'][:BODY_MAX_CHARS]
    #     })

    summarized_results = []
    for i in range(0, len(translation_queue), batch_size):
        batch = translation_queue[i : i + batch_size]
        print(f"âš™ï¸ Processing batch {i // batch_size + 1}...")

        for item in batch:
            try:
                # ãƒ‡ãƒãƒƒã‚°: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                print("----- DEBUG: Prompt Input -----")
                print(f"TITLE: {item['title']}")
                print(f"BODY[:{BODY_MAX_CHARS}]: {item['body'][:BODY_MAX_CHARS]}")

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œã€Irrawaddy ã¯ Step1/2 ã‚’ã‚¹ã‚­ãƒƒãƒ—
                is_irrawaddy = (item.get("source") == "Irrawaddy") or (
                    "irrawaddy.com" in (item.get("url") or "")
                )
                prompt = build_prompt(
                    item, skip_filters=is_irrawaddy, body_max=BODY_MAX_CHARS
                )

                resp = call_gemini_with_retries(
                    client_summary, prompt, model="gemini-2.5-flash"
                )
                output_text = resp.text.strip()

                # ãƒ‡ãƒãƒƒã‚°: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’ç¢ºèª
                print("----- DEBUG: Model Output -----")
                print(output_text)

                # exitãŒè¿”ã£ã¦ããŸã‚‰ã‚¹ã‚­ãƒƒãƒ—
                if output_text.strip().lower() == "exit":
                    continue

                # ã‚¿ã‚¤ãƒˆãƒ«ã€è¶…è¦ç´„ã‚’æŠ½å‡º
                lines = [ln.strip() for ln in output_text.splitlines() if ln.strip()]
                title_line = next(
                    (ln for ln in lines if ln.startswith("ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘")), None
                )
                ultra_line = next(
                    (ln for ln in lines if ln.startswith("ã€è¶…è¦ç´„ã€‘")), None
                )

                if title_line:
                    translated_title = title_line.replace("ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘", "").strip()
                else:
                    translated_title = "ï¼ˆç¿»è¨³å¤±æ•—ï¼‰"

                ultra_text = (
                    ultra_line.replace("ã€è¶…è¦ç´„ã€‘", "").strip() if ultra_line else ""
                )

                # è¦ç´„æœ¬æ–‡ã¯ã€Œã‚¿ã‚¤ãƒˆãƒ«è¡Œã€ã¨ã€Œè¶…è¦ç´„è¡Œã€ã‚’é™¤ã„ãŸæ®‹ã‚Š
                summary_lines = [
                    ln
                    for ln in lines
                    if not ln.startswith("ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘")
                    and not ln.startswith("ã€è¶…è¦ç´„ã€‘")
                ]
                summary_text = "\n".join(summary_lines).strip()
                summary_html = summary_text.replace("\n", "<br>")

                summarized_results.append(
                    {
                        "source": item["source"],
                        "url": item["url"],
                        "title": translated_title,
                        "summary": summary_html,
                        "ultra": ultra_text,  # â˜… è¿½åŠ 
                    }
                )

            except Exception as e:
                print(
                    "ğŸ›‘ Error during translation:", e.__class__.__name__, "|", repr(e)
                )
                continue

            # ãƒãƒƒãƒå†…ã§å¾®ã‚¹ãƒªãƒ¼ãƒ—ã—ã¦ãƒãƒ¼ã‚¹ãƒˆã‚’æŠ‘ãˆã‚‹
            time.sleep(0.6)

        if i + batch_size < len(translation_queue):
            print(f"ğŸ•’ Waiting {wait_seconds} seconds before next batch...")
            time.sleep(wait_seconds)

    # é‡è¤‡åˆ¤å®šâ†’ç‰‡æ–¹æ®‹ã—ï¼ˆæœ€çµ‚ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã®å½¢å¼ã¯å¤‰ãˆãªã„ï¼‰
    deduped = dedupe_articles_with_llm(client_dedupe, summarized_results, debug=True)

    # å¿µã®ãŸã‚ï¼šè¿”å´ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å›ºå®šï¼ˆä½™è¨ˆãªã‚­ãƒ¼ãŒæ··ã–ã£ã¦ã„ãŸã‚‰è½ã¨ã™ï¼‰
    normalized = [
        {
            "source": x.get("source"),
            "url": x.get("url"),
            "title": x.get("title"),
            "summary": x.get("summary"),
        }
        for x in deduped
    ]
    return normalized


def send_email_digest(summaries):
    sender_email = os.getenv("EMAIL_SENDER")
    sender_pass = os.getenv("GMAIL_APP_PASSWORD")
    # ãƒ¡ãƒ¼ãƒ«é€ä¿¡å…ˆæœ¬ç•ªç”¨
    recipient_emails = os.getenv("EMAIL_RECIPIENTS", "").split(",")

    # âœ… ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    digest_date = get_today_date_mmt()
    date_str = digest_date.strftime("%Yå¹´%-mæœˆ%-dæ—¥") + "åˆ†"

    # ãƒ¡ãƒ‡ã‚£ã‚¢ã”ã¨ã«ã¾ã¨ã‚ã‚‹
    media_grouped = defaultdict(list)
    for item in summaries:
        media_grouped[item["source"]].append(item)

    # ãƒ¡ãƒ¼ãƒ«ã‚¿ã‚¤ãƒˆãƒ«
    subject = "ãƒŸãƒ£ãƒ³ãƒãƒ¼é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€" + date_str + "ã€‘"

    # ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã®HTMLç”Ÿæˆ
    html_content = """
    <html>
    <body style="font-family: Arial, sans-serif; background-color: #ffffff; color: #333333;">
    """

    # ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã¯ä½¿ã†ãŒã€è¦‹å‡ºã—ã¯å„è¨˜äº‹ã®ä¸­ã«å…¥ã‚Œã‚‹
    for media, articles in media_grouped.items():
        for item in articles:
            title_jp = item["title"]  # ã€Œã‚¿ã‚¤ãƒˆãƒ«: ã€ã®æ¥é ­è¾ã¯å¤–ã™
            url = item["url"]
            summary_html = item["summary"]  # æ—¢ã« <br> æ•´å½¢æ¸ˆã¿

            # å‚è€ƒHTMLæº–æ‹ ï¼šè¦‹å‡ºã—(h2)ã®å³å´ã«ãƒ¡ãƒ‡ã‚£ã‚¢åã€‚
            heading_html = (
                "<h2 style='margin-bottom:5px'>"
                f"{title_jp}ã€€"
                "<span style='font-size:0.83rem;font-weight:600'>"  # â† h5ç›¸å½“
                f"{media} "
                "</span>"
                "</h2>"
            )

            html_content += (
                "<div style='margin-bottom:20px'>"
                f"{heading_html}"
                "<div style='background-color:#f9f9f9;padding:10px;border-radius:8px'>"
                f"{summary_html}"
                "</div>"
                f"<p><a href='{url}' style='color:#1a0dab' target='_blank'>æœ¬æ–‡ã‚’èª­ã‚€</a></p>"
                "</div><hr style='border-top: 1px solid #cccccc;'>"
                "</div>"
            )

    html_content += "</body></html>"
    html_content = clean_html_content(html_content)

    from_display_name = "Myanmar News Digest"

    msg = EmailMessage(policy=SMTPUTF8)
    msg["Subject"] = subject
    msg["From"] = formataddr((from_display_name, sender_email))
    msg["To"] = ", ".join(recipient_emails)
    msg.set_content("HTMLãƒ¡ãƒ¼ãƒ«ã‚’é–‹ã‘ã‚‹ç’°å¢ƒã§ã”ç¢ºèªãã ã•ã„ã€‚", charset="utf-8")
    msg.add_alternative(html_content, subtype="html", charset="utf-8")

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender_email, sender_pass)
            server.send_message(msg)
            print("âœ… ãƒ¡ãƒ¼ãƒ«é€ä¿¡å®Œäº†")
    except Exception as e:
        print(f"âŒ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    date_mmt = get_today_date_mmt()
    seen_urls = set()

    # articles = get_frontier_articles_for(date_mmt)
    # for art in articles:
    #     print(f"{art['date']} - {art['title']}\n{art['url']}\n")

    # === Mizzima (Burmese) ===
    print("=== Mizzima (Burmese) ===")
    articles_mizzima = get_mizzima_articles_from_category(
        date_mmt,
        "https://bur.mizzima.com",
        "Mizzima (Burmese)",
        "/category/%e1%80%9e%e1%80%90%e1%80%84%e1%80%ba%e1%80%b8/%e1%80%99%e1%80%bc%e1%80%94%e1%80%ba%e1%80%99%e1%80%ac%e1%80%9e%e1%80%90%e1%80%84%e1%80%ba%e1%80%b8",
        max_pages=3,
    )
    process_and_enqueue_articles(articles_mizzima, "Mizzima (Burmese)", seen_urls)

    print("=== BBC Burmese ===")
    articles_bbc = get_bbc_burmese_articles_for(date_mmt)
    process_and_enqueue_articles(articles_bbc, "BBC Burmese", seen_urls)

    print("=== Irrawaddy ===")
    articles_irrawaddy = get_irrawaddy_articles_for(date_mmt)
    # MEMO: ãƒ­ã‚°ç”¨ã€ãƒ‡ãƒãƒƒã‚¯ã§ãƒ­ã‚°ç¢ºèª
    # print("RESULTS:", json.dumps(articles_irrawaddy, ensure_ascii=False, indent=2))
    process_and_enqueue_articles(
        articles_irrawaddy,
        "Irrawaddy",
        seen_urls,
        bypass_keyword=True,  # â† Irrawaddyã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è½ã¨ã•ãªã„
        trust_existing_body=True,  # â† ã•ã£ãå…¥ã‚ŒãŸ body ã‚’ãã®ã¾ã¾ä½¿ã†ï¼ˆå†ãƒ•ã‚§ãƒƒãƒã—ãªã„ï¼‰
    )

    print("=== Khit Thit Media ===")
    articles_khit = get_khit_thit_media_articles_from_category(date_mmt, max_pages=3)
    process_and_enqueue_articles(articles_khit, "Khit Thit Media", seen_urls)

    # URLãƒ™ãƒ¼ã‚¹ã®é‡è¤‡æ’é™¤ã‚’å…ˆã«è¡Œã†
    print(f"âš™ï¸ Removing URL duplicates from {len(translation_queue)} articles...")
    translation_queue = deduplicate_by_url(translation_queue)

    # ãƒãƒƒãƒç¿»è¨³å®Ÿè¡Œ (5ä»¶ã”ã¨ã«1åˆ†å¾…æ©Ÿ)
    all_summaries = process_translation_batches(batch_size=5, wait_seconds=60)

    send_email_digest(all_summaries)
